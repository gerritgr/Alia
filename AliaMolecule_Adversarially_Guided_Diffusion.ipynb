{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerritgr/Alia/blob/main/AliaMolecule_Adversarially_Guided_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VWSEj4zWH9P"
      },
      "source": [
        "# AliaMolecule - Adversarially Guided Probabilistic Diffusion\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFu0doC8D-UW"
      },
      "source": [
        "## Todo\n",
        "- DEVICE\n",
        "- positional encoding\n",
        "- resnet units\n",
        "- save loss list\n",
        "- cosine schedule\n",
        "- predict endpoint and not noise\n",
        "- why is batch wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAao3t3nPHHQ"
      },
      "source": [
        "Molecules:\n",
        "- remove hydrogen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqaO-iDsNHDS"
      },
      "source": [
        "- $t=0$ is original image.  => ($i=999$)\n",
        "- $t=1$ means one noise addition was made.  => ($i=T-t=999-t$)\n",
        "- $t=T=999$ is maximal addition. => ($i=0$)\n",
        "- $T = TIMESTEPS-1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmgsr5qYDymy"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yGSIj3enuyM6"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME = \"AliaMolecule\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qfPpruBP7wod"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  import torch_geometric\n",
        "except:\n",
        "  os.system(\"pip install torch_geometric\")\n",
        "  # Optional dependencies:\n",
        "  os.system(\"pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\")\n",
        "  os.system(\"pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu102.html --force-reinstall\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2nNF8e3Hgiej"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import rdkit\n",
        "except:\n",
        "  os.system(\"pip install rdkit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-leX8Lpch0-s"
      },
      "outputs": [],
      "source": [
        "#!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu102.html --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E7HLKq3WKG_b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "USE_COLAB = False\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  USE_COLAB = True\n",
        "except:\n",
        "  pass\n",
        "\n",
        "if USE_COLAB and not os.path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM0dnwkxLhm_",
        "outputId": "8d0c178b-5883-4125-e0de-07b13013d2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory:  /content\n",
            "New Working Directory:  /content/drive/MyDrive/colab/AliaMolecule\n"
          ]
        }
      ],
      "source": [
        "if USE_COLAB:\n",
        "  dir_path = f'/content/drive/MyDrive/colab/{PROJECT_NAME}/'\n",
        "  if not os.path.exists(dir_path):\n",
        "    os.makedirs(dir_path)\n",
        "  print(\"Current Working Directory: \", os.getcwd())\n",
        "  if os.getcwd() != dir_path:\n",
        "    os.chdir(dir_path)\n",
        "    print(\"New Working Directory: \", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCUaC9ZqwH9E"
      },
      "source": [
        "## Git Clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wtZm7FbVwJGx"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"Alia\"):\n",
        "  os.system(\"git clone https://github.com/gerritgr/Alia.git\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIZNe0X4Z-f7"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xQTCAyyHjLjW"
      },
      "outputs": [],
      "source": [
        "#%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 100 # Set this to 300 to get better image quality\n",
        "from PIL import Image # We use PIL to load images\n",
        "import seaborn as sns\n",
        "import imageio # to generate .gifs\n",
        "import networkx as nx\n",
        "\n",
        "# always good to have\n",
        "import glob, random, os, traceback, time, copy\n",
        "import pickle\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Linear as Lin\n",
        "from torch.nn import Sequential as Seq\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GATv2Conv, GraphNorm, BatchNorm\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_4brRkfG9Za"
      },
      "source": [
        "### Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX33f9FYG9gL",
        "outputId": "5cb46e20-dbd8-4b52-986e-e72b3809c0cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Mol Gen\n",
        "NUM_SAMPLES = 500 # how many samples to generate for the trainings set\n",
        "NUM_GRAPHS_TO_GENERATE = 10 # during inference\n",
        "TRAIN_TEST_SPLIT = 0.8\n",
        "\n",
        "INDICATOR_FEATURE_DIM = 1\n",
        "FEATURE_DIM = 5 # (has to be the same for atom and bond)\n",
        "ATOM_FEATURE_DIM = FEATURE_DIM\n",
        "BOND_FEATURE_DIM = FEATURE_DIM\n",
        "NON_NODES = [True] + [False]*5 + [True] * 5\n",
        "NON_EDGES = [True] + [True]*5 + [False] * 5\n",
        "\n",
        "TIME_FEATURE_DIM = 1\n",
        "\n",
        "\n",
        "\n",
        "# General\n",
        "EPOCHS = 100000\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_ROUNDS = 10\n",
        "BASE_MODEL_EPOCHS = 20000\n",
        "EPOCHS_DISC_MODEL = 101\n",
        "EPOCHS_GUIDE_MODEL = 201\n",
        "GUIDE_FRACTION = 20   # only apply guidacne in the last 1/4 of the process\n",
        "\n",
        "\n",
        "# Diffusion\n",
        "TIMESTEPS = 1000\n",
        "START = 0.0001\n",
        "END = 0.015\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#DEVICE = torch.device('cpu')\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkf85cyRvGiT"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eSYvg6xIw-vV"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, graph_loss_list, loss_list, epoch_i):\n",
        "  if epoch_i == 0:\n",
        "    return\n",
        "  save_path = f\"aliamol_model_epoch_{epoch_i:08}.pth\"\n",
        "\n",
        "  # Save the model state dict and the optimizer state dict in a dictionary\n",
        "  torch.save({\n",
        "              'epoch': epoch_i,\n",
        "              'loss_list': loss_list,\n",
        "              'graph_loss_list': graph_loss_list,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict()\n",
        "              }, save_path)\n",
        "\n",
        "def load_latest_checkpoint(model, optimizer, graph_loss_list, loss_list, epoch_i):\n",
        "  try:\n",
        "    checkpoint_paths = sorted(glob.glob(\"aliamolmodel_epoch_*.pth\"))\n",
        "    if len(checkpoint_paths) == 0:\n",
        "      return model, optimizer, graph_loss_list, loss_list, epoch_i\n",
        "\n",
        "    latest_checkpoint_path = checkpoint_paths[-1]\n",
        "    checkpoint = torch.load(latest_checkpoint_path)\n",
        "\n",
        "    # Assuming model and optim are your initialized model and optimizer\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch_i = checkpoint['epoch']\n",
        "    graph_loss_list = checkpoint['graph_loss_list']\n",
        "    print(f\"read checkpoint of epoch {epoch_i:08} from disc.\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  return model, optimizer, graph_loss_list, loss_list, epoch_i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZSk-qMNvj6Q"
      },
      "source": [
        "## Build Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d8D6H1LGMp15"
      },
      "outputs": [],
      "source": [
        "# each node represents atom or bond\n",
        "# thus, each row of data.x has the form TAAAAABBBBB\n",
        "# T is 1 (atom) or -1 (bond). AAAAA is one-hot encoding of element (the five in qm9);\n",
        "# BBBBB is one hot encoding of bond type, single, double, triple, ring,\n",
        "# if a node is an atom then BBBBB is all -1, if it is a bond AAAAA is all -1\n",
        "# The diffusion only happens on either AAAAA or BBBBB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gq5S0kudvD8T"
      },
      "outputs": [],
      "source": [
        "def build_dataset(seed=1234):\n",
        "  from Alia.smiles_to_pyg.molecule_load_and_convert3 import read_qm9\n",
        "  dataset = read_qm9()\n",
        "  dataset = [g for g in dataset if g.x.shape[0] > 1]\n",
        "  random.Random(seed).shuffle(dataset)\n",
        "  split = int(len(dataset)*TRAIN_TEST_SPLIT + 0.5)\n",
        "  dataset_train = dataset[:split]\n",
        "  dataset_test = dataset[split:]\n",
        "  assert(dataset_train[0].x[0,:].numel() == INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM)\n",
        "\n",
        "  return dataset_train, dataset_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxwETR7mPq1b"
      },
      "source": [
        "## Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6K-jxdBPt5Z"
      },
      "source": [
        "### Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ybvf4HoWPtH2"
      },
      "outputs": [],
      "source": [
        "def generate_schedule(start = START, end = END, timesteps=TIMESTEPS):\n",
        "    \"\"\"\n",
        "    Generates a schedule of beta and alpha values for a forward process.\n",
        "\n",
        "    Args:\n",
        "    start (float): The starting value for the beta values. Default is START.\n",
        "    end (float): The ending value for the beta values. Default is END.\n",
        "    timesteps (int): The number of timesteps to generate. Default is TIMESTEPS.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple of three tensors containing the beta values, alpha values, and\n",
        "    cumulative alpha values (alpha bars).\n",
        "    \"\"\"\n",
        "    betas = torch.linspace(start, end, timesteps, device = DEVICE)\n",
        "    #alphas = 1.0 - betas\n",
        "    #alpha_bars = torch.cumprod(alphas, axis=0)\n",
        "    assert(betas.numel() == TIMESTEPS)\n",
        "    return betas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCrL8EfP2tM"
      },
      "source": [
        "## Forward Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zpJn8TzvD-5",
        "outputId": "c924445e-083e-4e36-cdd8-31e477563611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([[ 1.0196],\n",
              "          [ 2.0002],\n",
              "          [-1.0157]], device='cuda:0'),\n",
              "  tensor([[ 1.9652],\n",
              "          [ 0.0339],\n",
              "          [-1.0835]], device='cuda:0')),\n",
              " None,\n",
              " (tensor([[-0.3073],\n",
              "          [-0.3738],\n",
              "          [ 0.2333]], device='cuda:0'),\n",
              "  tensor([[-0.3299],\n",
              "          [-0.4189],\n",
              "          [ 0.1659]], device='cuda:0')))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "def forward_diffusion(node_features, future_t):\n",
        "  \"\"\"\n",
        "  Performs a forward diffusion process on an node_features tensor.\n",
        "  Each row can theoreetically have its own future time point.\n",
        "  Implements the second equation from https://youtu.be/a4Yfz2FxXiY?t=649\n",
        "  \"\"\"\n",
        "  row_num = node_features.shape[0]\n",
        "  feature_dim = node_features.shape[1]\n",
        "  future_t = future_t.view(-1)\n",
        "  assert(row_num == future_t.numel())\n",
        "\n",
        "  betas = generate_schedule()\n",
        "\n",
        "  noise = torch.randn_like(node_features, device=DEVICE)\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t).view(row_num, 1)\n",
        "  assert(alphabar_t.numel() == row_num)\n",
        "\n",
        "  new_node_features_mean = torch.sqrt(alphabar_t) * node_features # column-wise multiplication, now matrix\n",
        "  new_node_features_std = torch.sqrt(1.-alphabar_t) #this is a col vector\n",
        "  new_node_features_std = new_node_features_std.repeat(1,feature_dim) #this is a matrix\n",
        "  noisey_node_features =  new_node_features_mean + new_node_features_std * noise\n",
        "\n",
        "  return noisey_node_features, noise\n",
        "\n",
        "forward_diffusion(torch.tensor([1,2,3.], device=DEVICE).view(3,1), torch.tensor([0,0,999], device=DEVICE)), print(\"\"), forward_diffusion(torch.tensor([1,2,3.], device=DEVICE).view(3,1), torch.tensor([999,999,999], device=DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXDf7AsfW_PE"
      },
      "source": [
        "## Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QScWK9RzI0c"
      },
      "source": [
        "### Model Spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l1VOml_vvEBQ"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import PNA\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "\n",
        "def dataset_to_degree_bin(train_dataset):\n",
        "  # Compute the maximum in-degree in the training data.\n",
        "  max_degree = -1\n",
        "  for data in train_dataset:\n",
        "    data = data.to(DEVICE)\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    max_degree = max(max_degree, int(d.max()))\n",
        "\n",
        "  deg = torch.zeros(max_degree + 1, dtype=torch.long, device=DEVICE)\n",
        "  for data in train_dataset:\n",
        "    data = data.to(DEVICE)\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    deg += torch.bincount(d, minlength=deg.numel())\n",
        "  return deg\n",
        "\n",
        "class PNAnet(torch.nn.Module):\n",
        "  def __init__(self, train_dataset, hidden_channels=32, depth=4, dropout=0.05, towers=1, normalization=True, pre_post_layers=1):\n",
        "    super(PNAnet, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Calculate x as the difference between mult_y and hidden_dim\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1) #tod fix\n",
        "    #out_channels = towers * ((out_channels // towers) + 1)\n",
        "\n",
        "    in_channels = INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM+ TIME_FEATURE_DIM #INDICATOR_FEATURE_DIM entries are noise free\n",
        "    out_channels = FEATURE_DIM\n",
        "\n",
        "    deg = dataset_to_degree_bin(train_dataset)\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "    self.pnanet = PNA(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=hidden_channels, num_layers=depth, aggregators=aggregators, scalers=scalers, deg=deg, dropout=dropout, towers=towers, norm=self.normalization, pre_layers=pre_post_layers, post_layers=pre_post_layers)\n",
        "\n",
        "    self.final_mlp = Seq(Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, out_channels))\n",
        "\n",
        "\n",
        "  def forward(self, x_in, t, edge_index):\n",
        "    t = t.view(-1,TIME_FEATURE_DIM)\n",
        "    x = torch.concat((x_in, t), dim=1)\n",
        "    x = self.pnanet(x, edge_index)\n",
        "    x = self.final_mlp(x)\n",
        "    assert(x.numel() > 1 )\n",
        "\n",
        "    #node_indicator = x_in[:,0] > 0\n",
        "    #node_indicator = x_in[:,0] < 0\n",
        "    #x[node_indicator, NON_NODES] = x_in[node_indicator, NON_NODES]\n",
        "    #x[edge_indicator, NON_EDGES] = x_in[edge_indicator, NON_EDGES]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "#model = PNAnet([data])\n",
        "\n",
        "#model(data.x, data.edge_index, torch.ones(data.x.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SKlGT9rzFXU"
      },
      "source": [
        "### Train Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AfRQ1DBvvEIT"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer):\n",
        "  \"\"\"\n",
        "  Trains a denoising model for one epoch using a given data loader and optimization algorithm.\n",
        "\n",
        "  Args:\n",
        "  model: The denoising model.\n",
        "  dataloader: The data loader for the training data.\n",
        "  optimizer: The torch optimizer.\n",
        "\n",
        "  Returns:\n",
        "  float: The average loss for the epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  schedule = generate_schedule()\n",
        "  model.train()\n",
        "  start_time = time.time()\n",
        "  loss_list = list()\n",
        "\n",
        "  for g in tqdm(dataloader): #todo batches deactivated\n",
        "    if g.x.shape[0] < 2:\n",
        "      continue\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    g.to(DEVICE)\n",
        "    #num_graphs_in_batch = 1\n",
        "    num_graphs_in_batch = int(torch.max(g.batch).item()+1)\n",
        "    future_t_select = torch.randint(0, TIMESTEPS, (num_graphs_in_batch,), device = DEVICE)\n",
        "    #future_t = torch.tensor([0]*g.x.shape[0], device=DEVICE)#\n",
        "    future_t = torch.gather(future_t_select, 0, g.batch)\n",
        "    assert(future_t.numel() == g.x.shape[0])\n",
        "\n",
        "    mask = torch.concat((torch.tensor([False]*g.x.shape[0], device=DEVICE).view(-1,1), g.x[:,1:]>-0.5), dim=1) #this only works on original values\n",
        "    x = g.x[mask].view(g.x.shape[0], FEATURE_DIM)\n",
        "    x_with_noise, noise_gt = forward_diffusion(x, future_t)\n",
        "\n",
        "    x_in = g.x.clone()\n",
        "    x_in[mask] = x_with_noise.flatten()\n",
        "    noise_pred = model(x_in, future_t, g.edge_index)\n",
        "\n",
        "    loss = F.mse_loss(noise_gt, noise_pred)\n",
        "    loss.backward()\n",
        "    loss_list.append(loss.item())\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  return float(np.mean(loss_list)), time.time()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_ryCoO5wOoG",
        "outputId": "4db4a49b-f5f5-444a-dec4-2c2f2f3c1483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 72.21it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9904751658439637, 0.28139543533325195)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "def train_epoch_test():\n",
        "  dataset_train, dataset_test = build_dataset()\n",
        "  data = dataset_train[0]\n",
        "  model_base = PNAnet(dataset_train[:20]).to(DEVICE) #20 should be enough to estimate statistics\n",
        "\n",
        "  t = torch.tensor([1.0]).repeat(data.x.shape[0])\n",
        "  t = t.to(DEVICE)\n",
        "  model_base(data.x, t, data.edge_index)\n",
        "  dataloader = DataLoader(dataset_train[:20], batch_size=1, shuffle=True)\n",
        "  optimizer = Adam(model_base.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "  return train_epoch(model_base, dataloader, optimizer)\n",
        "\n",
        "#train_epoch_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV_QpxiPwpY1"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_list(xy_data, filename, title=\"\", xlabel=\"\", ylabel=\"\"):\n",
        "  plt.clf()\n",
        "  plt.plot([x for x, y in xy_data], [y for x, y in xy_data])\n",
        "  plt.title(title)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.savefig(filename)\n",
        "\n",
        "  with open(filename+\".pickle\", 'wb') as f:\n",
        "    pickle.dump(xy_data, f)"
      ],
      "metadata": {
        "id": "BtCkdi6M77nG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "R6N8YD6rvERr"
      },
      "outputs": [],
      "source": [
        "def train_base_model(train_loader, epoch_num=None):\n",
        "  print(\"train base model\")\n",
        "\n",
        "  dataset_train = train_loader.dataset\n",
        "  model_base = PNAnet(dataset_train)\n",
        "  model_base = model_base.to(DEVICE)\n",
        "\n",
        "  lr = LEARNING_RATE\n",
        "  if BATCH_SIZE > 1:\n",
        "    lr = lr/100.0\n",
        "  optimizer = Adam(model_base.parameters(), lr = LEARNING_RATE)\n",
        "  loss_list = list()\n",
        "  graph_loss_list = list()\n",
        "  model_base, optimizer, graph_loss_list, loss_list, epoch_start = load_latest_checkpoint(model_base, optimizer, graph_loss_list, loss_list, epoch_i=0)\n",
        "\n",
        "  epoch_num = epoch_num if epoch_num is not None else EPOCHS\n",
        "  epoch_start = min(epoch_start, epoch_num)\n",
        "\n",
        "\n",
        "  for epoch_i in range(epoch_start,epoch_num):\n",
        "    try:\n",
        "      loss, time_elapsed = train_epoch(model_base, train_loader, optimizer)\n",
        "      loss_list.append((epoch_i, loss))\n",
        "      if epoch_i % 1 == 0 or epoch_i == epoch_num - 1:\n",
        "        plot_list(loss_list, \"train_base.png\", title=\"train loss base model\", xlabel='epoch', ylabel='loss')\n",
        "        print(f\"loss in epoch {epoch_i:07} is: {loss:05.4f} with mean loss {np.mean([y for x,y in loss_list] + [loss]):05.4f} with runtime {time_elapsed:05.4f}\")\n",
        "\n",
        "      if (epoch_i % 10 == 0 and epoch_i >= 30) or epoch_i == epoch_num - 1:\n",
        "        #graphs = generate_examples(model_base, epoch_i, betas, dataset_train)\n",
        "        #graph_loss_list.append(compute_generation_loss(graphs, None))\n",
        "        #print(f\"generation loss: {graph_loss_list[-1]:06.4f}\")\n",
        "        #plot_base(graph_loss_list, loss_list)\n",
        "        save_model(model_base, optimizer, graph_loss_list, loss_list, epoch_i+1)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"An error occurred during training: \\n\", str(e))\n",
        "      traceback.print_exc()\n",
        "      raise e\n",
        "\n",
        "\n",
        "  return model_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dO7VLZT5vEUD"
      },
      "outputs": [],
      "source": [
        "def train_base_model_test():\n",
        "  dataset_train, dataset_test = build_dataset()\n",
        "  dataloader = DataLoader(dataset_train[:20], batch_size=1, shuffle=True)\n",
        "\n",
        "  train_base_model(dataloader, epoch_num=30)\n",
        "#train_base_model_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting Everything Together"
      ],
      "metadata": {
        "id": "FlxLHZN16p_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_base, dataset_base_test = build_dataset()\n",
        "dataloader_base = DataLoader(dataset_base, batch_size=BATCH_SIZE, shuffle=True)\n",
        "model_base = train_base_model(dataloader_base, epoch_num = BASE_MODEL_EPOCHS)\n",
        "model_cassandra = None\n",
        "# TODO test set train\n",
        "\n",
        "for round_i in range(NUM_ROUNDS):\n",
        "  model_base = train_base_model(dataloader_base)\n",
        "  BATCH_SIZE = 1\n",
        "\n",
        "  #samples = sample_base_model(model_base, round_i, model_guide=model_cassandra, num=NUM_SAMPLES)  #TODO\n",
        "  #plot_samples(samples, model_disc=None, round_i=round_i)\n",
        "  #dataloader_disc = get_merged_dataloader(samples[:num_samples_train], dataset_base)\n",
        "  #dataloader_disc_test = get_merged_dataloader(samples[num_samples_train:], dataset_base_test)\n",
        "  #model_cassandra = train_cassandra_model(dataloader_disc, dataloader_disc_test, round_i)\n",
        "  #new_epoch_num = BASE_MODEL_EPOCHS + (round_i+1)*int(BASE_MODEL_EPOCHS*0.2)\n",
        "  #model_base = train_base_model(dataloader_base, epoch_num = new_epoch_num)\n",
        "\n",
        "#samples = sample_base_model(model_base, round_i=NUM_ROUNDS, model_guide=model_cassandra, num=NUM_SAMPLES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVvB3KCd6f7f",
        "outputId": "7655cabb-0ac6-49e0-ce4b-1b5f222d7140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train base model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000000 is: 0.8411 with mean loss 0.8411 with runtime 20.9891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000001 is: 0.6407 with mean loss 0.7075 with runtime 20.9935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000002 is: 0.4960 with mean loss 0.6185 with runtime 20.6396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000003 is: 0.3378 with mean loss 0.5307 with runtime 20.7974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:24<00:00, 34.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000004 is: 0.1348 with mean loss 0.4309 with runtime 24.4481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000005 is: 0.1181 with mean loss 0.3838 with runtime 20.9421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000006 is: 0.1103 with mean loss 0.3486 with runtime 20.6707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000007 is: 0.1047 with mean loss 0.3209 with runtime 21.3317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000008 is: 0.0988 with mean loss 0.2981 with runtime 20.6926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000009 is: 0.0943 with mean loss 0.2792 with runtime 20.9902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000010 is: 0.0912 with mean loss 0.2633 with runtime 22.3290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000011 is: 0.0880 with mean loss 0.2495 with runtime 22.8063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000012 is: 0.0853 with mean loss 0.2376 with runtime 21.6784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000013 is: 0.0838 with mean loss 0.2273 with runtime 21.1226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000014 is: 0.0836 with mean loss 0.2183 with runtime 21.5840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000015 is: 0.0807 with mean loss 0.2100 with runtime 21.5609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000016 is: 0.0800 with mean loss 0.2027 with runtime 20.9965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000017 is: 0.0801 with mean loss 0.1963 with runtime 21.2008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000018 is: 0.0784 with mean loss 0.1903 with runtime 21.3839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000019 is: 0.0773 with mean loss 0.1849 with runtime 21.2005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000020 is: 0.0762 with mean loss 0.1799 with runtime 21.1200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000021 is: 0.0760 with mean loss 0.1754 with runtime 21.1889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000022 is: 0.0749 with mean loss 0.1711 with runtime 21.7366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000023 is: 0.0753 with mean loss 0.1673 with runtime 21.7855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000024 is: 0.0747 with mean loss 0.1637 with runtime 22.4550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000025 is: 0.0732 with mean loss 0.1603 with runtime 22.8326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000026 is: 0.0733 with mean loss 0.1572 with runtime 22.4719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000027 is: 0.0723 with mean loss 0.1543 with runtime 21.9691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000028 is: 0.0721 with mean loss 0.1515 with runtime 22.2362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000029 is: 0.0713 with mean loss 0.1489 with runtime 21.7896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000030 is: 0.0713 with mean loss 0.1465 with runtime 21.2600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000031 is: 0.0707 with mean loss 0.1442 with runtime 21.6667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000032 is: 0.0702 with mean loss 0.1420 with runtime 21.5955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000033 is: 0.0702 with mean loss 0.1399 with runtime 21.7715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000034 is: 0.0701 with mean loss 0.1380 with runtime 21.4976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000035 is: 0.0701 with mean loss 0.1361 with runtime 21.5422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000036 is: 0.0692 with mean loss 0.1344 with runtime 21.7999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000037 is: 0.0687 with mean loss 0.1327 with runtime 21.6892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000038 is: 0.0690 with mean loss 0.1311 with runtime 21.6584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000039 is: 0.0684 with mean loss 0.1295 with runtime 20.9038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000040 is: 0.0674 with mean loss 0.1280 with runtime 20.7827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 41.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000041 is: 0.0676 with mean loss 0.1266 with runtime 20.3280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000042 is: 0.0668 with mean loss 0.1252 with runtime 20.5563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000043 is: 0.0665 with mean loss 0.1239 with runtime 20.5703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000044 is: 0.0667 with mean loss 0.1227 with runtime 20.4779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000045 is: 0.0664 with mean loss 0.1215 with runtime 20.5030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000046 is: 0.0658 with mean loss 0.1203 with runtime 20.4856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000047 is: 0.0655 with mean loss 0.1192 with runtime 20.6898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 41.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000048 is: 0.0657 with mean loss 0.1181 with runtime 20.2626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000049 is: 0.0649 with mean loss 0.1171 with runtime 20.6323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000050 is: 0.0650 with mean loss 0.1161 with runtime 20.5307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 41.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000051 is: 0.0653 with mean loss 0.1151 with runtime 20.2976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000052 is: 0.0643 with mean loss 0.1142 with runtime 20.6771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 41.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000053 is: 0.0643 with mean loss 0.1132 with runtime 20.2490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000054 is: 0.0641 with mean loss 0.1124 with runtime 20.5011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000055 is: 0.0635 with mean loss 0.1115 with runtime 20.7059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 41.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000056 is: 0.0640 with mean loss 0.1107 with runtime 20.2344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000057 is: 0.0634 with mean loss 0.1099 with runtime 20.6013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000058 is: 0.0629 with mean loss 0.1091 with runtime 20.4776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000059 is: 0.0628 with mean loss 0.1083 with runtime 20.7798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000060 is: 0.0621 with mean loss 0.1076 with runtime 20.8379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000061 is: 0.0625 with mean loss 0.1069 with runtime 21.1605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000062 is: 0.0622 with mean loss 0.1062 with runtime 22.3989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000063 is: 0.0624 with mean loss 0.1055 with runtime 22.4630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000064 is: 0.0618 with mean loss 0.1048 with runtime 21.6488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000065 is: 0.0619 with mean loss 0.1042 with runtime 21.7698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000066 is: 0.0614 with mean loss 0.1035 with runtime 22.0831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000067 is: 0.0615 with mean loss 0.1029 with runtime 22.0030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000068 is: 0.0609 with mean loss 0.1023 with runtime 21.4891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000069 is: 0.0604 with mean loss 0.1017 with runtime 21.6250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000070 is: 0.0608 with mean loss 0.1012 with runtime 21.2425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000071 is: 0.0604 with mean loss 0.1006 with runtime 21.3639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000072 is: 0.0601 with mean loss 0.1000 with runtime 21.4678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000073 is: 0.0599 with mean loss 0.0995 with runtime 21.1704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000074 is: 0.0600 with mean loss 0.0990 with runtime 21.4725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000075 is: 0.0602 with mean loss 0.0985 with runtime 21.4886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000076 is: 0.0592 with mean loss 0.0980 with runtime 21.8298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 38.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000077 is: 0.0590 with mean loss 0.0975 with runtime 22.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000078 is: 0.0589 with mean loss 0.0970 with runtime 21.9795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000079 is: 0.0588 with mean loss 0.0965 with runtime 21.6697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000080 is: 0.0586 with mean loss 0.0961 with runtime 22.7651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000081 is: 0.0580 with mean loss 0.0956 with runtime 21.8545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000082 is: 0.0584 with mean loss 0.0952 with runtime 21.8938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000083 is: 0.0580 with mean loss 0.0947 with runtime 21.0487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000084 is: 0.0586 with mean loss 0.0943 with runtime 21.1067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000085 is: 0.0582 with mean loss 0.0939 with runtime 20.9022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000086 is: 0.0582 with mean loss 0.0935 with runtime 20.7581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000087 is: 0.0578 with mean loss 0.0931 with runtime 20.6703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000088 is: 0.0582 with mean loss 0.0927 with runtime 20.5549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000089 is: 0.0582 with mean loss 0.0923 with runtime 20.7455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000090 is: 0.0578 with mean loss 0.0919 with runtime 20.5703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000091 is: 0.0578 with mean loss 0.0916 with runtime 20.6719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000092 is: 0.0576 with mean loss 0.0912 with runtime 20.8984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000093 is: 0.0571 with mean loss 0.0908 with runtime 20.4344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000094 is: 0.0579 with mean loss 0.0905 with runtime 20.6468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000095 is: 0.0567 with mean loss 0.0901 with runtime 21.4953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000096 is: 0.0571 with mean loss 0.0898 with runtime 21.5480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000097 is: 0.0576 with mean loss 0.0895 with runtime 21.4584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000098 is: 0.0567 with mean loss 0.0891 with runtime 21.6583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000099 is: 0.0570 with mean loss 0.0888 with runtime 21.5577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000100 is: 0.0565 with mean loss 0.0885 with runtime 21.8397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000101 is: 0.0573 with mean loss 0.0882 with runtime 21.7322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000102 is: 0.0565 with mean loss 0.0879 with runtime 21.3445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000103 is: 0.0563 with mean loss 0.0876 with runtime 22.1875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000104 is: 0.0568 with mean loss 0.0873 with runtime 22.1602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000105 is: 0.0560 with mean loss 0.0870 with runtime 21.9343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000106 is: 0.0568 with mean loss 0.0867 with runtime 21.7178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000107 is: 0.0558 with mean loss 0.0864 with runtime 21.6573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000108 is: 0.0560 with mean loss 0.0862 with runtime 21.9447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000109 is: 0.0556 with mean loss 0.0859 with runtime 22.0973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000110 is: 0.0558 with mean loss 0.0856 with runtime 22.4794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000111 is: 0.0557 with mean loss 0.0854 with runtime 22.1181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000112 is: 0.0556 with mean loss 0.0851 with runtime 22.8685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000113 is: 0.0559 with mean loss 0.0848 with runtime 22.9420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000114 is: 0.0557 with mean loss 0.0846 with runtime 22.9106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000115 is: 0.0549 with mean loss 0.0843 with runtime 22.2083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000116 is: 0.0556 with mean loss 0.0841 with runtime 21.5180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000117 is: 0.0554 with mean loss 0.0839 with runtime 21.5520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000118 is: 0.0554 with mean loss 0.0836 with runtime 21.7161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000119 is: 0.0548 with mean loss 0.0834 with runtime 21.5390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000120 is: 0.0556 with mean loss 0.0831 with runtime 21.2644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000121 is: 0.0552 with mean loss 0.0829 with runtime 21.1115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000122 is: 0.0551 with mean loss 0.0827 with runtime 20.9708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000123 is: 0.0560 with mean loss 0.0825 with runtime 20.7995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000124 is: 0.0548 with mean loss 0.0823 with runtime 20.6655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000125 is: 0.0544 with mean loss 0.0820 with runtime 21.0213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000126 is: 0.0549 with mean loss 0.0818 with runtime 21.0459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000127 is: 0.0548 with mean loss 0.0816 with runtime 21.0780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000128 is: 0.0549 with mean loss 0.0814 with runtime 21.4928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000129 is: 0.0548 with mean loss 0.0812 with runtime 21.1258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000130 is: 0.0545 with mean loss 0.0810 with runtime 21.3705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000131 is: 0.0543 with mean loss 0.0808 with runtime 21.2208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000132 is: 0.0547 with mean loss 0.0806 with runtime 21.3042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000133 is: 0.0547 with mean loss 0.0804 with runtime 20.9268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000134 is: 0.0546 with mean loss 0.0802 with runtime 20.9628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000135 is: 0.0546 with mean loss 0.0800 with runtime 20.6897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000136 is: 0.0543 with mean loss 0.0799 with runtime 20.7725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000137 is: 0.0542 with mean loss 0.0797 with runtime 20.7506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000138 is: 0.0542 with mean loss 0.0795 with runtime 20.7633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000139 is: 0.0545 with mean loss 0.0793 with runtime 21.2827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000140 is: 0.0538 with mean loss 0.0791 with runtime 21.2334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000141 is: 0.0542 with mean loss 0.0790 with runtime 20.7477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000142 is: 0.0545 with mean loss 0.0788 with runtime 21.3288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000143 is: 0.0541 with mean loss 0.0786 with runtime 21.4618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000144 is: 0.0542 with mean loss 0.0784 with runtime 21.9590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000145 is: 0.0536 with mean loss 0.0783 with runtime 21.6364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000146 is: 0.0539 with mean loss 0.0781 with runtime 21.2446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000147 is: 0.0543 with mean loss 0.0780 with runtime 21.2459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000148 is: 0.0542 with mean loss 0.0778 with runtime 21.4199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000149 is: 0.0536 with mean loss 0.0776 with runtime 22.4092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000150 is: 0.0542 with mean loss 0.0775 with runtime 22.7577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000151 is: 0.0534 with mean loss 0.0773 with runtime 22.9994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000152 is: 0.0532 with mean loss 0.0772 with runtime 22.6347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000153 is: 0.0531 with mean loss 0.0770 with runtime 21.9104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000154 is: 0.0538 with mean loss 0.0769 with runtime 22.3049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000155 is: 0.0535 with mean loss 0.0767 with runtime 22.6646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000156 is: 0.0535 with mean loss 0.0766 with runtime 22.4205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000157 is: 0.0534 with mean loss 0.0764 with runtime 22.2481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000158 is: 0.0533 with mean loss 0.0763 with runtime 22.2692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000159 is: 0.0536 with mean loss 0.0761 with runtime 21.5530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000160 is: 0.0535 with mean loss 0.0760 with runtime 20.9114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000161 is: 0.0537 with mean loss 0.0759 with runtime 21.3487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000162 is: 0.0538 with mean loss 0.0757 with runtime 21.6089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000163 is: 0.0533 with mean loss 0.0756 with runtime 22.2473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000164 is: 0.0534 with mean loss 0.0754 with runtime 22.3238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000165 is: 0.0529 with mean loss 0.0753 with runtime 22.3732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000166 is: 0.0531 with mean loss 0.0752 with runtime 21.9257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000167 is: 0.0525 with mean loss 0.0750 with runtime 21.0315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000168 is: 0.0535 with mean loss 0.0749 with runtime 21.5972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000169 is: 0.0531 with mean loss 0.0748 with runtime 21.9995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000170 is: 0.0531 with mean loss 0.0747 with runtime 21.7854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000171 is: 0.0529 with mean loss 0.0745 with runtime 21.9379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000172 is: 0.0531 with mean loss 0.0744 with runtime 21.7868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000173 is: 0.0533 with mean loss 0.0743 with runtime 21.9655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000174 is: 0.0530 with mean loss 0.0742 with runtime 21.7751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000175 is: 0.0531 with mean loss 0.0741 with runtime 21.6915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000176 is: 0.0528 with mean loss 0.0739 with runtime 21.6999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000177 is: 0.0527 with mean loss 0.0738 with runtime 20.9475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000178 is: 0.0533 with mean loss 0.0737 with runtime 21.0071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000179 is: 0.0529 with mean loss 0.0736 with runtime 22.1834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000180 is: 0.0526 with mean loss 0.0735 with runtime 23.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000181 is: 0.0530 with mean loss 0.0734 with runtime 21.6502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000182 is: 0.0525 with mean loss 0.0732 with runtime 21.4657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000183 is: 0.0524 with mean loss 0.0731 with runtime 21.4636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000184 is: 0.0524 with mean loss 0.0730 with runtime 21.9918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000185 is: 0.0526 with mean loss 0.0729 with runtime 21.6360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000186 is: 0.0523 with mean loss 0.0728 with runtime 21.5902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000187 is: 0.0521 with mean loss 0.0727 with runtime 22.2331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000188 is: 0.0523 with mean loss 0.0726 with runtime 22.1813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000189 is: 0.0523 with mean loss 0.0725 with runtime 22.1100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:23<00:00, 36.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000190 is: 0.0526 with mean loss 0.0724 with runtime 23.2035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000191 is: 0.0526 with mean loss 0.0723 with runtime 21.5372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000192 is: 0.0521 with mean loss 0.0722 with runtime 21.2216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000193 is: 0.0526 with mean loss 0.0721 with runtime 21.5534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000194 is: 0.0519 with mean loss 0.0720 with runtime 21.5302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000195 is: 0.0517 with mean loss 0.0719 with runtime 21.7110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000196 is: 0.0519 with mean loss 0.0718 with runtime 21.4177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000197 is: 0.0519 with mean loss 0.0717 with runtime 21.4805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000198 is: 0.0512 with mean loss 0.0716 with runtime 21.4875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000199 is: 0.0516 with mean loss 0.0715 with runtime 21.7973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000200 is: 0.0513 with mean loss 0.0714 with runtime 21.4362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000201 is: 0.0516 with mean loss 0.0713 with runtime 21.9328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000202 is: 0.0515 with mean loss 0.0712 with runtime 22.1378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 36.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000203 is: 0.0512 with mean loss 0.0711 with runtime 22.8183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:23<00:00, 36.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000204 is: 0.0510 with mean loss 0.0710 with runtime 23.1063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000205 is: 0.0515 with mean loss 0.0709 with runtime 21.5708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000206 is: 0.0515 with mean loss 0.0708 with runtime 21.6556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000207 is: 0.0507 with mean loss 0.0707 with runtime 21.6181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000208 is: 0.0514 with mean loss 0.0706 with runtime 21.4123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000209 is: 0.0510 with mean loss 0.0705 with runtime 21.6573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000210 is: 0.0507 with mean loss 0.0704 with runtime 22.0413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000211 is: 0.0517 with mean loss 0.0703 with runtime 21.4290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000212 is: 0.0513 with mean loss 0.0702 with runtime 21.6213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000213 is: 0.0512 with mean loss 0.0701 with runtime 21.4438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000214 is: 0.0508 with mean loss 0.0700 with runtime 21.4263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000215 is: 0.0511 with mean loss 0.0700 with runtime 22.1930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000216 is: 0.0507 with mean loss 0.0699 with runtime 22.4400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000217 is: 0.0508 with mean loss 0.0698 with runtime 22.1671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000218 is: 0.0510 with mean loss 0.0697 with runtime 22.2661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000219 is: 0.0502 with mean loss 0.0696 with runtime 21.6619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000220 is: 0.0507 with mean loss 0.0695 with runtime 21.4528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000221 is: 0.0509 with mean loss 0.0694 with runtime 21.7278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000222 is: 0.0501 with mean loss 0.0694 with runtime 21.8447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000223 is: 0.0503 with mean loss 0.0693 with runtime 21.6491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000224 is: 0.0501 with mean loss 0.0692 with runtime 21.4274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 38.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000225 is: 0.0500 with mean loss 0.0691 with runtime 22.0160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000226 is: 0.0499 with mean loss 0.0690 with runtime 21.4292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000227 is: 0.0502 with mean loss 0.0689 with runtime 21.4367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:23<00:00, 36.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000228 is: 0.0500 with mean loss 0.0688 with runtime 23.1857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000229 is: 0.0502 with mean loss 0.0688 with runtime 21.6441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000230 is: 0.0500 with mean loss 0.0687 with runtime 21.0725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000231 is: 0.0503 with mean loss 0.0686 with runtime 21.5293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000232 is: 0.0501 with mean loss 0.0685 with runtime 21.3818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000233 is: 0.0495 with mean loss 0.0684 with runtime 21.2800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000234 is: 0.0499 with mean loss 0.0684 with runtime 21.5931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000235 is: 0.0497 with mean loss 0.0683 with runtime 21.1083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000236 is: 0.0496 with mean loss 0.0682 with runtime 21.2242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000237 is: 0.0495 with mean loss 0.0681 with runtime 21.1764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000238 is: 0.0496 with mean loss 0.0681 with runtime 21.0660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000239 is: 0.0493 with mean loss 0.0680 with runtime 21.4255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000240 is: 0.0501 with mean loss 0.0679 with runtime 21.1281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000241 is: 0.0491 with mean loss 0.0678 with runtime 20.8682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000242 is: 0.0495 with mean loss 0.0677 with runtime 21.2087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000243 is: 0.0496 with mean loss 0.0677 with runtime 20.9603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000244 is: 0.0498 with mean loss 0.0676 with runtime 20.9619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000245 is: 0.0494 with mean loss 0.0675 with runtime 21.2851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000246 is: 0.0494 with mean loss 0.0675 with runtime 20.8640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000247 is: 0.0498 with mean loss 0.0674 with runtime 21.2405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 39.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000248 is: 0.0493 with mean loss 0.0673 with runtime 21.1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 40.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000249 is: 0.0492 with mean loss 0.0672 with runtime 20.9117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:23<00:00, 35.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000250 is: 0.0495 with mean loss 0.0672 with runtime 23.7271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000251 is: 0.0492 with mean loss 0.0671 with runtime 21.4968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000252 is: 0.0492 with mean loss 0.0670 with runtime 20.9315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:20<00:00, 39.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000253 is: 0.0493 with mean loss 0.0670 with runtime 21.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000254 is: 0.0494 with mean loss 0.0669 with runtime 22.5765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:21<00:00, 38.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000255 is: 0.0489 with mean loss 0.0668 with runtime 21.6507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000256 is: 0.0494 with mean loss 0.0668 with runtime 22.3918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:22<00:00, 37.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000257 is: 0.0489 with mean loss 0.0667 with runtime 22.5165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 837/837 [00:23<00:00, 35.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss in epoch 0000258 is: 0.0489 with mean loss 0.0666 with runtime 23.8774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 730/837 [00:18<00:02, 40.65it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = z/0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLUFMZn2Juma",
        "outputId": "2a9a3cd5-0936-4b92-a649-20c7f56ded15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-52-3a5b6e98a3e3>\", line 1, in <cell line: 1>\n",
            "    z = z/0\n",
            "NameError: name 'z' is not defined\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1667, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1625, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-52-3a5b6e98a3e3>\", line 1, in <cell line: 1>\n",
            "    z = z/0\n",
            "NameError: name 'z' is not defined\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1667, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1625, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-52-3a5b6e98a3e3>\", line 1, in <cell line: 1>\n",
            "    z = z/0\n",
            "NameError: name 'z' is not defined\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1667, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1625, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gkiNNy7W6f-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cC16z0va6gAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5PJNvLD86gC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iXbodabU6gFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQVPgLM5vEYb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nSj0xibvEh1"
      },
      "source": [
        "# Old"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxPXpSLLaAdP"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BouSF2CGWIFI"
      },
      "outputs": [],
      "source": [
        "G = nx.star_graph(10)\n",
        "\n",
        "nx.draw(G, with_labels=True, node_color='lightblue', node_size=800, font_weight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8mltcfQ1oyq"
      },
      "outputs": [],
      "source": [
        "def get_weights(g):\n",
        "  edge_indices = g.x[:,0] < 0.1\n",
        "  edge_weights = g.x[:,1]\n",
        "  return edge_weights[edge_indices]\n",
        "\n",
        "def set_weights(g, w):\n",
        "  edge_indices = g.x[:,0] < 0.1\n",
        "  g.x[edge_indices,1] = w\n",
        "  return g\n",
        "\n",
        "def get_edge_index(g):\n",
        "  return g.x[:,0] < 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYttUAarWIbf"
      },
      "outputs": [],
      "source": [
        "def transform_graph(graph, make_complete=False):\n",
        "  # create graph where each edge becomes a node with weight 1\n",
        "  # if make_complete, each non-egde becomes node with weight -1\n",
        "\n",
        "  graph = nx.convert_node_labels_to_integers(graph, ordering=\"sorted\")\n",
        "\n",
        "  transformed_graph = nx.Graph()\n",
        "\n",
        "  for u, v in graph.edges():\n",
        "    new_node = (u+v)*1000000+100*(min(u,v)+1)+10000000000*max(u,v)\n",
        "    transformed_graph.add_edge(u, new_node)\n",
        "    transformed_graph.add_edge(new_node, v)\n",
        "    transformed_graph.nodes[u]['is_real'] = 1\n",
        "    transformed_graph.nodes[v]['is_real'] = 1\n",
        "    transformed_graph.nodes[new_node]['is_real'] = 0\n",
        "    transformed_graph.nodes[u]['weight'] = 0.0\n",
        "    transformed_graph.nodes[v]['weight'] = 0.0\n",
        "    transformed_graph.nodes[new_node]['weight'] = EDGE_INDICATOR\n",
        "\n",
        "\n",
        "  if make_complete:\n",
        "    for u in graph.nodes():\n",
        "      for v in graph.nodes():\n",
        "        if u > v and not graph.has_edge(u, v) and not graph.has_edge(v, u):\n",
        "          new_node = (u+v)*1000000+100*(min(u,v)+1)+10000000000*max(u,v)+10\n",
        "          transformed_graph.add_edge(u, new_node)\n",
        "          transformed_graph.add_edge(new_node, v)\n",
        "          transformed_graph.nodes[u]['is_real'] = 1\n",
        "          transformed_graph.nodes[v]['is_real'] = 1\n",
        "          transformed_graph.nodes[new_node]['is_real'] = 0\n",
        "          transformed_graph.nodes[new_node]['weight'] = NO_EDGE_INDICATOR\n",
        "\n",
        "  transformed_graph = nx.convert_node_labels_to_integers(transformed_graph, ordering=\"sorted\")\n",
        "  return transformed_graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdUpdWYpM99N"
      },
      "outputs": [],
      "source": [
        "def transform_to_complete_graph(graph):\n",
        "  global EDGE_INDEX_STORAGE\n",
        "  # create graph where each edge becomes a node with weight 1\n",
        "  # if make_complete, each non-egde becomes node with weight -1\n",
        "\n",
        "  graph_node_num = graph.number_of_nodes()\n",
        "  number_nodes_in_transformed_graph = graph_node_num*(graph_node_num+1)/2 #+ graph_node_num\n",
        "  graph = nx.convert_node_labels_to_integers(graph, ordering=\"sorted\")\n",
        "\n",
        "  transformed_graph = nx.Graph()\n",
        "  nodes = range(graph_node_num)\n",
        "\n",
        "  for u in nodes:\n",
        "    for v in nodes:\n",
        "      if u>=v:\n",
        "        continue\n",
        "      new_node = (u+v+1)*1000000+100*(min(u,v)+1)+10000000000*max(u,v)+10+u+20*(v+1)\n",
        "      transformed_graph.add_edge(u, new_node)\n",
        "      transformed_graph.add_edge(new_node, v)\n",
        "      transformed_graph.nodes[u]['is_real'] = 1\n",
        "      transformed_graph.nodes[v]['is_real'] = 1\n",
        "      transformed_graph.nodes[new_node]['is_real'] = 0\n",
        "\n",
        "      transformed_graph.nodes[u]['weight'] = 0.0\n",
        "      transformed_graph.nodes[v]['weight'] = 0.0\n",
        "      if graph.has_edge(u, v) or graph.has_edge(v, u):\n",
        "        transformed_graph.nodes[new_node]['weight'] = EDGE_INDICATOR\n",
        "      else:\n",
        "        transformed_graph.nodes[new_node]['weight'] = NO_EDGE_INDICATOR\n",
        "\n",
        "  transformed_graph = nx.convert_node_labels_to_integers(transformed_graph, ordering=\"sorted\")\n",
        "  assert(number_nodes_in_transformed_graph == transformed_graph.number_of_nodes())\n",
        "  return transformed_graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAoIITTOWhCp"
      },
      "outputs": [],
      "source": [
        "# Transform the graph\n",
        "transformed_G = transform_graph(G)\n",
        "\n",
        "# Visualize the transformed graph\n",
        "node_color = ['lightblue' if transformed_G.nodes[node]['is_real'] == 1 else 'gray' for node in transformed_G.nodes()]\n",
        "\n",
        "nx.draw(transformed_G, with_labels=True, node_size=800, font_weight='bold', node_color=node_color)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10mP4uXrWkU3"
      },
      "outputs": [],
      "source": [
        "G = nx.star_graph(3)\n",
        "\n",
        "\n",
        "# Transform the graph\n",
        "transformed_G  = transform_graph(G, make_complete=True)\n",
        "#transformed_G  = transform_to_complete_graph(G)\n",
        "\n",
        "# Visualize the transformed graph\n",
        "def get_node_color(g, v):\n",
        "  if g.nodes[v]['is_real'] == 1:\n",
        "    return \"lightblue\"\n",
        "  if g.nodes[v]['weight'] > 0.5:\n",
        "    return \"gray\"\n",
        "  return \"lightgray\"\n",
        "\n",
        "node_color = [get_node_color(transformed_G, node) for node in transformed_G.nodes()]\n",
        "\n",
        "nx.draw(transformed_G, with_labels=True, node_size=800, font_weight='bold', node_color=node_color)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uoDR578Yg4N"
      },
      "outputs": [],
      "source": [
        "G = nx.star_graph(3)\n",
        "transformed_G  = transform_graph(G, make_complete=True)\n",
        "transformed_G2 = transform_to_complete_graph(G)\n",
        "\n",
        "for v_i in transformed_G.nodes:\n",
        "  print(transformed_G.nodes(data=True)[v_i])\n",
        "  print(transformed_G2.nodes(data=True)[v_i])\n",
        "  print(\" \")\n",
        "\n",
        "#transformed_G.nodes(data=True), transformed_G2.nodes(data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFpC_zhrqtxf"
      },
      "outputs": [],
      "source": [
        "transformed_G.nodes(data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apjut4wbVfge"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SIEZLYgUDQg"
      },
      "outputs": [],
      "source": [
        "g = from_networkx(transformed_G, group_node_attrs=[\"is_real\", \"weight\"])\n",
        "g, g.x, g.edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGLwThw_EMzU"
      },
      "outputs": [],
      "source": [
        "class ShuffleList:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.index = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        random.shuffle(self.data)\n",
        "        self.index = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.index < len(self.data):\n",
        "            value = self.data[self.index]\n",
        "            self.index += 1\n",
        "            return value\n",
        "        else:\n",
        "            raise StopIteration\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RsTVhBtFMfG"
      },
      "source": [
        "#### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C828QNI-FN0j"
      },
      "outputs": [],
      "source": [
        "def remove_edges(graph, threshold = 0.0):\n",
        "    # Create a deep copy of the graph\n",
        "    new_graph = copy.deepcopy(graph)\n",
        "\n",
        "    # List to store edges to be removed\n",
        "    edges_to_remove = []\n",
        "\n",
        "    # Find edges with weight < 0.5\n",
        "    for u, v, data in new_graph.edges(data=True):\n",
        "        if 'weight' in data and data['weight'] < threshold:\n",
        "            edges_to_remove.append((u, v))\n",
        "\n",
        "    # Remove edges\n",
        "    for edge in edges_to_remove:\n",
        "        new_graph.remove_edge(*edge)\n",
        "\n",
        "    return new_graph\n",
        "\n",
        "def reduce_nx_graph(g_old):\n",
        "  g_new = nx.Graph()\n",
        "  for v_i in g_old.nodes():\n",
        "    if g_old.nodes[v_i]['x'][0] > 0.1:\n",
        "      g_new.add_node(v_i)\n",
        "      g_new.nodes[v_i]['x'] = g_old.nodes[v_i]['x']\n",
        "\n",
        "  for v_i in g_old.nodes():\n",
        "    if g_old.nodes[v_i]['x'][0] < 0.1:\n",
        "      neigh_list = list(g_old.neighbors(v_i))\n",
        "      if len(neigh_list) != 2:\n",
        "        print(neigh_list)\n",
        "      assert(len(neigh_list) == 2)\n",
        "      g_new.add_edge(neigh_list[0], neigh_list[1], weight = g_old.nodes[v_i]['x'][1])\n",
        "  return g_new\n",
        "\n",
        "def pyg_graph_to_nx(g_pyg, edge_weights):\n",
        "  g_pyg = g_pyg.clone()\n",
        "\n",
        "  edge_indices = g_pyg.x[:,0] < 0.1\n",
        "  g_pyg.x[edge_indices,1] = edge_weights\n",
        "\n",
        "  g_nx = to_networkx(g_pyg, node_attrs=['x'], to_undirected=True)\n",
        "  g_nx = reduce_nx_graph(g_nx)\n",
        "  return g_nx\n",
        "\n",
        "def pyg_to_sparsebinary_nx(g_pyg):\n",
        "  edge_indices = g_pyg.x[:,0] < 0.1\n",
        "  edge_weights = g_pyg.x[edge_indices,1]\n",
        "\n",
        "  # Assuming pyg_graph_to_nx and remove_edges are defined or imported correctly in your script\n",
        "  g_nx = pyg_graph_to_nx(g_pyg, edge_weights)\n",
        "  g_nx = remove_edges(g_nx, threshold = 0.0)\n",
        "\n",
        "  for edge in g_nx.edges:\n",
        "    del g_nx.edges[edge]['weight']\n",
        "  return g_nx\n",
        "\n",
        "\n",
        "def plot_weighted_graph(edge_weights, g_pyg, ax, pos=None, binarize=False):\n",
        "  if edge_weights is None:\n",
        "    edge_indices = g_pyg.x[:,0] < 0.1\n",
        "    edge_weights = g_pyg.x[edge_indices,1]\n",
        "\n",
        "  g_nx = pyg_graph_to_nx(g_pyg, edge_weights)\n",
        "\n",
        "  edge_weights = nx.get_edge_attributes(g_nx, 'weight')\n",
        "  edge_weights = [edge_weights[e] for e in g_nx.edges]\n",
        "\n",
        "  if binarize:\n",
        "    edge_weights = [1.0 if w>0.0 else -1.0 for w in edge_weights]\n",
        "    edge_weights_for_mean = [max(w,0.) for w in edge_weights]\n",
        "    print(\"Mean degree:\", 2*np.sum(edge_weights_for_mean)/g_nx.number_of_nodes())\n",
        "\n",
        "  if pos is None:\n",
        "    g_nx_sparse = remove_edges(g_nx)\n",
        "    pos = nx.spring_layout(g_nx_sparse)\n",
        "  edge_colors = [max(w, -1.) for w in edge_weights]\n",
        "  edge_colors = [min(w, 1.) for w in edge_colors]\n",
        "  edge_colors = [w/2.0+0.5 for w in edge_colors]\n",
        "\n",
        "  for i, e in enumerate(g_nx.edges):\n",
        "    nx.draw(g_nx, pos, edge_color=\"black\", edgelist = [e], ax=ax, alpha=edge_colors[i], nodelist=list())\n",
        "\n",
        "  nx.draw(g_nx, pos,\n",
        "          node_color='red', with_labels=False, ax=ax, alpha=0.5, node_size=4, edgelist=list())\n",
        "\n",
        "  if binarize:\n",
        "    return pos, g_nx_sparse\n",
        "  return pos, g_nx\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxLa9eUNYWCg"
      },
      "source": [
        "## Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_-L-Pc3Y1Zp"
      },
      "outputs": [],
      "source": [
        "def lift_nx_to_pyg(g):\n",
        "  g = from_networkx(g, group_node_attrs=[\"is_real\", \"weight\"])\n",
        "  return g\n",
        "\n",
        "EDGE_INDEX_STORAGE = dict()\n",
        "def lift_nx_to_complete_pyg(g):\n",
        "  global EDGE_INDEX_STORAGE\n",
        "  #g = transform_to_complete_graph(g)\n",
        "  g = transform_graph(g, make_complete=True)\n",
        "  g = lift_nx_to_pyg(g)\n",
        "  node_num_lifted = g.x.shape[0]\n",
        "  if node_num_lifted not in EDGE_INDEX_STORAGE:\n",
        "    EDGE_INDEX_STORAGE[node_num_lifted] = g.edge_index\n",
        "  #assert(torch.all(EDGE_INDEX_STORAGE[node_num_lifted] == g.edge_index))\n",
        "  return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TUFSCq4Ye2y"
      },
      "outputs": [],
      "source": [
        "def build_dataset(num_nodes=NUM_NODES, num_samples=NUM_SAMPLES, degree=DEGREE, seed=1234):\n",
        "  global EDGE_INDEX_STORAGE\n",
        "\n",
        "  try:\n",
        "    with open(f\"dataset_{NUM_NODES:07}_{NUM_SAMPLES:07}_{DEGREE:03}.pickle\", \"rb\") as f:\n",
        "      tain_set, test_set = pickle.load(f)\n",
        "      print(f\"found dataset: dataset_{NUM_NODES:07}_{NUM_SAMPLES:07}_{DEGREE:03}.pickle\")\n",
        "      return tain_set, test_set\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "  dataset = list()\n",
        "\n",
        "  for _ in range(num_samples):\n",
        "    while True:\n",
        "      seed += 1\n",
        "      graph = nx.random_regular_graph(d=degree, n=num_nodes, seed=seed)\n",
        "      if nx.is_connected(graph):\n",
        "        dataset.append(lift_nx_to_complete_pyg(graph))\n",
        "        break\n",
        "\n",
        "  dataset_train = dataset[:int(len(dataset)*TRAIN_TEST_SPLIT)]\n",
        "  dataset_test = dataset[int(len(dataset)*TRAIN_TEST_SPLIT):]\n",
        "  dataset_train = ShuffleList(dataset_train)\n",
        "  dataset_test = ShuffleList(dataset_test)\n",
        "\n",
        "  with open(f\"dataset_{NUM_NODES:07}_{NUM_SAMPLES:07}_{DEGREE:03}.pickle\", \"wb\") as f:\n",
        "    pickle.dump((dataset_train, dataset_test), f)\n",
        "  return dataset_train, dataset_test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LpCfVUcP7Rq"
      },
      "outputs": [],
      "source": [
        "#!rm dataset_*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU0cHznL2bjQ"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test  = build_dataset()\n",
        "g1 = dataset_train[0]\n",
        "g2 = dataset_train[1]\n",
        "g3 = dataset_train[2]\n",
        "print(g1.edge_index)\n",
        "print(g2.edge_index)\n",
        "print(g3.edge_index)\n",
        "#torch.all(g1.edge_index == g2.edge_index) and torch.all(g2.edge_index == g3.edge_index)\n",
        "# For this line \"g1.edge_index == g2.edge_index and g2.edge_index == g3.edge_index\", I get the error \"Boolean value of Tensor with more than one value is ambiguous\". Fix it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNhDqcUcYfKV"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test  = build_dataset()\n",
        "data = dataset_train[0]\n",
        "data, data.x\n",
        "#data.edge_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE8eu8YHxYSb"
      },
      "source": [
        "# Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5gRUpdExZnl"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import PNA\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "\n",
        "def dataset_to_degree_bin(train_dataset):\n",
        "  # Compute the maximum in-degree in the training data.\n",
        "  max_degree = -1\n",
        "  for data in train_dataset:\n",
        "    data = data.to(DEVICE)\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    max_degree = max(max_degree, int(d.max()))\n",
        "\n",
        "  deg = torch.zeros(max_degree + 1, dtype=torch.long, device=DEVICE)\n",
        "  for data in train_dataset:\n",
        "    data = data.to(DEVICE)\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    deg += torch.bincount(d, minlength=deg.numel())\n",
        "  return deg\n",
        "\n",
        "class PNAnet(torch.nn.Module):\n",
        "  def __init__(self, train_dataset, hidden_channels=32, depth=4, dropout=0.05, towers=1, normalization=True, pre_post_layers=1):\n",
        "    super(PNAnet, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Calculate x as the difference between mult_y and hidden_dim\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1) #tod fix\n",
        "    #out_channels = towers * ((out_channels // towers) + 1)\n",
        "\n",
        "    in_channels = 3\n",
        "    deg = dataset_to_degree_bin(train_dataset)\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "    self.pnanet = PNA(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=hidden_channels, num_layers=depth, aggregators=aggregators, scalers=scalers, deg=deg, dropout=dropout, towers=towers, norm=self.normalization, pre_layers=pre_post_layers, post_layers=pre_post_layers)\n",
        "\n",
        "    self.final_mlp = Seq(Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, 1))\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    x = self.pnanet(x, edge_index)\n",
        "    x = self.final_mlp(x)\n",
        "    assert(x.numel() > 1 )\n",
        "    return x\n",
        "\n",
        "\n",
        "model = PNAnet([data])\n",
        "\n",
        "#model(data.x, data.edge_index, torch.ones(data.x.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyUalHA2Ehyn"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wmE0o9liKb8"
      },
      "outputs": [],
      "source": [
        "def generate_schedule(start = START, end = END, timesteps=TIMESTEPS):\n",
        "    \"\"\"\n",
        "    Generates a schedule of beta and alpha values for a forward process.\n",
        "\n",
        "    Args:\n",
        "    start (float): The starting value for the beta values. Default is START.\n",
        "    end (float): The ending value for the beta values. Default is END.\n",
        "    timesteps (int): The number of timesteps to generate. Default is TIMESTEPS.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple of three tensors containing the beta values, alpha values, and\n",
        "    cumulative alpha values (alpha bars).\n",
        "    \"\"\"\n",
        "    betas = torch.linspace(start, end, timesteps, device = DEVICE)\n",
        "    alphas = 1.0 - betas\n",
        "    alpha_bars = torch.cumprod(alphas, axis=0)\n",
        "    assert(betas.numel() == TIMESTEPS)\n",
        "    return betas, alphas, alpha_bars\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZkmSsNfE41g"
      },
      "outputs": [],
      "source": [
        "def compute_generation_loss(graphs, train_loader):\n",
        "  #graphs = [remove_edges(g) for g in graphs]\n",
        "  loss_list = list()\n",
        "  for graph in graphs:\n",
        "    degree_list = [graph.degree(i) for i in graph.nodes()]\n",
        "    mean_degree = np.mean(degree_list)\n",
        "    var_degree = np.var(degree_list)\n",
        "    loss = (3.0-mean_degree)**2 + var_degree\n",
        "    loss_list.append(loss)\n",
        "  return np.mean(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3CMj8TyDOGV"
      },
      "source": [
        "### Load checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPWqWq5oE9n5"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, graph_loss_list, loss_list, epoch_i):\n",
        "  if epoch_i == 0:\n",
        "    return\n",
        "  save_path = f\"model_epoch_{epoch_i:08}.pth\"\n",
        "\n",
        "  # Save the model state dict and the optimizer state dict in a dictionary\n",
        "  torch.save({\n",
        "              'epoch': epoch_i,\n",
        "              'loss_list': loss_list,\n",
        "              'graph_loss_list': graph_loss_list,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict()\n",
        "              }, save_path)\n",
        "\n",
        "def load_latest_checkpoint(model, optimizer, graph_loss_list, loss_list, epoch_i):\n",
        "  try:\n",
        "    checkpoint_paths = sorted(glob.glob(\"model_epoch_*.pth\"))\n",
        "    if len(checkpoint_paths) == 0:\n",
        "      return model, optimizer, graph_loss_list, loss_list, epoch_i\n",
        "\n",
        "    latest_checkpoint_path = checkpoint_paths[-1]\n",
        "    checkpoint = torch.load(latest_checkpoint_path)\n",
        "\n",
        "    # Assuming model and optim are your initialized model and optimizer\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch_i = checkpoint['epoch']\n",
        "    graph_loss_list = checkpoint['graph_loss_list']\n",
        "    print(f\"read checkpoint of epoch {epoch_i:08} from disc.\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  return model, optimizer, graph_loss_list, loss_list, epoch_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFOtSm7-qwBg"
      },
      "outputs": [],
      "source": [
        "#!ls\n",
        "#!ls ../Toad_relaxed_large5/model_epoch_*\n",
        "#!cp ../Toad_relaxed_large5/model_epoch_00020000.pth model_epoch_00020000.pth\n",
        "#!ls model_epoch_00020000*\n",
        "#!ls\n",
        "#!cp model_epoch_00020000.pth ../Alia1/model_epoch_00020000.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHMwkfUXEk3G"
      },
      "source": [
        "### Forward Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPtXnyVpElAc"
      },
      "outputs": [],
      "source": [
        "def forward_diffusion(input_vec, future_t, betas):\n",
        "  \"\"\"\n",
        "  Performs a forward diffusion process on an input image tensor.\n",
        "  Implements the second equation from https://youtu.be/a4Yfz2FxXiY?t=649\n",
        "  \"\"\"\n",
        "  assert(input_vec.shape == future_t.shape)\n",
        "  #future_t = torch.tensor([future_t], dtype=torch.int64)\n",
        "\n",
        "  noise = torch.randn_like(input_vec, device=DEVICE)\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t)\n",
        "\n",
        "  img_mean = torch.sqrt(alphabar_t) * input_vec\n",
        "  img_std = torch.sqrt(1.-alphabar_t)\n",
        "  noise_img = img_mean + img_std * noise\n",
        "\n",
        "  return noise_img, noise\n",
        "\n",
        "betas, alphas, alpha_bars = generate_schedule()\n",
        "forward_diffusion(torch.tensor([1,2,3.], device=DEVICE), torch.tensor([0,0,0], device=DEVICE), betas), forward_diffusion(torch.tensor([1,2,3.], device=DEVICE), torch.tensor([999,999,999], device=DEVICE), betas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6sk4-QGJAUX"
      },
      "outputs": [],
      "source": [
        "betas, alphas, alpha_bars = generate_schedule()\n",
        "plt.clf()\n",
        "plt.plot(betas.cpu(), label=\"betas\", alpha=0.7)\n",
        "plt.plot(alphas.cpu(), label=\"alphas\", alpha=0.7, ls='--')\n",
        "plt.plot(torch.sqrt(alphas.cpu()), label=\"sqrt(alphas)\", alpha=0.7, ls=':')\n",
        "plt.plot(alpha_bars.cpu(), label=\"alpha_bars\", alpha=0.7)\n",
        "plt.plot((1-alphas.cpu())/(torch.sqrt(1-alpha_bars.cpu())), label=\"1-alpha/(sqrt...)\", alpha=0.3, lw=4)\n",
        "plt.legend()\n",
        "plt.savefig(\"schedule.png\")\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9OLoSsXiYEZ"
      },
      "outputs": [],
      "source": [
        "def test_forward(num=1000000):\n",
        "  g = dataset_train[0]\n",
        "  try:\n",
        "      with open(\"edge_weights_simple_example.pickle\", \"rb\") as f:\n",
        "          edge_weights = pickle.load(f)\n",
        "  except:\n",
        "      edge_weights = torch.tensor([random.choice([NO_EDGE_INDICATOR, EDGE_INDICATOR]) for _ in range(num)])\n",
        "      with open(\"edge_weights_simple_example.pickle\", \"wb\") as f:\n",
        "          pickle.dump(edge_weights, f)\n",
        "\n",
        "  #print(edge_weights)\n",
        "  edge_weights = edge_weights.to(DEVICE)\n",
        "  #betas.to(DEVICE)\n",
        "  future_t = torch.tensor([999]*len(edge_weights), device=DEVICE)\n",
        "  edge_weights, noise_gt = forward_diffusion(edge_weights, future_t, betas)\n",
        "  edge_weights.tolist()\n",
        "\n",
        "  #print(edge_weights)\n",
        "  sns.kdeplot(edge_weights.cpu())\n",
        "\n",
        "\n",
        "  #Gaussian\n",
        "  from scipy.stats import norm\n",
        "  standard_normal_pdf = lambda x: norm(0, 1).pdf(x)\n",
        "  x_values = np.linspace(-4,4,100)\n",
        "  plt.scatter(x_values, [standard_normal_pdf(x) for x in x_values], c='black', alpha=0.5, marker='x', edgecolors=None, label='Gaussian')\n",
        "test_forward(num=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6hzSAZK0h7q"
      },
      "source": [
        "### Train epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "QEN_3s2zlss7"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, schedule):\n",
        "  \"\"\"\n",
        "  Trains a denoising model for one epoch using a given data loader and optimization algorithm.\n",
        "\n",
        "  Args:\n",
        "  model: The denoising model.\n",
        "  dataloader: The data loader for the training data.\n",
        "  optimizer: The torch optimizer.\n",
        "  schedule (torch.Tensor): The schedule of beta values.\n",
        "\n",
        "  Returns:\n",
        "  float: The average loss for the epoch.\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  start_time = time.time()\n",
        "  loss_list = list() #TODO differnt future_t for all graphs\n",
        "\n",
        "  for g in dataloader:\n",
        "    g.to(DEVICE)\n",
        "    #print(\"batch\", g.batch)\n",
        "    num_graphs_in_batch = int(torch.max(g.batch).item()+1)\n",
        "    future_t_select = torch.randint(0, TIMESTEPS, (num_graphs_in_batch,), device = DEVICE)\n",
        "    future_t = torch.tensor([0]) #torch.gather(future_t_select, 0, g.batch)\n",
        "    #print(\"future_t\", future_t)\n",
        "    assert(future_t.numel() == g.x.shape[0])\n",
        "\n",
        "    #return 0/0\n",
        "    #future_t = torch.randint(0, TIMESTEPS, (1,), device = DEVICE)\n",
        "\n",
        "    edge_indices = g.x[:,0] < 0.1\n",
        "    edge_weights = g.x[:,1].clone()\n",
        "    edges_with_noise, noise_gt = forward_diffusion(edge_weights[edge_indices], future_t[edge_indices], schedule)\n",
        "    edge_weights[edge_indices] = edges_with_noise\n",
        "    #future_t_vec = future_t.repeat(edge_weights.numel())\n",
        "\n",
        "    #print(\"x in train \", g.x[:,0].shape, edge_weights.shape, future_t_vec.shape)\n",
        "    x_in = torch.concat((g.x[:,0].view(-1,1), edge_weights.view(-1,1), future_t.view(-1,1)), dim=1)\n",
        "    assert(x_in.shape[0] == g.x.shape[0])\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    noise_pred = model(x_in, g.edge_index)\n",
        "    noise_pred = noise_pred[edge_indices].flatten()\n",
        "\n",
        "\n",
        "    loss = F.mse_loss(noise_gt, noise_pred)\n",
        "    loss.backward()\n",
        "    loss_list.append(loss.item())\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  return np.mean(loss_list), time.time()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "7pld6IY2GK2Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "bab850a0-3705-4d03-fcc1-9593a4005291"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab/AliaMolecule/Alia/smiles_to_pyg/qm9_as_graphs_from_0_to_-1.pickle not found. Creating it now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 39480/133884 [02:43<06:31, 241.16it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-e2f32b6d3dcf>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_bars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mvisualize_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-7f2d9364c5a7>\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mAlia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmiles_to_pyg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmolecule_load_and_convert3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_qm9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_qm9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mTRAIN_TEST_SPLIT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/colab/AliaMolecule/Alia/smiles_to_pyg/molecule_load_and_convert3.py\u001b[0m in \u001b[0;36mread_qm9\u001b[0;34m(start, end)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0msmiles_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_canonical_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmiles_to_pyg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/colab/AliaMolecule/Alia/smiles_to_pyg/molecule_load_and_convert3.py\u001b[0m in \u001b[0;36msmiles_to_pyg\u001b[0;34m(smiles_mol)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0mpyg_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_node_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0mpyg_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyg_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyg_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/convert.py\u001b[0m in \u001b[0;36mfrom_networkx\u001b[0;34m(G, group_node_attrs, group_edge_attrs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_node_labels_to_integers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_directed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_directed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiDiGraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36mto_directed\u001b[0;34m(self, as_view)\u001b[0m\n\u001b[1;32m   1696\u001b[0m         \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1698\u001b[0;31m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1699\u001b[0m         G.add_edges_from(\n\u001b[1;32m   1700\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/classes/digraph.py\u001b[0m in \u001b[0;36madd_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \"\"\"\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes_for_adding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0mnewnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1696\u001b[0m         \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1698\u001b[0;31m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1699\u001b[0m         G.add_edges_from(\n\u001b[1;32m   1700\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__deepcopy__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;31m# TODO: skipping storage copy is wrong for meta, as meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# does accurate alias tracking; however, the code below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \"\"\"\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def visualize_forward(schedule, g, steps=10):\n",
        "  plt.close()\n",
        "  future_t_index = np.linspace(0, TIMESTEPS-0.5, steps).astype(int)\n",
        "\n",
        "  fig, axes = plt.subplots(1, 10, figsize=(20, 2))\n",
        "  pos = None\n",
        "\n",
        "  for i, future_t in enumerate(future_t_index):\n",
        "    future_t = torch.tensor(future_t, device=DEVICE)\n",
        "    edge_indices = g.x[:,0] < 0.1\n",
        "    edge_weights = g.x[:,1].clone()\n",
        "    edges_with_noise, noise_gt = forward_diffusion(edge_weights[edge_indices], future_t.repeat(len(edge_weights[edge_indices])), schedule)\n",
        "    if i == 0:\n",
        "      edges_with_noise = edge_weights[edge_indices]\n",
        "    pos, g_nx = plot_weighted_graph(edges_with_noise, g, axes[i], pos=pos)\n",
        "    axes[i].axis('off')  # to hide the axis\n",
        "  plt.savefig(\"example_foward.png\")\n",
        "\n",
        "\n",
        "dataset_train, dataset_test = build_dataset()\n",
        "betas, alphas, alpha_bars = generate_schedule()\n",
        "visualize_forward(betas, dataset_train[0].to(DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZXAFwdYr--b"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suojddTWtx-8"
      },
      "outputs": [],
      "source": [
        "def denoise_one_step(model, g, i, betas):\n",
        "  edge_indices = g.x[:,0] < 0.1\n",
        "  t = TIMESTEPS - i - 1\n",
        "\n",
        "  beta_t = betas[t]\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphas_cumprod_t = alphas_cumprod[t]\n",
        "  sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1. - alphas_cumprod_t)\n",
        "  sqrt_recip_alphas_t = torch.sqrt(1.0 / alphas[t])\n",
        "  t_tensor = torch.tensor([float(t)], device=DEVICE)\n",
        "\n",
        "  t_vec = t_tensor.repeat(g.x.shape[0],1)\n",
        "  x_in = torch.concat((g.x, t_vec), dim=1)\n",
        "  noise_pred = model(x_in, g.edge_index)\n",
        "  noise_pred = noise_pred[edge_indices].flatten()\n",
        "\n",
        "  # extract edge weights\n",
        "\n",
        "  values_in = g.x[edge_indices,1]\n",
        "\n",
        "  # actual denoising\n",
        "  model_mean = sqrt_recip_alphas_t * (values_in - beta_t * noise_pred / sqrt_one_minus_alphas_cumprod_t)\n",
        "  alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "  posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) # in the paper this is in 3.2. note that sigma^2 is variance, not std\n",
        "  posterior_variance_t = posterior_variance[t]\n",
        "  posterior_std_t = torch.sqrt(posterior_variance_t)\n",
        "\n",
        "  if t == 0:\n",
        "    return model_mean\n",
        "  else:\n",
        "    noise = torch.randn_like(values_in, device = DEVICE)\n",
        "    return model_mean + posterior_std_t * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm7itEUUJtNI"
      },
      "outputs": [],
      "source": [
        "def model_inference(model, g, t):\n",
        "  future_t = torch.tensor(t, device=DEVICE).repeat(g.x.shape[0])\n",
        "  x_in = torch.concat((g.x.view(-1,2), future_t.view(-1,1)), dim=1)\n",
        "\n",
        "  # prediction\n",
        "  try:\n",
        "    batch = g.batch\n",
        "  except:\n",
        "    batch = None #torch.zeros(g.x.shape[0], dtype=torch.long, device=D)\n",
        "  prediction = model(x_in, g.edge_index, batch=batch)\n",
        "  #print(g, prediction, batch)\n",
        "  if g.batch is None or torch.max(g.batch) > 0:\n",
        "    return prediction.cpu().numpy()\n",
        "  return prediction.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcuFV1JMFjFu"
      },
      "outputs": [],
      "source": [
        "def make_choice(choice_list, scores):\n",
        "    assert len(scores) == len(choice_list)\n",
        "\n",
        "    batches = choice_list[0].batch.tolist()\n",
        "    best_graph_for_batch = []\n",
        "\n",
        "    for b_i in batches:\n",
        "        s_i = [s[b_i] for s in scores]\n",
        "        best_graph_for_batch.append(np.argmax(s_i))\n",
        "\n",
        "    g1 = choice_list[0]\n",
        "\n",
        "    for i in range(g1.x.shape[0]):\n",
        "        batch_i = g1.batch[i]\n",
        "        best_graph_in_i = best_graph_for_batch[batch_i]\n",
        "        best_graph = choice_list[best_graph_in_i]\n",
        "        g1.x[i, :] = best_graph.x[i, :]#.clone()\n",
        "\n",
        "    return g1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ktPawiOMvUS"
      },
      "outputs": [],
      "source": [
        "def make_choice_sparse(g, choice_list, scores):\n",
        "    assert len(scores) == len(choice_list)\n",
        "\n",
        "    batches = g.batch.tolist()\n",
        "    best_graph_for_batch = []\n",
        "\n",
        "    for b_i in batches:\n",
        "        s_i = [s[b_i].cpu() for s in scores]\n",
        "        best_graph_for_batch.append(np.argmax(s_i))\n",
        "\n",
        "    for i in range(g.x.shape[0]):\n",
        "        if g.x[i,0] > 0.1:\n",
        "          continue\n",
        "        batch_i = g.batch[i]\n",
        "        best_graph_in_i = best_graph_for_batch[batch_i]\n",
        "        best_weights = choice_list[best_graph_in_i]\n",
        "        g.x[i, 1] = best_weights[i]#.clone()\n",
        "\n",
        "    return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxx6aU2UuvLa"
      },
      "outputs": [],
      "source": [
        "def overwrite_with_noise(g):\n",
        "  edge_indices = g.x[:,0] < 0.1\n",
        "  #edge_weights = g.x[:,1]\n",
        "  #edges_without_noise = edge_weights[edge_indices]\n",
        "  g.x[edge_indices,1] = torch.randn_like(g.x[edge_indices,1], device=DEVICE)\n",
        "  return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pda5tncJFKP"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def generate_examples_batchedX(model, betas, dataset_train, model_guide, num=100, choices=2):\n",
        "  print(\"generate samples batched\")\n",
        "  model.eval()\n",
        "  if model_guide is None:\n",
        "    choices=1\n",
        "  else:\n",
        "    model_guide = model_guide.to(DEVICE)\n",
        "    model_guide.eval()\n",
        "\n",
        "  dataset_train_start = list()\n",
        "  while len(dataset_train_start) < num:\n",
        "    g = dataset_train[random.sample(range(len(dataset_train)),1)[0]]\n",
        "    dataset_train_start.append(g.clone().to(DEVICE))\n",
        "  assert(len(dataset_train_start) == num)\n",
        "  dataloader = DataLoader(dataset_train_start, batch_size = num)\n",
        "  for g in dataloader:\n",
        "    g = g.to(DEVICE)\n",
        "    print(\"load g\", g, g.batch)\n",
        "    edge_indices = g.x[:,0] < 0.1\n",
        "    g = overwrite_with_noise(g)\n",
        "    t_max = TIMESTEPS // GUIDE_FRACTION # where to start guideance  TODO\n",
        "\n",
        "    for i in tqdm(range(TIMESTEPS)):\n",
        "      t = int(TIMESTEPS-i-1)\n",
        "      if t <= t_max and model_guide is not None: # with guide\n",
        "\n",
        "        edges_with_less_noise1 = denoise_one_step(model, g, i, betas)\n",
        "        g_orig_w1 = g.x[:,1].clone().view(-1)\n",
        "        g_orig_w1[edge_indices] = edges_with_less_noise1\n",
        "\n",
        "        edges_with_less_noise2 = denoise_one_step(model, g, i, betas)\n",
        "        g_orig_w2 = g.x[:,1].clone().view(-1)\n",
        "        g_orig_w2[edge_indices] = edges_with_less_noise2\n",
        "\n",
        "        future_t = torch.tensor(t, device=DEVICE).repeat(g.x.shape[0])\n",
        "        x_in1 = torch.concat((g.x[:,0].view(-1,1), g_orig_w1.view(-1,1), future_t.view(-1,1)), dim=1)\n",
        "        x_in2 = torch.concat((g.x[:,0].view(-1,1), g_orig_w2.view(-1,1), future_t.view(-1,1)), dim=1)\n",
        "        scores1 = model_guide(x_in1, g.edge_index, batch=g.batch)\n",
        "        scores2 = model_guide(x_in2, g.edge_index, batch=g.batch)\n",
        "        g = make_choice_sparse(g, [g_orig_w1, g_orig_w2], [scores1, scores2])\n",
        "\n",
        "        if i == TIMESTEPS -1:\n",
        "          edges_denoised_binary = torch.where(g.x[edge_indices,1] > 0.0, 1., -1.)\n",
        "          g.x[edge_indices,1] = edges_denoised_binary\n",
        "      else: #without guide\n",
        "        edges_with_less_noise = denoise_one_step(model, g, i, betas)\n",
        "        g.x[edge_indices,1] = edges_with_less_noise\n",
        "        if i == TIMESTEPS -1:\n",
        "          print(\"edges_denoised_binary\", edges_denoised_binary)\n",
        "          edges_denoised_binary = torch.where(g.x[edge_indices,1] > 0.0, 1., -1.)\n",
        "          g.x[edge_indices,1] = edges_denoised_binary\n",
        "\n",
        "    graph_list = g.to_data_list()\n",
        "    #break\n",
        "\n",
        "    print(\"generated graphs \", graph_list)\n",
        "    return graph_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTwsjZHvfXgj"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def generate_examples_batched(model, betas, dataset_train, model_guide, num=100, choices=2):\n",
        "  print(\"generate samples batched\")\n",
        "\n",
        "  #model.eval()\n",
        "  assert(choices == 1 or choices == 2)\n",
        "  if model_guide is None:\n",
        "    choices=1\n",
        "  else:\n",
        "    model_guide = model_guide.to(DEVICE)\n",
        "    model_guide.eval()\n",
        "\n",
        "\n",
        "  dataset_train_start = list()\n",
        "  while len(dataset_train_start) < num:\n",
        "    g = dataset_train[random.sample(range(len(dataset_train)),1)[0]]\n",
        "    dataset_train_start.append(g.clone().to(DEVICE))\n",
        "  assert(len(dataset_train_start) == num)\n",
        "  dataloader = DataLoader(dataset_train_start, batch_size = num)\n",
        "  for g in dataloader:\n",
        "    print(\"load g\", g, g.batch)\n",
        "    edge_indices = g.x[:,0] < 0.1\n",
        "    g = overwrite_with_noise(g)\n",
        "    t_max = TIMESTEPS // GUIDE_FRACTION # where to start guideance  TODO\n",
        "\n",
        "    for i in tqdm(range(TIMESTEPS)):\n",
        "      t = int(TIMESTEPS-i-1)\n",
        "      if t <= t_max and model_guide is not None: # with guide\n",
        "        choice_list = list()\n",
        "        for j in range(choices):\n",
        "          if j == choices-1:\n",
        "            g_alt = g\n",
        "          else:\n",
        "            g_alt = g.clone()\n",
        "          edges_with_less_noise = denoise_one_step(model, g_alt, i, betas)\n",
        "          g_alt.x[edge_indices,1] = edges_with_less_noise\n",
        "          choice_list.append(g_alt)\n",
        "        if choices == 1:\n",
        "          g = choice_list[0]\n",
        "        else:\n",
        "          scores = [model_inference(model_guide, g, t) for g in choice_list] # todo add batching\n",
        "          g = make_choice(choice_list, scores)\n",
        "        if i == TIMESTEPS -1:\n",
        "          edges_denoised_binary = torch.where(g.x[edge_indices,1] > 0.0, 1., -1.)\n",
        "          g.x[edge_indices,1] = edges_denoised_binary\n",
        "      else: #without guide\n",
        "        edges_with_less_noise = denoise_one_step(model, g, i, betas)\n",
        "        g.x[edge_indices,1] = edges_with_less_noise\n",
        "        if i == TIMESTEPS -1:\n",
        "          #print(\"weights\",torch.mean(g.x[edge_indices,1]),g.x[edge_indices,1] )\n",
        "          edges_denoised_binary = torch.where(g.x[edge_indices,1] > 0.0, 1., -1.)\n",
        "          g.x[edge_indices,1] = edges_denoised_binary\n",
        "\n",
        "    graph_list = g.to_data_list()\n",
        "    #break\n",
        "\n",
        "    return graph_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuyPKETI9c-I"
      },
      "outputs": [],
      "source": [
        "def generate_examples_guided(model, betas, dataset_train, model_guide, num=100, choices=2):\n",
        "  print(\"generate samples with guidance\")\n",
        "  gen_set = list()\n",
        "  model_guide = model_guide.to(DEVICE)\n",
        "  tqdm_x = tqdm if num > 10 else lambda x: x\n",
        "  for i in tqdm_x(range(num)):\n",
        "    g = dataset_train[random.choice(range(len(dataset_train)))].clone().to(DEVICE)\n",
        "    edge_indices = g.x[:,0] < 0.1\n",
        "    edge_weights = g.x[:,1].clone()\n",
        "    edges_without_noise = edge_weights[edge_indices]\n",
        "    edges_with_noise = torch.randn_like(edges_without_noise, device=DEVICE)\n",
        "    g.x[edge_indices,1] = edges_with_noise\n",
        "\n",
        "    t_max = TIMESTEPS // GUIDE_FRACTION\n",
        "\n",
        "    for i in range(TIMESTEPS):\n",
        "      t = int(TIMESTEPS-i-1)\n",
        "      if t <= t_max:\n",
        "        choice_list = list()\n",
        "        for j in range(choices):\n",
        "          g_alt = g.clone()\n",
        "          edges_with_noise = denoise_one_step(model, g_alt, i, betas)\n",
        "          g_alt.x[edge_indices,1] = edges_with_noise\n",
        "          choice_list.append(g_alt)\n",
        "        scores = [model_inference(model_guide, g, t) for g in choice_list]\n",
        "        g = choice_list[np.argmax(scores)]\n",
        "        edges_with_noise = g.x[edge_indices,1] # relevant for last step\n",
        "      else:\n",
        "        edges_with_noise = denoise_one_step(model, g, i, betas)\n",
        "        g.x[edge_indices,1] = edges_with_noise\n",
        "\n",
        "    edges_denoised_binary = torch.where(edges_with_noise > 0.0, 1., -1.)\n",
        "\n",
        "    g.x[edge_indices,1] = edges_denoised_binary\n",
        "    gen_set.append(g)\n",
        "\n",
        "  return gen_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO-QiVKxEmAb"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def generate_examples_silent(model, betas, dataset_train, model_guide = None, num=100):\n",
        "  if model_guide is not None:\n",
        "    return generate_examples_guided(model, betas, dataset_train, model_guide, num=num)\n",
        "  print(\"generate samples without guidance\")\n",
        "  gen_set = list()\n",
        "  tqdm_x = tqdm if num > 10 else lambda x: x\n",
        "  for i in tqdm_x(range(num)):\n",
        "    g = dataset_train[random.choice(range(len(dataset_train)))].clone().to(DEVICE)\n",
        "    edge_indices = g.x[:,0] < 0.1\n",
        "    edge_weights = g.x[:,1].clone()\n",
        "    edges_without_noise = edge_weights[edge_indices]\n",
        "    edges_with_noise = torch.randn_like(edges_without_noise, device=DEVICE)\n",
        "    g.x[edge_indices,1] = edges_with_noise\n",
        "\n",
        "    for i in range(TIMESTEPS):\n",
        "      edges_with_noise = denoise_one_step(model, g, i, betas)\n",
        "      g.x[edge_indices,1] = edges_with_noise\n",
        "\n",
        "    edges_denoised_binary = torch.where(edges_with_noise > 0.0, 1., -1.)\n",
        "\n",
        "    g.x[edge_indices,1] = edges_denoised_binary\n",
        "    gen_set.append(g)\n",
        "\n",
        "  return gen_set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6OVKWQVp4lR"
      },
      "outputs": [],
      "source": [
        "#@torch.inference_mode()\n",
        "def generate_example(model, epoch_i, betas, dataset_train, steps=10, silent=False):\n",
        "  plt.close()\n",
        "  #model.eval()\n",
        "\n",
        "  g = dataset_train[random.choice(range(len(dataset_train)))].clone() # we sample to get a random number of node\n",
        "\n",
        "  edge_indices = g.x[:,0] < 0.1\n",
        "  edge_weights = g.x[:,1].clone()\n",
        "  edges_without_noise = edge_weights[edge_indices]\n",
        "  edges_with_noise = torch.randn_like(edges_without_noise, device=DEVICE)\n",
        "  g.x[edge_indices,1] = edges_with_noise\n",
        "\n",
        "  future_t_index = list(np.linspace(0, TIMESTEPS-0.5, steps).astype(int))\n",
        "\n",
        "  if not silent:\n",
        "    fig, axes = plt.subplots(1, steps, figsize=(20, 2))\n",
        "  else:\n",
        "    axes = range(steps)\n",
        "  pos = None\n",
        "\n",
        "  graphs_to_plot = list()\n",
        "\n",
        "  ax_count = -1\n",
        "  for i in range(TIMESTEPS):\n",
        "    if i == future_t_index[0]:\n",
        "      ax_count += 1\n",
        "      binarize = len(future_t_index) == 1\n",
        "      graphs_to_plot.append((edges_with_noise,g.clone(),axes[ax_count],binarize))\n",
        "      future_t_index.pop(0)\n",
        "\n",
        "    g.x[edge_indices,1] = edges_with_noise\n",
        "    edges_with_noise = denoise_one_step(model, g, i, betas)\n",
        "\n",
        "  # we want that pos is computed based on the final graph\n",
        "  graph_to_return = None\n",
        "  for (edges_with_noise,g,ax,binarize) in graphs_to_plot[::-1]:\n",
        "    if not silent:\n",
        "      pos, g_nx = plot_weighted_graph(edges_with_noise, g, ax, pos=pos, binarize=binarize)\n",
        "      ax.axis('off')\n",
        "      if binarize:\n",
        "        graph_to_return = g_nx\n",
        "\n",
        "  if not silent:\n",
        "    plt.savefig(f\"reverse_process_reference_epoch_{str(epoch_i).zfill(6)}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return graph_to_return\n",
        "\n",
        "  return graphs_to_plot[-1][1]\n",
        "  # save model\n",
        "\n",
        "\n",
        "\n",
        "def execute_function_times(function, num_executions, *args, **kwargs):\n",
        "  results = []\n",
        "  show = tqdm if num_executions>15 else lambda x:x\n",
        "  for _ in show(range(num_executions)):\n",
        "    result = function(*args, **kwargs)\n",
        "    results.append(result)\n",
        "  return results\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_examples(model, epoch_i, betas, dataset_train, num_times=NUM_GRAPHS_TO_GENERATE, silent=False):\n",
        "  return execute_function_times(generate_example, num_times, model, epoch_i, betas, dataset_train, silent=silent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdPFgj3nEx9_"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftwezkN8f_sS"
      },
      "outputs": [],
      "source": [
        "##path = \"model_epoch_00019900.pth\"\n",
        "#checkpoint = torch.load(path)\n",
        "#checkpoint.keys()\n",
        "#checkpoint[\"graph_loss_list\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSgh0o0H3Tv5"
      },
      "outputs": [],
      "source": [
        "def plot_base(graph_loss_list, loss_list):\n",
        "  plt.clf()\n",
        "  plt.plot(graph_loss_list)\n",
        "  plt.title(\"graph_loss_list\")\n",
        "  plt.savefig(\"train_base_graph_loss.png\")\n",
        "  plt.clf()\n",
        "  plt.plot(loss_list)\n",
        "  plt.title(\"loss_list\")\n",
        "  plt.savefig(\"train_base_train_loss.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAmeusYrfwAb"
      },
      "outputs": [],
      "source": [
        "def train_base_model(train_loader, epoch_num=None):\n",
        "  print(\"train base model\")\n",
        "\n",
        "  dataset_train = train_loader.dataset\n",
        "  model_base = PNAnet(dataset_train)\n",
        "  model_base = model_base.to(DEVICE)\n",
        "  betas, alphas, alpha_bars = generate_schedule()\n",
        "  lr = LEARNING_RATE\n",
        "  if BATCH_SIZE > 1:\n",
        "    lr = lr/100.0\n",
        "  optimizer = Adam(model_base.parameters(), lr = LEARNING_RATE)\n",
        "  loss_list = list()\n",
        "  graph_loss_list = list()\n",
        "  model_base, optimizer, graph_loss_list, loss_list, epoch_start = load_latest_checkpoint(model_base, optimizer, graph_loss_list, loss_list, epoch_i=0)\n",
        "\n",
        "  epoch_num = epoch_num if epoch_num is not None else EPOCHS\n",
        "  epoch_start = min(epoch_start, epoch_num)\n",
        "\n",
        "\n",
        "  for epoch_i in range(epoch_start,epoch_num):\n",
        "    if (epoch_i % 100 == 0 and epoch_i > 1000) or epoch_i == epoch_num - 1:\n",
        "      graphs = generate_examples(model_base, epoch_i, betas, dataset_train)\n",
        "      graph_loss_list.append(compute_generation_loss(graphs, None))\n",
        "      print(f\"generation loss: {graph_loss_list[-1]:06.4f}\")\n",
        "      plot_base(graph_loss_list, loss_list)\n",
        "      save_model(model_base, optimizer, graph_loss_list, loss_list, epoch_i)\n",
        "      if epoch_i == epoch_num - 1:\n",
        "        break # dont train in final epoch so that saved model is final\n",
        "\n",
        "    try:\n",
        "      loss, time_elapsed = train_epoch(model_base, train_loader, optimizer, betas)\n",
        "      loss_list.append(loss.item())\n",
        "      if epoch_i % 10 == 0 or epoch_i == epoch_num - 1:\n",
        "        print(f\"loss in epoch {epoch_i:07} is: {loss.item():05.4f} with mean loss {np.mean(loss_list + [loss.item()]):05.4f} with runtime {time_elapsed:05.4f}\")\n",
        "    except Exception as e:\n",
        "      print(\"An error occurred during training: \\n\", str(e))\n",
        "      traceback.print_exc()\n",
        "\n",
        "\n",
        "  return model_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNFoBC6PNfkY"
      },
      "source": [
        "# Discriminative Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgtMvfeuRw7F"
      },
      "source": [
        "- sample 100 train and 100 test graphs from DM\n",
        "- sample 100 train and 100 test graphs from trainset\n",
        "- train discriminator on DM-train and DM-test, D: g -> [0,1]\n",
        "- evaluate discriminator on DM-test, D(g)\n",
        "- initialize predictor P: g' -> [0,1], where g' is a (diffused) graph\n",
        "- for each g in DM-test, do forward diffusion until random t to get g'\n",
        "- train P(g') to predict D(g)\n",
        "- To sample graph: do inference on DM with the guidance function P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PvBw1h-RPPi"
      },
      "source": [
        "### Gen Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0Q8pRbdhXGY"
      },
      "outputs": [],
      "source": [
        "#!rm generated_samples.pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2_K2CQgUAoI"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "\n",
        "class PNAdisc(torch.nn.Module):\n",
        "  def __init__(self, train_dataset, hidden_channels=16, depth=4, dropout=0.05, towers=1, normalization=True, pre_post_layers=1):\n",
        "    super(PNAdisc, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Calculate x as the difference between mult_y and hidden_dim\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1)\n",
        "    #out_channels = towers * ((out_channels // towers) + 1)\n",
        "\n",
        "    in_channels = 1\n",
        "    deg = dataset_to_degree_bin(train_dataset)\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "    self.pnanet = PNA(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=hidden_channels, num_layers=depth, aggregators=aggregators, scalers=scalers, deg=deg, dropout=dropout, towers=towers, norm=self.normalization, pre_layers=pre_post_layers, post_layers=pre_post_layers)\n",
        "\n",
        "    self.final_mlp = Seq(Lin(hidden_channels, hidden_channels), nn.ReLU(),Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, 1))\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    if x.shape[1] == 2:\n",
        "      x = x[:,1].reshape(-1,1)\n",
        "    x = self.pnanet(x, edge_index)\n",
        "    #x = self.final_mlp(x)\n",
        "    x = global_mean_pool(x, batch)\n",
        "    x = torch.sum(x)\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "#model_disc = PNAdisc(disc_train_set)\n",
        "#model_disc.to(DEVICE)\n",
        "#g = binary_graphs[0]\n",
        "#g.to(DEVICE)\n",
        "#print(g, g.x.shape, g.edge_index.shape)  #Data(x=[18, 1], edge_index=[2, 50], weight=[50], y=0) torch.Size([18, 1]) torch.Size([2, 50])\n",
        "#model_disc(g.x, g.edge_index, batch=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7iN9Tgcp4YB"
      },
      "outputs": [],
      "source": [
        "def train_epoch_disc(model_disc, dataloader, optimizer, update_model=True):\n",
        "  if update_model:\n",
        "    model_disc.train()\n",
        "  else:\n",
        "    model_disc.eval()\n",
        "\n",
        "  start_time = time.time()\n",
        "  loss_list = list()\n",
        "  acc_list = list()\n",
        "  for g in dataloader:\n",
        "    g.to(DEVICE)\n",
        "    if update_model:\n",
        "     optimizer.zero_grad()\n",
        "    target = model_disc(g.x, g.edge_index, batch=torch.zeros(g.x.shape[0], dtype=torch.long, device=DEVICE))\n",
        "    loss = ((target.view(-1) - g.y.view(-1)))**2\n",
        "    if update_model:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    loss = np.sqrt(loss.item())  # convert MSE to L1\n",
        "    loss_list.append(loss)\n",
        "    acc_list.append(0.0 if loss > 0.5 else 1.0)\n",
        "\n",
        "  return np.mean(loss_list), np.mean(acc_list), time.time()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CWNQS7n9rzx"
      },
      "outputs": [],
      "source": [
        "def test_disc(model_disc, dataloader_disc_test):\n",
        "  return train_epoch_disc(model_disc, dataloader_disc_test, optimizer=None, update_model=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9FH4yvbnMAb"
      },
      "outputs": [],
      "source": [
        "def train_disc_model(dataloader_disc, dataloader_disc_test, round_i):\n",
        "  model_disc = PNAdisc(dataloader_disc)\n",
        "  weight_path = f\"discriminator_model_{round_i:03}.pth\"\n",
        "\n",
        "  try:\n",
        "    checkpoint = torch.load(weight_path)\n",
        "    model_disc.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"found disc model in round {round_i:04}\")\n",
        "    return model_disc\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  epochs = list()\n",
        "  losses_train = list()\n",
        "  losses_test = list()\n",
        "\n",
        "  optimizer_disc = Adam(model_disc.parameters(), lr = 0.0001)\n",
        "  for epoch_i in range(EPOCHS_DISC_MODEL):\n",
        "    loss_train, acc_train, t_train = train_epoch_disc(model_disc, dataloader_disc, optimizer_disc)\n",
        "    if epoch_i % 10 == 0:\n",
        "      loss_test, acc_test, t_test = test_disc(model_disc, dataloader_disc_test)\n",
        "      print(f\"train discriminator: epoch: {epoch_i:05}, loss: {loss_train:02.4f}, loss test: {loss_test:02.4f}, acc: {acc_train:01.3f}, acc test: {acc_test:01.3f}, time: {t_train:01.3f}\")\n",
        "      epochs.append(epoch_i)\n",
        "      losses_train.append(loss_train)\n",
        "      losses_test.append(loss_test)\n",
        "      plt.clf()\n",
        "      plt.plot(epochs, losses_train, label='train')\n",
        "      plt.plot(epochs, losses_test, label='test')\n",
        "      plt.legend()\n",
        "      plt.savefig(f\"discriminator_model_{round_i:03}.png\")\n",
        "\n",
        "  torch.save({'model_state_dict': model_disc.state_dict(), 'epochs': epochs, \"losses_train\": losses_train, \"losses_test\": losses_test}, weight_path)\n",
        "  return model_disc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isOhA5Kecd2U"
      },
      "outputs": [],
      "source": [
        "#train_loader_disc = DataLoader(binary_graphs, batch_size=1, shuffle=True)\n",
        "#train_loader_disc = DataLoader(disc_train_set, batch_size=1, shuffle=True)\n",
        "\n",
        "#model_disc = PNAdisc(train_loader_disc)\n",
        "#model_disc.to(DEVICE)\n",
        "\n",
        "#optimizer_disc = Adam(model_disc.parameters(), lr = 0.0001)\n",
        "\n",
        "#for epoch_i in range(100):\n",
        "#  loss, acc, t = train_epoch_disc(model_disc, train_loader_disc, optimizer_disc)\n",
        " # print(loss, acc, t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6SQ1V96LDpH"
      },
      "outputs": [],
      "source": [
        "def plot_graph_gen_loss(graph_gen_loss_list, round_i):\n",
        "  plt.clf()\n",
        "  plt.plot(graph_gen_loss_list)\n",
        "  plt.title(\"graph gen loss\")\n",
        "  plt.savefig(f\"graph_gen_loss_{round_i:03}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGbVSPgK_jlL"
      },
      "source": [
        "### Sample base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtAf3tRJlTJg"
      },
      "outputs": [],
      "source": [
        "def sample_base_model(model_base, round_i, model_guide=None, num=100):\n",
        "  try:\n",
        "    with open(f\"generated_samples_{round_i:03}.pickle\", \"rb\") as f:\n",
        "      generated_samples, generated_samples_nx, graph_gen_loss_list = pickle.load(f)\n",
        "      assert(len(generated_samples) == num)\n",
        "      print(f\"found generated samples in round {round_i:04}.\")\n",
        "      #graph_gen_loss = compute_generation_loss(generated_samples_nx, None)\n",
        "      print(f\"Generated {len(generated_samples_nx):05} graphs. Graph generation loss list is:\", graph_gen_loss_list)\n",
        "      #plot_graph_gen_loss(graph_gen_loss, round_i)\n",
        "      return generated_samples\n",
        "  except Exception as e:\n",
        "    print(\"An error occurred during training: \\n\", str(e))\n",
        "    traceback.print_exc()\n",
        "\n",
        "\n",
        "  graph_gen_loss_list = list()\n",
        "  try:\n",
        "    if len(graph_gen_loss_list) == 0 and round_i>0:\n",
        "      round_x = round_i-1\n",
        "      with open(f\"generated_samples_{round_x:03}.pickle\", \"rb\") as f:\n",
        "        _, _, graph_gen_loss_list = pickle.load(f)\n",
        "  except Exception as e:\n",
        "    print(\"An error occurred during training: \\n\", str(e))\n",
        "    traceback.print_exc()\n",
        "\n",
        "  #generated_samples = generate_examples_silent(model_base, betas, dataset_train, num=num, model_guide=model_guide)\n",
        "  generated_samples = generate_examples_batched(model_base, betas, dataset_train, num=num, model_guide=model_guide)\n",
        "\n",
        "  #random.shuffle(generated_samples)\n",
        "  assert(len(generated_samples) > 0)\n",
        "\n",
        "  generated_samples_nx = [pyg_to_sparsebinary_nx(g) for g in generated_samples]\n",
        "  graph_gen_loss = compute_generation_loss(generated_samples_nx, None)\n",
        "  print(\"graph_gen_loss: \",graph_gen_loss)\n",
        "  graph_gen_loss_list.append(graph_gen_loss)\n",
        "  plot_graph_gen_loss(graph_gen_loss_list, round_i)\n",
        "\n",
        "  with open(f\"generated_samples_{round_i:03}.pickle\", \"wb\") as f:\n",
        "    pickle.dump((generated_samples, generated_samples_nx, graph_gen_loss_list), f)\n",
        "\n",
        "  return generated_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7vBbaspAZRB"
      },
      "outputs": [],
      "source": [
        "def plot_samples(disc_train_set, model_disc=None, round_i=0):\n",
        "  if os.path.exists(f'grid_images_for_disc_pred_{round_i:04}.png'):\n",
        "    print(\"found\", f'grid_images_for_disc_pred_{round_i:04}.png')\n",
        "    return\n",
        "\n",
        "  fig, axs = plt.subplots(16, 10, figsize=(20, 32))\n",
        "  nx_list = list()\n",
        "  mean_degree_list = list()\n",
        "\n",
        "  random.shuffle(disc_train_set)\n",
        "\n",
        "  ix = -1\n",
        "  for i in tqdm(range(16)):\n",
        "    for j in range(10):\n",
        "      ix += 1\n",
        "      g = disc_train_set[ix]\n",
        "      ax = axs[i, j]\n",
        "      pos, g_nx = plot_weighted_graph(None, g, ax, pos=None, binarize=True)\n",
        "      pred = 0\n",
        "      if model_disc is not None:\n",
        "        pred = model_disc(g.x, g.edge_index, batch=None).item()\n",
        "      g.pred = pred\n",
        "      #print(pred)\n",
        "\n",
        "      edge_weights = nx.get_edge_attributes(g_nx, 'weight')\n",
        "      edge_weights = [edge_weights[e]/2.0+0.5 for e in g_nx.edges] #done in plot_weighted...\n",
        "      mean_degree = 2*np.sum(edge_weights)/g_nx.number_of_nodes()\n",
        "      mean_degree_list.append(mean_degree)\n",
        "\n",
        "      ax.set_title(f\"{pred:.4f} - {mean_degree:01.2f}\")\n",
        "      ax.axis('off')\n",
        "      nx_list.append(g_nx)\n",
        "\n",
        "  #with open(f'grid_images_for_disc_pred_{round_i:04}.pickle', \"wb\") as f:\n",
        "  #  pickle.dump(nx_list, f)\n",
        "\n",
        "  #plt.tight_layout()\n",
        "  plt.savefig(f'grid_images_for_disc_pred_{round_i:04}.png', dpi=300)\n",
        "  print(f\"mean degree is {np.mean(mean_degree_list):02.5f}\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZtmsTm3s5z9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pgjxArr_q_5"
      },
      "source": [
        "# Guidance Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUCpj81oDihD"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "\n",
        "class PNAguide(torch.nn.Module):\n",
        "  def __init__(self, train_dataset, hidden_channels=32, depth=5, dropout=0.00, towers=3, normalization=True, pre_post_layers=1):\n",
        "    super(PNAguide, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Calculate x as the difference between mult_y and hidden_dim\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1)\n",
        "    #out_channels = towers * ((out_channels // towers) + 1)\n",
        "\n",
        "    in_channels = 3\n",
        "    deg = dataset_to_degree_bin(train_dataset)\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "    self.pnanet = PNA(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=hidden_channels, num_layers=depth, aggregators=aggregators, scalers=scalers, deg=deg, dropout=dropout, towers=towers, norm=self.normalization, pre_layers=pre_post_layers, post_layers=pre_post_layers)\n",
        "\n",
        "    self.final_mlp = Seq(Lin(hidden_channels, hidden_channels), nn.ReLU(),Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, 1))\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    x = self.pnanet(x, edge_index)\n",
        "    x = self.final_mlp(x)\n",
        "    x = global_mean_pool(x, batch)\n",
        "    x = torch.sum(x, dim=1)\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "#model_guide = PNAguide(train_loader_disc)\n",
        "#model_guide.to(DEVICE)\n",
        "#g = train_loader_disc.dataset[0]\n",
        "#g.to(DEVICE)\n",
        "#print(g, g.x.shape, g.edge_index.shape)  #Data(x=[18, 1], edge_index=[2, 50], weight=[50], y=0) torch.Size([18, 1]) torch.Size([2, 50])\n",
        "#node_nun = g.x.shape[0]\n",
        "#x_in = torch.randn([node_nun, 3])\n",
        "#model_guide(x_in, g.edge_index, batch=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlkD45LK_w-_"
      },
      "outputs": [],
      "source": [
        "def train_epoch_guide(model_guide, model_disc, dataloader, optimizer, schedule, update_model=True, epoch_i=1):\n",
        "  if update_model:\n",
        "    model_guide.train()\n",
        "  else:\n",
        "    model_guide.eval()\n",
        "  start_time = time.time()\n",
        "  loss_list = list()\n",
        "\n",
        "  for g in dataloader:\n",
        "    g.to(DEVICE)\n",
        "    if update_model:\n",
        "      optimizer.zero_grad()\n",
        "    target = model_disc(g.x, g.edge_index, batch=torch.zeros(g.x.shape[0], dtype=torch.long, device=DEVICE))\n",
        "\n",
        "    # comupte t vec\n",
        "    num_graphs_in_batch = int(torch.max(g.batch).item()+1)\n",
        "    t_max = TIMESTEPS\n",
        "    #t_max = min(epoch_i+1, t_max // 3) # todo\n",
        "    #t_min = t_max - t_max // GUIDE_FRACTION\n",
        "    t_max=TIMESTEPS// GUIDE_FRACTION\n",
        "    future_t_select = torch.randint(0, t_max, (num_graphs_in_batch,), device = DEVICE)\n",
        "    future_t = torch.gather(future_t_select, 0, g.batch)\n",
        "\n",
        "    # compute noisy edge weights\n",
        "    edge_indices = g.x[:,0] < 0.1\n",
        "    edge_weights = g.x[:,1].clone()\n",
        "    edges_with_noise, noise_gt = forward_diffusion(edge_weights[edge_indices], future_t[edge_indices], schedule)\n",
        "    edge_weights[edge_indices] = edges_with_noise\n",
        "    x_in = torch.concat((g.x[:,0].view(-1,1), edge_weights.view(-1,1), future_t.view(-1,1)), dim=1)\n",
        "\n",
        "    # prediction\n",
        "    prediction = model_guide(x_in, g.edge_index, batch=torch.zeros(g.x.shape[0], dtype=torch.long, device=DEVICE))\n",
        "    loss = ((target.view(-1) - prediction.view(-1)))**2\n",
        "    if update_model:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    loss = np.sqrt(loss.item())  # convert MSE to L1\n",
        "    loss_list.append(loss)\n",
        "\n",
        "  return np.mean(loss_list), time.time()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgL1i5cOBwRT"
      },
      "outputs": [],
      "source": [
        "def test_guide(model_guide, model_disc, train_loader_disc_test, schedule, epoch_i):\n",
        "  return train_epoch_guide(model_guide, model_disc, dataloader=train_loader_disc_test, optimizer=None, schedule=schedule, update_model=False, epoch_i=epoch_i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD2wR_7zGEO7"
      },
      "outputs": [],
      "source": [
        "def get_merged_dataloader(generated_samples, dataset):\n",
        "\n",
        "  #random.shuffle(dataset_train) # not needed because shuffle list\n",
        "  random.shuffle(generated_samples) # also prob not needed\n",
        "\n",
        "  num_elem = min(len(generated_samples), len(dataset))\n",
        "  disc_set = list()\n",
        "\n",
        "  for fake_graph in generated_samples[:num_elem]:\n",
        "    fake_graph = fake_graph.clone()\n",
        "    fake_graph.y =  0.0\n",
        "    disc_set.append(fake_graph)\n",
        "\n",
        "  for real_graph in dataset[:num_elem]:\n",
        "    real_graph = real_graph.clone()\n",
        "    real_graph.y = 1.0\n",
        "    disc_set.append(real_graph)\n",
        "\n",
        "  dataloader_disc = DataLoader(disc_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  return dataloader_disc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntRL02_Jxbss"
      },
      "outputs": [],
      "source": [
        "def train_guide_model(model_disc, train_loader_disc, train_loader_disc_test, round_i):\n",
        "  model_guide = PNAguide(train_loader_disc)\n",
        "\n",
        "  weight_path = f\"guide_model_{round_i:03}.pth\"\n",
        "  try:\n",
        "    checkpoint = torch.load(weight_path)\n",
        "    model_guide.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"found guide model in round {round_i:04}\")\n",
        "    return model_guide\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "  model_guide = model_guide.to(DEVICE)\n",
        "  optimizer_guide = Adam(model_guide.parameters(), lr = 0.0001)\n",
        "  betas, alphas, alpha_bars = generate_schedule()\n",
        "\n",
        "  epochs = list()\n",
        "  losses_train = list()\n",
        "  losses_test = list()\n",
        "\n",
        "  for epoch_i in range(EPOCHS_GUIDE_MODEL):\n",
        "    loss_train, t_train = train_epoch_guide(model_guide, model_disc, train_loader_disc, optimizer_guide, schedule=betas, epoch_i=epoch_i)\n",
        "    if epoch_i % 10 == 0:\n",
        "      loss_test, t_test = test_guide(model_guide, model_disc, train_loader_disc_test, betas, epoch_i=epoch_i)\n",
        "      print(f\"train guide: epoch: {epoch_i:05}, loss: {loss_train:02.4f}, loss test: {loss_test:02.4f}, time: {t_train:02.3f}\")\n",
        "\n",
        "      epochs.append(epoch_i)\n",
        "      losses_train.append(loss_train)\n",
        "      losses_test.append(loss_test)\n",
        "      plt.clf()\n",
        "      plt.plot(epochs, losses_train, label='train')\n",
        "      plt.plot(epochs, losses_test, label='test')\n",
        "      plt.legend()\n",
        "      plt.savefig(f\"guide_model_{round_i:03}.png\")\n",
        "\n",
        "  torch.save({'model_state_dict': model_guide.state_dict(), 'epochs': epochs, \"losses_train\": losses_train, \"losses_test\": losses_test}, weight_path)\n",
        "  return model_guide\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9P2NDVv_GGG"
      },
      "source": [
        "# Putting thinigs together I - Classical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YBTkSsL_HTT"
      },
      "outputs": [],
      "source": [
        "#dataset_base, dataset_base_test = build_dataset()\n",
        "#dataloader_base = DataLoader(dataset_base, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#num_samples_train = int(NUM_SAMPLES*TRAIN_TEST_SPLIT)\n",
        "#num_samples_test = NUM_SAMPLES - num_samples_train\n",
        "#model_base = train_base_model(dataloader_base, epoch_num = BASE_MODEL_EPOCHS)\n",
        "#model_guide = None\n",
        "\n",
        "#for round_i in range(NUM_ROUNDS):\n",
        "#  samples = sample_base_model(model_base, round_i, model_guide=model_guide, num=NUM_SAMPLES)\n",
        "#  dataloader_disc = get_merged_dataloader(samples[:num_samples_train], dataset_base)\n",
        "#  dataloader_disc_test = get_merged_dataloader(samples[num_samples_train:], dataset_base_test)\n",
        "#  model_disc = train_disc_model(dataloader_disc, dataloader_disc_test, round_i)\n",
        "#  plot_samples(dataloader_disc.dataset, model_disc, round_i)\n",
        "#  model_guide = train_guide_model(model_disc, dataloader_disc, dataloader_disc_test, round_i)\n",
        "\n",
        "#  #new_epoch_num = BASE_MODEL_EPOCHS + (round_i+1)*int(BASE_MODEL_EPOCHS*0.2)\n",
        "#  #model_base = train_base_model(dataloader_base, epoch_num = new_epoch_num)\n",
        "\n",
        "#samples = sample_base_model(model_base, round_i=NUM_ROUNDS, model_guide=model_guide)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVfF7ZyuR4sC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDhvDZK_YPhu"
      },
      "source": [
        "# Putting thinigs together II - Single Disc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olNy5xb9ZEcI"
      },
      "outputs": [],
      "source": [
        "def train_epoch_cassandra(model_cassandra, dataloader, optimizer, schedule, update_model=True, epoch_i=1):\n",
        "  if update_model:\n",
        "    model_cassandra.train()\n",
        "  else:\n",
        "    model_cassandra.eval()\n",
        "  model_cassandra = model_cassandra.to(DEVICE)\n",
        "  start_time = time.time()\n",
        "  loss_list = list()\n",
        "  acc_list = list()\n",
        "\n",
        "  for g in dataloader:\n",
        "    g = g.to(DEVICE)\n",
        "    if update_model:\n",
        "      optimizer.zero_grad()\n",
        "    target = g.y\n",
        "\n",
        "    # comupte t vec\n",
        "    num_graphs_in_batch = int(torch.max(g.batch).item()+1)\n",
        "    t_max = TIMESTEPS\n",
        "    #t_max = min(epoch_i+1, t_max // 3) # todo\n",
        "    #t_min = t_max - t_max // GUIDE_FRACTION\n",
        "    t_max=TIMESTEPS// GUIDE_FRACTION\n",
        "    future_t_select = torch.randint(0, t_max, (num_graphs_in_batch,), device = DEVICE)\n",
        "    future_t = torch.gather(future_t_select, 0, g.batch)\n",
        "\n",
        "    # compute noisy edge weights\n",
        "    edge_indices = g.x[:,0] < 0.1\n",
        "    edge_weights = g.x[:,1].clone()\n",
        "    edges_with_noise, noise_gt = forward_diffusion(edge_weights[edge_indices], future_t[edge_indices], schedule)\n",
        "    edge_weights[edge_indices] = edges_with_noise\n",
        "    x_in = torch.concat((g.x[:,0].view(-1,1), edge_weights.view(-1,1), future_t.view(-1,1)), dim=1)\n",
        "\n",
        "    # prediction\n",
        "    prediction = model_cassandra(x_in, g.edge_index, batch=torch.zeros(g.x.shape[0], dtype=torch.long, device=DEVICE))\n",
        "    loss = ((target.view(-1) - prediction.view(-1)))**2\n",
        "    if update_model:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    loss = np.sqrt(loss.item())  # convert MSE to L1\n",
        "    loss_list.append(loss)\n",
        "    acc_list.append(0.0 if loss > 0.5 else 1.0)\n",
        "\n",
        "  return np.mean(loss_list), np.mean(acc_list), time.time()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J465bmjZXPl"
      },
      "outputs": [],
      "source": [
        "def test_cassandra(model_cassandra, train_loader_disc_test, schedule, epoch_i):\n",
        "  return train_epoch_cassandra(model_cassandra, dataloader=train_loader_disc_test, optimizer=None, schedule=schedule, update_model=False, epoch_i=epoch_i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AO4cFiNYhbm"
      },
      "outputs": [],
      "source": [
        "def train_cassandra_model(train_loader_disc, train_loader_disc_test, round_i):\n",
        "  model_cassandra = PNAguide(train_loader_disc)\n",
        "\n",
        "\n",
        "  weight_path = f\"cassandra_model_{round_i:03}.pth\"\n",
        "  try:\n",
        "    checkpoint = torch.load(weight_path)\n",
        "    model_cassandra.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"found cassandra model in round {round_i:04}\")\n",
        "    return model_cassandra\n",
        "  except:\n",
        "    print(weight_path, \"not found\")\n",
        "\n",
        "  model_cassandra = model_cassandra.to(DEVICE)\n",
        "  optimizer_cassandra = Adam(model_cassandra.parameters(), lr = 0.0001)\n",
        "  betas, alphas, alpha_bars = generate_schedule()\n",
        "\n",
        "  epochs = list()\n",
        "  losses_train = list()\n",
        "  losses_test = list()\n",
        "\n",
        "  for epoch_i in range(EPOCHS_GUIDE_MODEL):\n",
        "    loss_train, acc_train, t_train = train_epoch_cassandra(model_cassandra, train_loader_disc, optimizer_cassandra, schedule=betas, epoch_i=epoch_i)\n",
        "    if epoch_i % 10 == 0:\n",
        "      loss_test, acc_test, t_test = test_cassandra(model_cassandra, train_loader_disc_test, betas, epoch_i=epoch_i)\n",
        "      print(f\"train cassandra: epoch: {epoch_i:05}, loss: {loss_train:02.4f}, loss test: {loss_test:02.4f},  acc: {acc_train:01.3f}, acc test: {acc_test:01.3f}, time: {t_train:02.3f}\")\n",
        "\n",
        "      epochs.append(epoch_i)\n",
        "      losses_train.append(loss_train)\n",
        "      losses_test.append(loss_test)\n",
        "      plt.clf()\n",
        "      plt.plot(epochs, losses_train, label='train')\n",
        "      plt.plot(epochs, losses_test, label='test')\n",
        "      plt.legend()\n",
        "      plt.savefig(f\"cassandra_model_{round_i:03}.png\")\n",
        "\n",
        "  torch.save({'model_state_dict': model_cassandra.state_dict(), 'epochs': epochs, \"losses_train\": losses_train, \"losses_test\": losses_test}, weight_path)\n",
        "  return model_cassandra\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2bmaEFZYR0n"
      },
      "outputs": [],
      "source": [
        "dataset_base, dataset_base_test = build_dataset()\n",
        "dataloader_base = DataLoader(dataset_base, batch_size=BATCH_SIZE, shuffle=True)\n",
        "num_samples_train = int(NUM_SAMPLES*TRAIN_TEST_SPLIT)\n",
        "num_samples_test = NUM_SAMPLES - num_samples_train\n",
        "model_base = train_base_model(dataloader_base, epoch_num = BASE_MODEL_EPOCHS)\n",
        "model_cassandra = None\n",
        "\n",
        "for round_i in range(NUM_ROUNDS):\n",
        "  samples = sample_base_model(model_base, round_i, model_guide=model_cassandra, num=NUM_SAMPLES)  #TODO\n",
        "  plot_samples(samples, model_disc=None, round_i=round_i)\n",
        "  dataloader_disc = get_merged_dataloader(samples[:num_samples_train], dataset_base)\n",
        "  dataloader_disc_test = get_merged_dataloader(samples[num_samples_train:], dataset_base_test)\n",
        "  model_cassandra = train_cassandra_model(dataloader_disc, dataloader_disc_test, round_i)\n",
        "  #new_epoch_num = BASE_MODEL_EPOCHS + (round_i+1)*int(BASE_MODEL_EPOCHS*0.2)\n",
        "  #model_base = train_base_model(dataloader_base, epoch_num = new_epoch_num)\n",
        "\n",
        "samples = sample_base_model(model_base, round_i=NUM_ROUNDS, model_guide=model_cassandra, num=NUM_SAMPLES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC5nnw3GDaAN"
      },
      "outputs": [],
      "source": [
        "#!rm cassandra_model_*.pth\tgenerated_samples_001.pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kvYvQ509ckB"
      },
      "outputs": [],
      "source": [
        "#!rm generated_samples_002.pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egG7sdK6RFms"
      },
      "outputs": [],
      "source": [
        " z=z/0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QnaWzDLyBzn"
      },
      "outputs": [],
      "source": [
        "s1 = [pyg_to_sparsebinary_nx(s) for s in samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-rzR_kPyTMF"
      },
      "outputs": [],
      "source": [
        "samples[0].x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuMUNIAbz2rE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric.data as data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Define the adjacency matrices of the graphs\n",
        "adj_matrix1 = torch.tensor([[0, 1, 0, 0], [1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0]])\n",
        "adj_matrix2 = torch.tensor([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
        "\n",
        "# Create the GraphData objects for each graph\n",
        "graph1 = data.Data(x=torch.randn(4, 3), edge_index=adj_matrix1.nonzero().t())\n",
        "graph2 = data.Data(x=torch.randn(3, 3), edge_index=adj_matrix2.nonzero().t())\n",
        "\n",
        "# Create a list of the graphs\n",
        "graph_list = [graph1, graph2]\n",
        "\n",
        "# Create the DataLoader with batch size 2\n",
        "batch_size = 2\n",
        "dataloader = DataLoader(graph_list, batch_size=batch_size)\n",
        "\n",
        "# Iterate over the DataLoader\n",
        "for batch in dataloader:\n",
        "    # Unpack the batch into individual graphs\n",
        "    graphs = batch.to_data_list()\n",
        "    print(batch)\n",
        "    print(batch.x)\n",
        "    batch.x = torch.zeros_like(batch.x, device=DEVICE)\n",
        "    for g in batch.to_data_list():\n",
        "      print(g.x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJD24ijLswiu"
      },
      "outputs": [],
      "source": [
        "samples = sample_base_model(model_base, 0, model_guide=None, num=NUM_SAMPLES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irKpz9IIs0xb"
      },
      "outputs": [],
      "source": [
        "samples\n",
        "generated_samples_nx = [pyg_to_sparsebinary_nx(g) for g in samples]\n",
        "graph_gen_loss = compute_generation_loss(generated_samples_nx, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvzA0zejtMLy"
      },
      "outputs": [],
      "source": [
        "#g = generated_samples_nx[0]\n",
        "dlist = list()\n",
        "for g in generated_samples_nx:\n",
        "  for v_i in g.nodes():\n",
        "    dlist.append(g.degree(v_i))\n",
        "print(dlist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMJa5xi3_Nyo"
      },
      "outputs": [],
      "source": [
        "#!rm *cassandra*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXprq5xuhzsI"
      },
      "outputs": [],
      "source": [
        "#ä!rm generated_samples_001.pickle generated_samples_002.pickle generated_samples_003.pickle generated_samples_004.pickle generated_samples_005.pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QB8C63TdYwO"
      },
      "source": [
        "# Putting Things Together III - Oracle Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4zr1nyRmF0c"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "\n",
        "class PNAoracle(torch.nn.Module):\n",
        "  def __init__(self, train_dataset, hidden_channels=32, depth=4, dropout=0.05, towers=1, normalization=True, pre_post_layers=1):\n",
        "    super(PNAoracle, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Calculate x as the difference between mult_y and hidden_dim\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1)\n",
        "    #out_channels = towers * ((out_channels // towers) + 1)\n",
        "\n",
        "    in_channels = 3\n",
        "    deg = dataset_to_degree_bin(train_dataset)\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "    self.pnanet = PNA(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=hidden_channels, num_layers=depth, aggregators=aggregators, scalers=scalers, deg=deg, dropout=dropout, towers=towers, norm=self.normalization, pre_layers=pre_post_layers, post_layers=pre_post_layers)\n",
        "\n",
        "    self.final_mlp = Seq(Lin(hidden_channels, hidden_channels), nn.ReLU(),Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, 1))\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    x = self.pnanet(x, edge_index)\n",
        "    x = self.final_mlp(x)\n",
        "    x = global_mean_pool(x, batch)\n",
        "    x = torch.sum(x)\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9kiTfBAlDwb"
      },
      "outputs": [],
      "source": [
        "def test_oracle(model_oracle, g, t, return_float=False):\n",
        "  if model_oracle is None:\n",
        "    # here we also return float\n",
        "    return 0.0\n",
        "\n",
        "  future_t = torch.tensor(t).repeat(g.x.shape[0])\n",
        "  x_in = torch.concat((g.x.reshape(-1,2), future_t.reshape(-1,1)), dim=1)\n",
        "\n",
        "  # prediction\n",
        "  prediction = model_oracle(x_in, g.edge_index, batch=torch.zeros(g.x.shape[0], dtype=torch.long, device=DEVICE))\n",
        "  if return_float:\n",
        "    return prediction.item()\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpu-_HU7gvol"
      },
      "outputs": [],
      "source": [
        "def forward_diffusion_until(g, t, schedule):\n",
        "  g = g.clone()\n",
        "\n",
        "  future_t = torch.tensor([int(t)]).repeat(g.x.shape[0])\n",
        "  assert(future_t.numel() == g.x.shape[0])\n",
        "\n",
        "  edge_indices = g.x[:,0] < 0.1\n",
        "  edge_weights = g.x[:,1]\n",
        "  edges_with_noise, noise_gt = forward_diffusion(edge_weights[edge_indices], future_t[edge_indices], schedule)\n",
        "  g.x[edge_indices,1] = edges_with_noise\n",
        "  return g\n",
        "\n",
        "@torch.inference_mode()\n",
        "def graph_inference_until(g, model, compare_times, schedule, oracle_old=None, choice_num=2):\n",
        "  g = g.clone().to(DEVICE)\n",
        "  compare_times = list(compare_times)\n",
        "  assert(\"int\" in str(type(compare_times[0])))\n",
        "\n",
        "  generated_graphs = list()\n",
        "  edge_indices = g.x[:,0] < 0.1\n",
        "  edge_weights = g.x[:,1]\n",
        "  edges_without_noise = edge_weights[edge_indices]\n",
        "  edges_with_noise = torch.randn_like(edges_without_noise, device=DEVICE)\n",
        "  g.x[edge_indices,1] = edges_with_noise\n",
        "\n",
        "  for i in range(TIMESTEPS):\n",
        "    t = TIMESTEPS - i - 1\n",
        "    if len(compare_times) == 0:\n",
        "      break\n",
        "    if t == compare_times[-1]:\n",
        "      compare_times.pop()\n",
        "      generated_graphs.append((g, t)) # clone necessary?\n",
        "\n",
        "    choice_list = list()\n",
        "    if oracle_old is None:\n",
        "      choice_num = 1\n",
        "    for j in range(choice_num):\n",
        "      g_alt = g.clone()\n",
        "      edges_with_noise = denoise_one_step(model, g_alt, i, schedule)\n",
        "      g_alt.x[edge_indices,1] = edges_with_noise\n",
        "      choice_list.append(g_alt)\n",
        "    scores = [test_oracle(oracle_old, g, t, return_float=True) for g in choice_list]\n",
        "    g = choice_list[np.argmax(scores)]\n",
        "\n",
        "  return generated_graphs\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEHhF2iSgD5a"
      },
      "outputs": [],
      "source": [
        "def train_epoch_oracle(dataloader, model_oracle, model_base, oracle_old, optimizer_oracle, schedule, epoch_i):\n",
        "  # 1 is real, 0 is fake\n",
        "  model_oracle.train()\n",
        "  if oracle_old is not None:\n",
        "    oracle_old.eval()\n",
        "  start_time = time.time()\n",
        "  loss_list = list()\n",
        "  comparisons_per_graph = 10\n",
        "\n",
        "  for g in dataloader:\n",
        "    g = g.to(DEVICE)\n",
        "    optimizer_oracle.zero_grad()\n",
        "    compare_times = torch.randint(0, TIMESTEPS, (comparisons_per_graph,), device = DEVICE) #t=0 is original image\n",
        "    compare_times = sorted(compare_times.tolist())\n",
        "\n",
        "    g_fake_list = graph_inference_until(g, model_base, compare_times, schedule, oracle_old)\n",
        "    for i, (g_fake, t) in enumerate(g_fake_list):\n",
        "      g_fake = g_fake.clone()  #this is somehow necessary\n",
        "      #t = compare_times[i]\n",
        "      g_real = forward_diffusion_until(g, t, schedule)\n",
        "      loss = (test_oracle(model_oracle, g_fake, t) - 0.0)**2\n",
        "      loss = loss + (test_oracle(model_oracle, g_real, t) - 1.0)**2\n",
        "      loss.backward()\n",
        "      optimizer_oracle.step()\n",
        "      loss_list.append(loss.item())\n",
        "\n",
        "  return np.mean(loss_list), time.time() - start_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2Www9EjfaZN"
      },
      "outputs": [],
      "source": [
        "def train_oracle_model(dataloader_base, model_base, round_i, oracle_old=None):\n",
        "  model_oracle = PNAoracle(dataloader_base)\n",
        "\n",
        "  weight_path = f\"oracle_model_{round_i:03}.pth\"\n",
        "  try:\n",
        "    checkpoint = torch.load(weight_path)\n",
        "    model_oracle.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"found oracle model in round {round_i:04}\")\n",
        "    return model_oracle\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  model_oracle.to(DEVICE)\n",
        "  optimizer_oracle = Adam(model_oracle.parameters(), lr = 0.0001)\n",
        "  betas, alphas, alpha_bars = generate_schedule()\n",
        "\n",
        "  for epoch_i in range(EPOCHS_GUIDE_MODEL):\n",
        "    loss, t = train_epoch_oracle(dataloader_base, model_oracle, model_base, oracle_old, optimizer_oracle, schedule=betas, epoch_i=epoch_i)\n",
        "    print(f\"train oracle: epoch: {epoch_i:05}, loss: {loss:02.4f}, time: {t:02.3f}\")\n",
        "\n",
        "  torch.save({'model_state_dict': model_oracle.state_dict()}, weight_path)\n",
        "  return model_oracle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEeM26UqjO4e"
      },
      "outputs": [],
      "source": [
        "def get_last(elem_list):\n",
        "  return None if len(elem_list) == 0 else elem_list[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU8Og9XIda_P"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_base, dataset_test = build_dataset()\n",
        "dataloader_base = DataLoader(dataset_base, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "model_base = train_base_model(dataloader_base, epoch_num = BASE_MODEL_EPOCHS)\n",
        "print(\"finished training base model\")\n",
        "#generate_examples(model_base, BASE_MODEL_EPOCHS+1, generate_schedule()[0], dataset_base, num_times=30)\n",
        "\n",
        "\n",
        "model_oracle_old = list()\n",
        "\n",
        "\n",
        "for round_i in range(NUM_ROUNDS):\n",
        "  model_base.eval()\n",
        "  samples = sample_base_model(model_base, round_i, model_guide=model_oracle_old, num=len(dataset_base))\n",
        "  #plot_samples(dataloader_base.dataset, model_disc, round_i)\n",
        "  model_oracle = train_oracle_model(dataloader_base, model_base, round_i, oracle_old=model_oracle_old)\n",
        "  model_oracle.eval()\n",
        "  model_oracle_old = model_oracle\n",
        "\n",
        "samples = sample_base_model(model_base, round_i=NUM_ROUNDS, model_guide=model_oracle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmAvO2-kuzWC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "mount_file_id": "1d7uhXgyzzsVVb0WVyh7vHRMFHIymHs8f",
      "authorship_tag": "ABX9TyOvjL8/u6vsnDzyXUBJJvYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}