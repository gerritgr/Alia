{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerritgr/Alia/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCMM1H64tA7p"
      },
      "source": [
        "# AliaMolecule Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5NrAGm7to1n"
      },
      "source": [
        "#### Project Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "833DrsZU2o7r"
      },
      "outputs": [],
      "source": [
        "#!pip install wandb --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCeS1g3btmT9"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME = \"AliaMoleculePaper2\"\n",
        "PATH_PATTERN_BASE = \"aliamol_paper\" #aliamol2 is trained on denoised image\n",
        "BASELINE = False\n",
        "\n",
        "\n",
        "DEBUG = False\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z7g6wEvtFrr"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9JUGxi3tEae",
        "outputId": "ea763a4c-6d54-4e9b-8b54-c893033e141a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Current Working Directory:  /content\n",
            "New Working Directory:  /content/drive/MyDrive/colab/AliaMoleculePaper\n"
          ]
        }
      ],
      "source": [
        "# Load drive\n",
        "\n",
        "import os\n",
        "USE_COLAB = False\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  USE_COLAB = True\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  import wandb # need to do this before chaning cwd\n",
        "except:\n",
        "  os.system(\"pip install wandb\")\n",
        "\n",
        "\n",
        "if USE_COLAB:\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "  dir_path = f'/content/drive/MyDrive/colab/{PROJECT_NAME}/'\n",
        "  if not os.path.exists(dir_path):\n",
        "    os.makedirs(dir_path)\n",
        "  print(\"Current Working Directory: \", os.getcwd())\n",
        "  if os.getcwd() != dir_path:\n",
        "    os.chdir(dir_path)\n",
        "    print(\"New Working Directory: \", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9oArycxtC6J"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "\n",
        "import os\n",
        "import torch\n",
        "torch_version = torch.__version__.split(\"+\")\n",
        "#os.environ[\"TORCH\"] = torch_version[0]\n",
        "#os.environ[\"CUDA\"] = torch_version[1]\n",
        "try:\n",
        "  import torch_geometric\n",
        "except:\n",
        "  os.system(\"pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\")\n",
        "  os.system(\"pip install torch-geometric\")\n",
        "\n",
        "try:\n",
        "  import rdkit\n",
        "except:\n",
        "  os.system(\"pip install rdkit\")\n",
        "\n",
        "PATH_PATTERN = PATH_PATTERN_BASE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LU94GR6x1y-"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD1DOMjwx29q"
      },
      "outputs": [],
      "source": [
        "#%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 100 # Set this to 300 to get better image quality\n",
        "from PIL import Image # We use PIL to load images\n",
        "import seaborn as sns\n",
        "#import imageio # to generate .gifs\n",
        "import networkx as nx\n",
        "\n",
        "# always good to have\n",
        "import glob, random, os, traceback, time, copy\n",
        "import pickle\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import gzip\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Linear as Lin\n",
        "from torch.nn import Sequential as Seq\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GATv2Conv, GraphNorm, BatchNorm\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#DEVICE = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oscW9KW_NOTi"
      },
      "source": [
        "### Load External"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCNRU4kbNQAJ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"smiles_to_pyg\"):\n",
        "  os.system(\"git clone https://github.com/gerritgr/Alia.git && cp -R Alia/* .\")\n",
        "from smiles_to_pyg.molecule_load_and_convert import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef33eK-2yBVa"
      },
      "source": [
        "#### Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3INYZ1OeyDZs"
      },
      "outputs": [],
      "source": [
        "##\n",
        "## Diffusion\n",
        "##\n",
        "TIMESTEPS = 1000\n",
        "START = 0.0001\n",
        "END = 0.015\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 128*2\n",
        "GAMMA = 0.1\n",
        "\n",
        "##\n",
        "## Pred\n",
        "##\n",
        "LEARNING_RATE_GEN = 0.001\n",
        "EPOCHS_GEN = 100\n",
        "\n",
        "### PNA Pred\n",
        "DROPOUT_PRED = 0.05\n",
        "DEPTH_PRED = 4\n",
        "HIDDEN_CHANNELS_PRED = 32\n",
        "TOWERS_PRED = 1\n",
        "NORMALIZATION_PRED = True\n",
        "\n",
        "##\n",
        "## Disc\n",
        "##\n",
        "EPOCHS_DISC_MODEL = 70\n",
        "DISC_NOISE=0.3\n",
        "\n",
        "### PNA Disc\n",
        "HIDDEN_CHANNELS_DISC = 8\n",
        "DEPTH_DISC = 4\n",
        "DROPOUT_DISC = 0.05\n",
        "NORMALIZATION_DISC = True\n",
        "\n",
        "\n",
        "##\n",
        "## Molecule Encoding\n",
        "##\n",
        "\n",
        "INDICATOR_FEATURE_DIM = 1\n",
        "FEATURE_DIM = 5 # (has to be the same for atom and bond)\n",
        "ATOM_FEATURE_DIM = FEATURE_DIM\n",
        "BOND_FEATURE_DIM = FEATURE_DIM\n",
        "NON_NODES = [True] + [False]*5 + [True] * 5\n",
        "NON_EDGES = [True] + [True]*5 + [False] * 5\n",
        "\n",
        "TIME_FEATURE_DIM = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCUzkUbpyRAP"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0-ubb5pwu84"
      },
      "outputs": [],
      "source": [
        "def log(d):\n",
        "  try:\n",
        "    import wandb\n",
        "    wandb.log(d)\n",
        "  except:\n",
        "    print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isR2usjlQr0k"
      },
      "outputs": [],
      "source": [
        "def load_file(filepath):\n",
        "  print(\"try to read \", filepath)\n",
        "  try:\n",
        "    with gzip.open(filepath, 'rb') as f:\n",
        "      return pickle.load(f)\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {str(e)}\")\n",
        "      raise\n",
        "\n",
        "def write_file(filepath, data):\n",
        "  try:\n",
        "    data = data.cpu()\n",
        "  except:\n",
        "    pass\n",
        "  print(\"try to write \", filepath)\n",
        "  with gzip.open(filepath, 'wb') as f:\n",
        "    pickle.dump(data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pb4cD9fMxkl"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_dataset(seed=1234):\n",
        "  try:\n",
        "    dataset_train, dataset_test = load_file('dataset.pickle')\n",
        "    return dataset_train, dataset_test\n",
        "  except Exception as e:\n",
        "    print(f\"Could not load dataset due to error: {str(e)}, generate it now\")\n",
        "\n",
        "  dataset = read_qm9()\n",
        "  dataset_all = [g for g in dataset if g.x.shape[0] > 1]\n",
        "  dataset = list()\n",
        "  for g in tqdm(dataset_all):\n",
        "    try:\n",
        "      assert \"None\" not in str(pyg_to_smiles(g))\n",
        "      dataset.append(g)\n",
        "    except:\n",
        "      pass\n",
        "  print(\"Built and clean dataset, length is \", len(dataset), \"old length was\", len(dataset_all))\n",
        "  random.Random(seed).shuffle(dataset)\n",
        "  split = int(len(dataset)*0.8 + 0.5)\n",
        "  dataset_train = dataset[:split]\n",
        "  dataset_test = dataset[split:]\n",
        "  assert(dataset_train[0].x[0,:].numel() == INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM)\n",
        "\n",
        "  write_file(\"dataset.pickle\", (dataset_train, dataset_test))\n",
        "  return dataset_train, dataset_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVaAhhMoySLZ"
      },
      "outputs": [],
      "source": [
        "def generate_schedule(start = START, end = END, timesteps=TIMESTEPS):\n",
        "  \"\"\"\n",
        "  Generates a schedule of beta and alpha values for a forward process.\n",
        "\n",
        "  Args:\n",
        "  start (float): The starting value for the beta values. Default is START.\n",
        "  end (float): The ending value for the beta values. Default is END.\n",
        "  timesteps (int): The number of timesteps to generate. Default is TIMESTEPS.\n",
        "\n",
        "  Returns:\n",
        "  tuple: A tuple of three tensors containing the beta values, alpha values, and\n",
        "  cumulative alpha values (alpha bars).\n",
        "  \"\"\"\n",
        "  betas = torch.linspace(start, end, timesteps, device = DEVICE)\n",
        "  #alphas = 1.0 - betas\n",
        "  #alpha_bars = torch.cumprod(alphas, axis=0)\n",
        "  assert(betas.numel() == TIMESTEPS)\n",
        "  return betas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1QHrqCDxdwY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "def visualize_smiles_from_file(filepath):\n",
        "    # Read SMILES from file\n",
        "    with open(filepath, 'r') as file:\n",
        "        smiles_list = [line.strip() for line in file.readlines()]\n",
        "\n",
        "    # Convert SMILES to RDKit Mol objects, filtering out invalid ones\n",
        "    mols = [Chem.MolFromSmiles(smile) for smile in smiles_list[:100]]\n",
        "    mols = [mol for mol in mols if mol is not None]\n",
        "\n",
        "    # Determine grid size\n",
        "    num_mols = len(mols)\n",
        "    cols = 10\n",
        "    rows = min(10, -(-num_mols // cols))  # ceil division\n",
        "\n",
        "    # Create a subplot grid\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(20, 20),\n",
        "                            gridspec_kw={'wspace': 0.3, 'hspace': 0.3})\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            ax = axs[i, j]\n",
        "            ax.axis(\"off\")  # hide axis\n",
        "            idx = i * cols + j  # index in mols list\n",
        "            if idx < num_mols:\n",
        "                img = Draw.MolToImage(mols[idx], size=(200, 200))\n",
        "                ax.imshow(img)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    # Save the figure\n",
        "    plt.savefig(filepath + '.jpg', format='jpg', bbox_inches='tight')\n",
        "    plt.close(fig)  # Close the figure after saving to free up memory\n",
        "    try:\n",
        "        time.sleep(0.01)\n",
        "        wandb.log_artifact(filepath + '.jpg', name=f\"jpg_{SWEEP_ID}_{filepath.replace('.','')}\", type=\"smiles_grid_graph\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        pass\n",
        "\n",
        "# Example usage:\n",
        "# Replace YOUR_FILE_PATH with the path to your SMILES file.\n",
        "# visualize_smiles_from_file(YOUR_FILE_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnya1MDuxseM"
      },
      "source": [
        "# Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1FxjM0jyd1k"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import PNA\n",
        "from torch_geometric.utils import degree\n",
        "\n",
        "\n",
        "def dataset_to_degree_bin(train_dataset):\n",
        "  try:\n",
        "    deg = load_file('deg.pickle')\n",
        "    deg = deg.to(DEVICE)\n",
        "    return deg\n",
        "  except Exception as e:\n",
        "    print(f\"Could not find degree bin due to error: {str(e)}, generate it now\")\n",
        "  assert(train_dataset is not None)\n",
        "\n",
        "\n",
        "  # Compute the maximum in-degree in the training data.\n",
        "  max_degree = -1\n",
        "  for data in train_dataset:\n",
        "    data = data.to(DEVICE)\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    max_degree = max(max_degree, int(d.max()))\n",
        "\n",
        "  deg = torch.zeros(max_degree + 1, dtype=torch.long, device=DEVICE)\n",
        "  for data in train_dataset:\n",
        "    data = data.to(DEVICE)\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    deg += torch.bincount(d, minlength=deg.numel())\n",
        "\n",
        "  write_file(\"deg.pickle\", deg.cpu())\n",
        "  return deg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PNAnet(torch.nn.Module):\n",
        "  def __init__(self, train_dataset=None, hidden_channels=HIDDEN_CHANNELS_PRED, depth=DEPTH_PRED, dropout=DROPOUT_PRED, towers=TOWERS_PRED, normalization=NORMALIZATION_PRED, pre_post_layers=1):\n",
        "    super(PNAnet, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Calculate x as the difference between mult_y and hidden_dim\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1) #tod fix\n",
        "    #out_channels = towers * ((out_channels // towers) + 1)\n",
        "\n",
        "    in_channels = INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM+ TIME_FEATURE_DIM #INDICATOR_FEATURE_DIM entries are noise free\n",
        "    out_channels = FEATURE_DIM\n",
        "\n",
        "    deg = dataset_to_degree_bin(train_dataset)\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "    self.pnanet = PNA(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=hidden_channels, num_layers=depth, aggregators=aggregators, scalers=scalers, deg=deg, dropout=dropout, towers=towers, norm=self.normalization, pre_layers=pre_post_layers, post_layers=pre_post_layers)\n",
        "\n",
        "    self.final_mlp = Seq(Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, out_channels))\n",
        "\n",
        "\n",
        "  def forward(self, x_in, t, edge_index):\n",
        "    row_num = x_in.shape[0]\n",
        "    t = t.view(-1,TIME_FEATURE_DIM)\n",
        "    x = torch.concat((x_in, t), dim=1)\n",
        "    x = self.pnanet(x, edge_index)\n",
        "    x = self.final_mlp(x)\n",
        "    assert(x.numel() > 1 )\n",
        "    assert(x.shape[0] == row_num)\n",
        "\n",
        "    #node_indicator = x_in[:,0] > 0\n",
        "    #node_indicator = x_in[:,0] < 0\n",
        "    #x[node_indicator, NON_NODES] = x_in[node_indicator, NON_NODES]\n",
        "    #x[edge_indicator, NON_EDGES] = x_in[edge_indicator, NON_EDGES]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "#model = PNAnet([data])\n",
        "\n",
        "#model(data.x, data.edge_index, torch.ones(data.x.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSm4qgdXRlo0"
      },
      "outputs": [],
      "source": [
        "#path_pattern = \"aliamol_model_epoch_*.pth\"\n",
        "#sorted(glob.glob(path_pattern))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvkRRWDcrRqZ"
      },
      "outputs": [],
      "source": [
        "def load_latest_checkpoint(model, optimizer, loss_list, epoch_i, path_pattern=None):\n",
        "  if path_pattern is None:\n",
        "    path_pattern = PATH_PATTERN + \"_model_epoch_*.pth\"\n",
        "  try:\n",
        "    checkpoint_paths = sorted(glob.glob(path_pattern))\n",
        "    if len(checkpoint_paths) == 0:\n",
        "      return model, optimizer, loss_list, epoch_i\n",
        "\n",
        "    latest_checkpoint_path = checkpoint_paths[-1]\n",
        "    checkpoint = torch.load(latest_checkpoint_path, map_location=DEVICE)\n",
        "\n",
        "    # Assuming model and optim are your initialized model and optimizer\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch_i = checkpoint['epoch']\n",
        "    loss_list = checkpoint['loss_list']\n",
        "    print(f\"read checkpoint of epoch {epoch_i:08} from disc.\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  return model, optimizer, loss_list, epoch_i\n",
        "\n",
        "def save_model(model, optimizer, loss_list, epoch_i, upload=False):\n",
        "  if epoch_i == 0:\n",
        "    return\n",
        "  save_path = f\"{PATH_PATTERN}_model_epoch_{epoch_i:08}.pth\"\n",
        "\n",
        "  # Save the model state dict and the optimizer state dict in a dictionary\n",
        "  torch.save({\n",
        "              'epoch': epoch_i,\n",
        "              'loss_list': loss_list,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict()\n",
        "              }, save_path)\n",
        "  if upload:\n",
        "    try:\n",
        "      wandb.log_artifact(save_path, name=f\"src_txt_{SWEEP_ID}_{epoch_i:08}_weightfile\", type=\"weight\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDm63cFIrG8K"
      },
      "outputs": [],
      "source": [
        "def load_base_model(dataset_train, path_pattern=None):\n",
        "  model_base = PNAnet(dataset_train)\n",
        "  model_base = model_base.to(DEVICE)\n",
        "  loss_list = None\n",
        "  optimizer = Adam(model_base.parameters(), lr = LEARNING_RATE_GEN)\n",
        "  model_base, optimizer, loss_list, epoch_start = load_latest_checkpoint(model_base, optimizer, loss_list, epoch_i=0, path_pattern=path_pattern)\n",
        "\n",
        "  return model_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4NRBuWuxUDl"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP3_DiRvGjKr"
      },
      "outputs": [],
      "source": [
        "def denoise_one_step_wild(model, g, i):\n",
        "  betas = generate_schedule()\n",
        "  t = TIMESTEPS - i - 1 # i=0 is full noise\n",
        "  beta_t = betas[t]\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphas_cumprod_t = alphas_cumprod[t]\n",
        "  sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1. - alphas_cumprod_t)\n",
        "  sqrt_recip_alphas_t = torch.sqrt(1.0 / alphas[t])\n",
        "  alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "  row_num = g.x.shape[0]\n",
        "\n",
        "  mask = torch.concat((torch.tensor([False]*g.x_old.shape[0], device=DEVICE).view(-1,1), g.x_old[:,1:]>-0.5), dim=1)\n",
        "  future_t = torch.tensor([float(t)] * g.x.shape[0], device=DEVICE).view(-1,1)\n",
        "\n",
        "  denoised_x = g.x.clone()\n",
        "  original_pred = model(g.x, future_t, g.edge_index)\n",
        "\n",
        "  #noise_pred = noise_pred.view(row_num, -1)\n",
        "  #x_with_noise = g.x[mask].view(row_num, -1)\n",
        "  #assert(noise_pred.shape == x_with_noise.shape)\n",
        "  #future_t = torch.tensor([int(t)] * g.x.shape[0], device=DEVICE).view(-1)\n",
        "  #original_pred = get_pred_from_noise(noise_pred, x_with_noise, future_t)\n",
        "\n",
        "  if t-1>0:\n",
        "    x_with_noise_again, _ = forward_diffusion(original_pred, t-1)\n",
        "    denoised_x[mask] = x_with_noise_again.flatten()\n",
        "  else:\n",
        "    denoised_x[mask] = original_pred.flatten()\n",
        "  return denoised_x\n",
        "\n",
        "\n",
        "\n",
        "  #x_in = g.x[mask].flatten()\n",
        "  #original_pred = get_pred_from_noise(noise_pred, x_in, future_t)\n",
        "  ##original_pred = (x_in - torch.sqrt(1. - alphas_cumprod_t) * noise_pred)/torch.sqrt(alphas_cumprod_t)\n",
        "  #assert(original_pred.shape[0] = x_in.shape[0])\n",
        "  #x = g.x.clone()\n",
        "  #x[mask] = original_pred\n",
        "  #if t-1 <= 0:\n",
        "  #  return x\n",
        "  #x_with_noise_again, _ = forward_diffusion(x, t-1)\n",
        "  #denoised_x[mask] = x_with_noise_again[mask]\n",
        "  #return denoised_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbcU3sZBxqwh"
      },
      "outputs": [],
      "source": [
        "def denoise_one_step(model, g, i):\n",
        "  row_num = g.x.shape[0]\n",
        "\n",
        "  betas = generate_schedule()\n",
        "  t = TIMESTEPS - i - 1 # i=0 is full noise\n",
        "  beta_t = betas[t]\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphas_cumprod_t = alphas_cumprod[t]\n",
        "  sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1. - alphas_cumprod_t)\n",
        "  sqrt_recip_alphas_t = torch.sqrt(1.0 / alphas[t])\n",
        "  alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "\n",
        "\n",
        "  mask = torch.concat((torch.tensor([False]*g.x_old.shape[0], device=DEVICE).view(-1,1), g.x_old[:,1:]>-0.5), dim=1)\n",
        "\n",
        "  future_t = torch.tensor([float(t)] * g.x.shape[0], device=DEVICE).view(-1,1)\n",
        "\n",
        "  original_pred = model(g.x, future_t, g.edge_index)\n",
        "\n",
        "  x_with_noise = g.x[mask].view(row_num, -1)\n",
        "  future_t = torch.tensor([int(t)] * g.x.shape[0], device=DEVICE).view(-1)\n",
        "  noise_pred = get_noise_from_pred(original_pred, x_with_noise, future_t)\n",
        "\n",
        "  values_now = g.x[mask].view(row_num, -1)\n",
        "  values_endpoint = noise_pred.view(row_num, -1)#[mask] network only prdicts noise\n",
        "\n",
        "  assert(values_now.shape == values_endpoint.shape)\n",
        "\n",
        "  # now compute values_one_step_denoised\n",
        "  model_mean = sqrt_recip_alphas_t * (values_now - beta_t * values_endpoint / sqrt_one_minus_alphas_cumprod_t)\n",
        "  values_one_step_denoised = model_mean # if t == 0\n",
        "  if t != 0:\n",
        "    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) # in the paper this is in 3.2. note that sigma^2 is variance, not std\n",
        "    posterior_std_t = torch.sqrt(posterior_variance[t])\n",
        "    noise = torch.randn_like(values_now, device = DEVICE)\n",
        "    values_one_step_denoised = model_mean + posterior_std_t * noise\n",
        "\n",
        "  denoised_x = g.x.clone()\n",
        "  denoised_x[mask] = values_one_step_denoised.flatten()\n",
        "  return denoised_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSHkoX_-xngv"
      },
      "outputs": [],
      "source": [
        "def overwrite_with_noise(g):\n",
        "  g.x_old = g.x.clone()\n",
        "  mask = torch.concat((torch.tensor([False]*g.x_old.shape[0], device=DEVICE).view(-1,1), g.x_old[:,1:]>-0.5), dim=1)\n",
        "  g.x[mask] = torch.randn_like(g.x[mask])\n",
        "  return g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbvUY6H2xaWb"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def generate_examples(model, dataset_train, num=100,wild=False):\n",
        "  # Setup\n",
        "  print(\"generate samples batched\")\n",
        "  model.eval()\n",
        "  dataset_train_start = list()\n",
        "  while len(dataset_train_start) < num:\n",
        "    g = dataset_train[random.sample(range(len(dataset_train)),1)[0]]\n",
        "    dataset_train_start.append(g.clone().to(DEVICE))\n",
        "    g = dataset_train_start[-1]\n",
        "  assert(len(dataset_train_start) == num)\n",
        "  dataloader = DataLoader(dataset_train_start, batch_size = num)\n",
        "\n",
        "  # Inference\n",
        "  for g in dataloader:\n",
        "    g = g.to(DEVICE)\n",
        "    print(\"load g\", g, g.batch)\n",
        "    g = overwrite_with_noise(g)\n",
        "    for i in tqdm(range(TIMESTEPS)):\n",
        "      t = int(TIMESTEPS-i-1)\n",
        "      if wild:\n",
        "        x_with_less_noise = denoise_one_step_wild(model, g, i)\n",
        "      else:\n",
        "        x_with_less_noise = denoise_one_step(model, g, i)\n",
        "      g.x = x_with_less_noise\n",
        "\n",
        "    graph_list = g.to_data_list()\n",
        "    graph_list = [g.cpu() for g in graph_list]\n",
        "\n",
        "    print(\"generated graphs \", graph_list[:10])\n",
        "    return graph_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9AAQNu54u5f"
      },
      "source": [
        "#### Frac Correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na5iiitt4w63"
      },
      "outputs": [],
      "source": [
        "def find_frac_correct(graphs):\n",
        "  correct = 0\n",
        "  smiles_list = list()\n",
        "  for i, g in tqdm(list(enumerate(graphs))):\n",
        "    smiles = pyg_to_smiles(g)\n",
        "    if smiles is not None and '.' not in smiles:\n",
        "      mol = Chem.MolFromSmiles(smiles)\n",
        "      if mol is not None:\n",
        "        correct += 1\n",
        "        smiles_list.append((smiles, i))\n",
        "\n",
        "  frac_correct = correct/len(graphs)\n",
        "  unique_frac = len(list(set(smiles_list)))/len(graphs)\n",
        "  return frac_correct, smiles_list, unique_frac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr8BWfguzgy5"
      },
      "source": [
        "### Gen many graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2mkhyZ2xMbc"
      },
      "outputs": [],
      "source": [
        "#!ls aliamol2*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeHCO82gziKT"
      },
      "outputs": [],
      "source": [
        "def gen_graphs(num_per_generation=1000, num_generations=40, wild=False, path_pattern=None):\n",
        "  if DEBUG:\n",
        "    num_generations = int(num_generations/10)\n",
        "  if path_pattern is None:\n",
        "    path_pattern = PATH_PATTERN+\"_model_epoch_*.pth\" #\"aliamol_model_epoch_*.pth\"\n",
        "  path = sorted(glob.glob(path_pattern))[-1]\n",
        "  num_samples = num_per_generation*num_generations\n",
        "  filepath = path.replace(\".pth\", f'_{num_samples:06d}_w{wild}_generated.pickle')\n",
        "\n",
        "  results = list()\n",
        "  try:\n",
        "    results = load_file(filepath)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  if len(results) == num_per_generation*num_generations:\n",
        "    return results\n",
        "\n",
        "  dataset_base, dataset_base_test = build_dataset()\n",
        "  scatter_list = list()\n",
        "  model_base = load_base_model(dataset_base, path_pattern = path)\n",
        "\n",
        "  i = 0\n",
        "  while len(results) < num_samples:\n",
        "    i += 1\n",
        "    num = max(num_per_generation, len(results) - num_samples)\n",
        "    graphs = generate_examples(model_base, dataset_base, num=num, wild=wild)\n",
        "    results = results + graphs\n",
        "    if i % 5 == 0 or len(results) >= num_samples:\n",
        "      write_file(filepath, results)\n",
        "\n",
        "  assert(len(results) == num_per_generation*num_generations)\n",
        "  return results\n",
        "\n",
        "\n",
        "\n",
        "def test_graph_generation(path_pattern=None, wild=False):\n",
        "  generated_graphs = gen_graphs(wild=wild, path_pattern=path_pattern)\n",
        "  return find_frac_correct(generated_graphs) #0.54 #0.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0p9ss6BJ_UM"
      },
      "outputs": [],
      "source": [
        "#test_graph_generation(path_pattern=\"aliamol_model_epoch_00003901.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7dJOf1G5O0K"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEo07bzN4bAP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfJgXwe15QG4"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import PNA\n",
        "\n",
        "\n",
        "class PNAdisc(torch.nn.Module):\n",
        "  def __init__(self, train_dataset=None, hidden_channels=HIDDEN_CHANNELS_DISC, depth=DEPTH_DISC, dropout=DROPOUT_DISC, towers=1, normalization=NORMALIZATION_DISC, pre_post_layers=1):\n",
        "    super(PNAdisc, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1)\n",
        "\n",
        "    in_channels = INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM\n",
        "    assert in_channels == 11\n",
        "    deg = dataset_to_degree_bin(train_dataset)\n",
        "    deg = deg.to(DEVICE)\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "    self.pnanet = PNA(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=1, num_layers=depth, aggregators=aggregators, scalers=scalers, deg=deg, dropout=dropout, towers=towers, norm=self.normalization, pre_layers=pre_post_layers, post_layers=pre_post_layers)\n",
        "    #self.pnanet = PNA(in_channels=11, hidden_channels=hidden_channels, out_channels=1, num_layers=depth, aggregators=aggregators, scalers=scalers, deg=deg)\n",
        "\n",
        "    #self.final_mlp = Seq(Lin(hidden_channels, hidden_channels), nn.ReLU(),Lin(hidden_channels, hidden_channels), nn.ReLU(), Lin(hidden_channels, 1))\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, batch=None):\n",
        "    #print(\"before: x.shape\",x.shape, \"edge_index.shape\",edge_index.shape)\n",
        "    x = x + torch.randn_like(x)*DISC_NOISE\n",
        "    x = self.pnanet(x, edge_index)\n",
        "    #print(\"after: x.shape\",x.shape, \"edge_index.shape\",edge_index.shape)\n",
        "    x = global_mean_pool(x, batch)\n",
        "    #x = torch.sum(x)\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91xp2SGvGTSU"
      },
      "outputs": [],
      "source": [
        "def train_epoch_disc(model_disc, dataloader, optimizer):\n",
        "  model_disc.train()\n",
        "  start_time = time.time()\n",
        "  loss_list = list()\n",
        "  acc_list = list()\n",
        "  for batch in dataloader:\n",
        "    batch = batch.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    #print(\"batch.x, batch.edge_index, batch.batch\", batch, batch.x, batch.edge_index, batch.batch)\n",
        "    pred = model_disc(batch.x, batch.edge_index, batch.batch)\n",
        "    #print(\"pred \",pred, \"y \", batch.y)\n",
        "    loss = F.binary_cross_entropy(pred.flatten(), batch.y.flatten())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    acc = (torch.abs(pred.flatten()-batch.y.flatten()) < 0.5).float()\n",
        "    acc_list = acc_list + acc.detach().cpu().tolist()\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "  return np.mean(loss_list), np.mean(acc_list), time.time()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZcRLtjSKIfA"
      },
      "outputs": [],
      "source": [
        "def test_disc(model_disc, dataloader):\n",
        "  model_disc.eval()\n",
        "  start_time = time.time()\n",
        "  loss_list = list()\n",
        "  acc_list = list()\n",
        "  for batch in dataloader:\n",
        "    batch = batch.to(DEVICE)\n",
        "    pred = model_disc(batch.x, batch.edge_index, batch.batch)\n",
        "    loss = F.binary_cross_entropy(pred.flatten(), batch.y.flatten())\n",
        "    acc = (torch.abs(pred.flatten()-batch.y.flatten()) < 0.5).float()\n",
        "    acc_list = acc_list + acc.detach().cpu().tolist()\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "  return np.mean(loss_list), np.mean(acc_list), time.time()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaC-s2FMJkPq"
      },
      "outputs": [],
      "source": [
        "def train_disc_model(dataloader_disc, dataloader_disc_test, round_i):\n",
        "  model_disc = PNAdisc(dataloader_disc)\n",
        "  model_disc = model_disc.to(DEVICE)\n",
        "  weight_path = f\"discriminator_model_{round_i:05}.pth\"\n",
        "\n",
        "  try:\n",
        "    checkpoint = torch.load(weight_path)\n",
        "    model_disc.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"found disc model in round {round_i:05}\")\n",
        "    return model_disc\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  epochs = list()\n",
        "  losses_train = list()\n",
        "  losses_test = list()\n",
        "\n",
        "  optimizer_disc = Adam(model_disc.parameters(), lr = 0.0001)\n",
        "  for epoch_i in range(EPOCHS_DISC_MODEL):\n",
        "    loss_train, acc_train, t_train = train_epoch_disc(model_disc, dataloader_disc, optimizer_disc)\n",
        "    if epoch_i % 10 == 1 or epoch_i == EPOCHS_DISC_MODEL-1:\n",
        "      loss_test, acc_test, t_test = test_disc(model_disc, dataloader_disc_test)\n",
        "      #print(loss_train,loss_test,acc_train,acc_test,t_train)\n",
        "      print(f\"train discriminator: epoch: {epoch_i:05}, loss: {loss_train:02.4f}, loss test: {loss_test:02.4f}, acc: {acc_train:01.3f}, acc test: {acc_test:01.3f}, time: {t_train:01.3f}\")\n",
        "      epochs.append(epoch_i)\n",
        "      losses_train.append(loss_train)\n",
        "      losses_test.append(loss_test)\n",
        "      plt.clf()\n",
        "      plt.plot(epochs, losses_train, label='train')\n",
        "      plt.plot(epochs, losses_test, label='test')\n",
        "      plt.legend()\n",
        "      plt.savefig(f\"discriminator_model_{round_i:05}.png\")\n",
        "\n",
        "  torch.save({'model_state_dict': model_disc.state_dict(), 'epochs': epochs, \"losses_train\": losses_train, \"losses_test\": losses_test}, weight_path)\n",
        "  return model_disc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH85qTaDKkF7"
      },
      "outputs": [],
      "source": [
        "def run_disc(round_i=1):\n",
        "  fake_graphs = gen_graphs(wild=True)\n",
        "  dataset_base, dataset_base_test = build_dataset()\n",
        "  real_graphs = random.sample(dataset_base, len(fake_graphs))\n",
        "  dataset = list()\n",
        "\n",
        "  for g in fake_graphs:\n",
        "    g_i = g.clone()\n",
        "    g_i.y = torch.tensor(0.0)\n",
        "    dataset.append(g_i)\n",
        "\n",
        "  for g in real_graphs:\n",
        "    g_i = g.clone()\n",
        "    g_i.y = torch.tensor(1.0)\n",
        "    dataset.append(g_i)\n",
        "\n",
        "  random.shuffle(dataset)\n",
        "  cut_off = int(len(dataset) * 0.8)\n",
        "  dataloader_train = DataLoader(dataset[:cut_off], batch_size = BATCH_SIZE, shuffle=True)\n",
        "  dataloader_test = DataLoader(dataset[cut_off:], batch_size = BATCH_SIZE, shuffle=True)\n",
        "\n",
        "  model_disc = train_disc_model(dataloader_train, dataloader_test, round_i)\n",
        "  return model_disc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLHpXu-W6GbJ"
      },
      "outputs": [],
      "source": [
        "#model_disc = run_disc() #0000390 is the last good one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF7pU0QnPQMb"
      },
      "source": [
        "# Forward Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZQ5kwb5PTkC",
        "outputId": "75fd722f-0c70-477b-9fd4-f8302cb5c244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((tensor([[0.9948],\n",
              "          [1.9985],\n",
              "          [0.1775]], device='cuda:0'),\n",
              "  tensor([[-0.5115],\n",
              "          [-0.1381],\n",
              "          [ 0.1100]], device='cuda:0')),\n",
              " None,\n",
              " (tensor([[ 0.7344],\n",
              "          [-0.3833],\n",
              "          [-1.2735]], device='cuda:0'),\n",
              "  tensor([[ 0.7121],\n",
              "          [-0.4284],\n",
              "          [-1.3413]], device='cuda:0')))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def forward_diffusion(node_features, future_t):\n",
        "  \"\"\"\n",
        "  Performs a forward diffusion process on an node_features tensor.\n",
        "  Each row can theoreetically have its own future time point.\n",
        "  Implements the second equation from https://youtu.be/a4Yfz2FxXiY?t=649\n",
        "  \"\"\"\n",
        "  row_num = node_features.shape[0]\n",
        "\n",
        "  if \"class 'int'\" in str(type(future_t)) or \"class 'float'\" in str(type(future_t)):\n",
        "    future_t = torch.tensor([int(future_t)] * row_num).to(DEVICE)\n",
        "\n",
        "  feature_dim = node_features.shape[1]\n",
        "  future_t = future_t.view(-1)\n",
        "  assert(row_num == future_t.numel())\n",
        "  assert(future_t[0] == future_t[1]) #lets assume the belong to the same graph\n",
        "\n",
        "  betas = generate_schedule()\n",
        "\n",
        "  noise = torch.randn_like(node_features, device=DEVICE)\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t).view(row_num, 1)\n",
        "  assert(alphabar_t.numel() == row_num)\n",
        "\n",
        "  new_node_features_mean = torch.sqrt(alphabar_t) * node_features # column-wise multiplication, now matrix #todo but we want row wise #.view(row_num,1)\n",
        "  assert(new_node_features_mean.shape == node_features.shape)\n",
        "  new_node_features_std = torch.sqrt(1.-alphabar_t) #this is a col vector\n",
        "  new_node_features_std = new_node_features_std.repeat(1,feature_dim) #this is a matrix\n",
        "  assert(new_node_features_mean.shape == new_node_features_std.shape)\n",
        "  noisey_node_features =  new_node_features_mean + new_node_features_std * noise\n",
        "\n",
        "  return noisey_node_features, noise\n",
        "\n",
        "forward_diffusion(torch.tensor([1,2,3.], device=DEVICE).view(3,1), torch.tensor([0,0,999], device=DEVICE)), print(\"\"), forward_diffusion(torch.tensor([1,2,3.], device=DEVICE).view(3,1), torch.tensor([999,999,999], device=DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihPbotsmRafu"
      },
      "source": [
        "# Train Jointly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG8AOy2CfG8F"
      },
      "outputs": [],
      "source": [
        "def get_pred_from_noise(noise_pred, x_with_noise, future_t):\n",
        "\n",
        "  row_num = x_with_noise.shape[0]\n",
        "  betas = generate_schedule()\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t).view(row_num, 1)\n",
        "\n",
        "  scaled_noise = torch.sqrt(1.0-alphabar_t)\n",
        "  x_without_noise = x_with_noise - scaled_noise*noise_pred\n",
        "  x_without_noise = x_without_noise/torch.sqrt(alphabar_t)\n",
        "  return x_without_noise\n",
        "\n",
        "\n",
        "def get_noise_from_pred(original_pred, x_with_noise, future_t):\n",
        "\n",
        "  row_num = x_with_noise.shape[0]\n",
        "  betas = generate_schedule()\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t).view(row_num, 1)\n",
        "\n",
        "  scaled_noise = torch.sqrt(alphabar_t)\n",
        "  noise = x_with_noise - scaled_noise*original_pred\n",
        "  noise = noise / torch.sqrt(1.0-alphabar_t)\n",
        "\n",
        "  return noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBBNjxFxRZef"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, model_disc=None):\n",
        "  schedule = generate_schedule()\n",
        "  model.train()\n",
        "  start_time = time.time()\n",
        "  loss_list = list()\n",
        "  loss_list_start = list()\n",
        "  loss_row = nn.MSELoss(reduction='none')\n",
        "\n",
        "  for batch in tqdm(dataloader): #todo batches deactivated\n",
        "    if batch.x.shape[0] < 2:\n",
        "      continue\n",
        "    optimizer.zero_grad()\n",
        "    batch.to(DEVICE)\n",
        "    row_num = batch.x.shape[0]\n",
        "\n",
        "    num_graphs_in_batch = int(torch.max(batch.batch).item()+1)\n",
        "    future_t_select = torch.randint(0, TIMESTEPS, (num_graphs_in_batch,), device = DEVICE)\n",
        "    future_t = torch.gather(future_t_select, 0, batch.batch)\n",
        "    assert(future_t.numel() == row_num)\n",
        "\n",
        "    mask = torch.concat((torch.tensor([False]*row_num, device=DEVICE).view(-1,1), batch.x[:,1:]>-0.5), dim=1) #this only works on original values\n",
        "    x_start_gt = batch.x[mask].view(row_num, FEATURE_DIM)\n",
        "    x_with_noise, noise_gt = forward_diffusion(x_start_gt, future_t)\n",
        "\n",
        "    x_in = batch.x.clone()\n",
        "    x_in[mask] = x_with_noise.flatten()\n",
        "    x_start_pred = model(x_in, future_t, batch.edge_index)\n",
        "    loss = F.mse_loss(x_start_gt, x_start_pred)\n",
        "\n",
        "\n",
        "    #row_num = x_in.shape[0]\n",
        "    #assert(x_with_noise.shape[0] == row_num)\n",
        "   # assert(noise_pred.shape[0] == row_num)\n",
        "    #assert(noise_pred.shape == x_with_noise.shape)\n",
        "    #assert(noise_pred.shape == noise_gt.shape)\n",
        "    #assert(noise_pred.shape == x_start_gt.shape)\n",
        "    #x_start_pred = get_pred_from_noise(noise_pred, x_with_noise, future_t)\n",
        "\n",
        "    #assert(F.mse_loss(get_pred_from_noise(noise_gt, x_with_noise, future_t), x_start_gt) < 0.00001)\n",
        "\n",
        "    #loss = F.mse_loss(noise_gt, noise_pred)\n",
        "    #loss_start = F.mse_loss(x_start_gt, x_start_pred)  #multiply with torch.sqrt(1.0-alphabar_t)  #F.mse_loss(x_start_gt, x_start_pred)  # torch.sum(F.mse_loss(x_start_gt, x_start_pred, dim=1)/future_t) #torch.sum(torch.sum((x_start_gt- x_start_pred)**2,dim=1) / (1+future_t.view(-1,1)))\n",
        "    #loss_agg = loss + 0.5*loss_start\n",
        "\n",
        "    #x_in = batch.x.clone()\n",
        "    #x_in[mask] = x_start_pred.flatten()\n",
        "    #disc_loss = torch.abs(1.0- model_disc(x_in, batch.edge_index, batch=batch.batch))\n",
        "    #disc_loss = torch.mean(disc_loss)\n",
        "    #loss_agg = loss + 0.25*disc_loss\n",
        "\n",
        "    disc_loss = torch.tensor(0.0, device=DEVICE)\n",
        "    if model_disc is not None:\n",
        "      x_in[mask] = x_start_pred.flatten()\n",
        "      disc_loss = torch.mean((1.0- model_disc(x_in, batch.edge_index, batch=batch.batch))**2)\n",
        "      loss = (1.0 - GAMMA) * loss + GAMMA*disc_loss\n",
        "\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    loss_list.append(loss.item())\n",
        "    loss_list_start.append(disc_loss.item())\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  return np.mean(loss_list),np.mean(loss_list_start), time.time()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47jE9PU9bVBC"
      },
      "outputs": [],
      "source": [
        "def log_smiles(smiles, filename):\n",
        "  try:\n",
        "    with open(filename, \"w\") as file:\n",
        "      for string in smiles:\n",
        "        file.write(str(string) + \"\\n\")\n",
        "    wandb.log_artifact(filename, name=f\"src_txt_{SWEEP_ID}_{filename}\", type=\"smiles\")\n",
        "    time.sleep(0.01)\n",
        "    visualize_smiles_from_file(filename)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQU6xUvvRf6t"
      },
      "outputs": [],
      "source": [
        "def train_base_model(train_loader, epoch_num=EPOCHS_GEN, model_disc=None):\n",
        "  print(\"train base model\")\n",
        "  if DEBUG:\n",
        "    epoch_num = int(epoch_num/10)\n",
        "\n",
        "  dataset_train = train_loader.dataset\n",
        "  model_base = PNAnet(dataset_train)\n",
        "  model_base = model_base.to(DEVICE)\n",
        "\n",
        "  optimizer = Adam(model_base.parameters(), lr = LEARNING_RATE_GEN*0.01) #ok makes no sense\n",
        "  loss_list = list()\n",
        "  model_base, optimizer, loss_list, epoch_start = load_latest_checkpoint(model_base, optimizer, loss_list, epoch_i=0)\n",
        "\n",
        "  epoch_start = min(epoch_start, epoch_num)\n",
        "  print(\"from\", epoch_start, \"to\", epoch_num)\n",
        "\n",
        "\n",
        "  for epoch_i in range(epoch_start,epoch_num):\n",
        "    try:\n",
        "      loss, loss_start, time_elapsed = train_epoch(model_base, train_loader, optimizer, model_disc=model_disc)\n",
        "      loss_list.append((epoch_i, loss))\n",
        "      if epoch_i % 1 == 0 or epoch_i == epoch_num - 1 or BATCH_SIZE == 1:\n",
        "        #plot_list(loss_list, \"train_base.png\", title=\"train loss base model\", xlabel='epoch', ylabel='loss')\n",
        "        mean_loss = np.mean([y for x,y in loss_list] + [loss])\n",
        "        print(f\"loss in epoch {epoch_i:07} is: {loss:05.4f} with mean loss {mean_loss:05.4f} with start loss {loss_start:05.4f} with runtime {time_elapsed:05.4f}\")\n",
        "        log({\"step\": epoch_i, \"epoch\": epoch_i, \"loss\": loss, \"mean_loss\": mean_loss, \"start_loss\": loss_start, \"runtime\": time_elapsed})\n",
        "\n",
        "      if (epoch_i % 20 == 0 and epoch_i > 0) or epoch_i == epoch_num - 1 or BATCH_SIZE == 1:\n",
        "        #graphs = generate_examples(model_base, epoch_i, betas, dataset_train)\n",
        "        #graph_loss_list.append(compute_generation_loss(graphs, None))\n",
        "        #print(f\"generation loss: {graph_loss_list[-1]:06.4f}\")\n",
        "        #plot_base(graph_loss_list, loss_list)\n",
        "        #pass\n",
        "        print(\"save\")\n",
        "        save_model(model_base, optimizer, loss_list, epoch_i+1, upload=epoch_i % 100 == 0 and epoch_i>9) #todo really +1?\n",
        "        time.sleep(0.1)\n",
        "        frac, smiles_list, unique_frac = test_graph_generation(wild=False)\n",
        "        frac_wild, smiles_list_wild, unique_frac_wild = test_graph_generation(wild=True)\n",
        "        print(\"frac correct graphs: \", frac, \"with wild inference\", frac_wild)\n",
        "        log({\"step\": epoch_i, \"epoch\": epoch_i, \"frac_normal\": frac, \"frac_wild\": frac_wild, \"frac_normal_unique\": unique_frac, \"frac_wild_unique\": unique_frac_wild})\n",
        "        log_smiles(smiles_list, f\"{PATH_PATTERN}_smiles_{epoch_i}_normal.txt\")\n",
        "        log_smiles(smiles_list_wild, f\"{PATH_PATTERN}_smiles_{epoch_i}_wild.txt\")\n",
        "        try:\n",
        "          print(smiles_list[:20])\n",
        "          print(smiles_list_wild[:20])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "          pass\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"An error occurred during training: \\n\", str(e))\n",
        "      traceback.print_exc()\n",
        "      raise e\n",
        "\n",
        "\n",
        "  return model_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBLiBcZLRokF"
      },
      "outputs": [],
      "source": [
        "def start_experiments():\n",
        "  global DISC_NOISE\n",
        "  dataset_base, dataset_base_test = build_dataset()\n",
        "  dataloader_base = DataLoader(dataset_base, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  model_base = train_base_model(dataloader_base, epoch_num = EPOCHS_GEN*1)\n",
        "\n",
        "  if BASELINE:\n",
        "    model_disc = None\n",
        "  else:\n",
        "    model_disc = run_disc(round_i=1)\n",
        "  model_base = train_base_model(dataloader_base, epoch_num = EPOCHS_GEN*2, model_disc=model_disc)\n",
        "\n",
        "  DISC_NOISE = DISC_NOISE\n",
        "  if BASELINE:\n",
        "    model_disc = None\n",
        "  else:\n",
        "    model_disc = run_disc(round_i=2)\n",
        "  model_base = train_base_model(dataloader_base, epoch_num = EPOCHS_GEN*3, model_disc=model_disc)\n",
        "\n",
        "  DISC_NOISE = DISC_NOISE*0.5\n",
        "  if BASELINE:\n",
        "    model_disc = None\n",
        "  else:\n",
        "    model_disc = run_disc(round_i=3)\n",
        "  model_base = train_base_model(dataloader_base, epoch_num = EPOCHS_GEN*4, model_disc=model_disc)\n",
        "\n",
        "  DISC_NOISE = DISC_NOISE*0.5\n",
        "  if BASELINE:\n",
        "    model_disc = None\n",
        "  else:\n",
        "    model_disc = run_disc(round_i=4)\n",
        "  model_base = train_base_model(dataloader_base, epoch_num = EPOCHS_GEN*4, model_disc=model_disc)\n",
        "\n",
        "  save_src_file() # do it again\n",
        "  return  model_base\n",
        "\n",
        "\n",
        "#0000390 is the last good one\n",
        "\n",
        "#model_base = start_experiments()# loss in epoch 0000410 is: 0.0486 with mean loss 0.0643 with start loss 1.6791 with runtime 18.3035"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFXyZBfWMfxo"
      },
      "outputs": [],
      "source": [
        "#!rm aliamol_model_epoch_00004001_010000_generated.pickle aliamol_model_epoch_00004001.pth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpKbb3ArSqg0"
      },
      "source": [
        "### With WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "719HFLnH3Eqm",
        "outputId": "5508158f-f7a6-4701-e199-8020db10bd84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/usr/local/lib/python3.10/dist-packages/wandb']\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "print(wandb.__path__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF6EmzdCYJSZ"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"AliaMol\",\n",
        "    \"method\": \"random\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"ENZYMES/besttest_acc\",\n",
        "        \"goal\": \"maximize\",\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"BATCH_SIZE\": {\"values\": [128*2]},\n",
        "        \"GAMMA\": {\"values\": [0.1]},\n",
        "        \"DISC_NOISE\": {\"values\": [0.3]},  # unused\n",
        "        \"EPOCHS_DISC_MODEL\": {\"values\": [100]},\n",
        "        \"EPOCHS_GEN\": {\"values\": [100]},\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkqiydqAtMx7"
      },
      "outputs": [],
      "source": [
        "def save_src_file():\n",
        "  os.system(\"pip list > pip_list.txt 2>&1\")\n",
        "  for txt_file in sorted(glob.glob('*.txt')):\n",
        "    z = \"\".join(filter(str.isalnum, txt_file))\n",
        "    wandb.log_artifact(txt_file, name=f\"src_txt_{SWEEP_ID}_{z}\", type=\"my_dataset_txt\")\n",
        "  for python_file in sorted(glob.glob('*.ipynb')):\n",
        "    z = \"\".join(filter(str.isalnum, python_file))\n",
        "    wandb.log_artifact(python_file, name=f\"src_ipynb_{SWEEP_ID}_{z}\", type=\"my_dataset_ipynb\")\n",
        "  for python_file in sorted(glob.glob('*.py')):\n",
        "    z = \"\".join(filter(str.isalnum, python_file))\n",
        "    wandb.log_artifact(python_file, name=f\"src_py_{SWEEP_ID}_{z}\", type=\"my_dataset_py\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgG12j-1yQ3X"
      },
      "outputs": [],
      "source": [
        "#! cp ../Insa/api_key.txt api_key.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoYB0l4fwBUT"
      },
      "outputs": [],
      "source": [
        "#os.system('wandb login --relogin --host=https://api.wandb.ai --key='+get_wand_api_key())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJDIzKzmtQUJ"
      },
      "outputs": [],
      "source": [
        "def get_wand_api_key():\n",
        "  import sys\n",
        "  IN_COLAB = 'google.colab' in sys.modules\n",
        "  if not IN_COLAB:\n",
        "    os.system(\"cp ~/api_key.txt api_key.txt\")\n",
        "  file_path = 'api_key.txt'\n",
        "  with open(file_path, 'r') as file:\n",
        "      api_key = file.read().strip()\n",
        "  return api_key\n",
        "\n",
        "#wandb.login(key=get_wand_api_key())\n",
        "\n",
        "def main():\n",
        "  global PATH_PATTERN\n",
        "  with wandb.init() as run:\n",
        "    PATH_PATTERN = PATH_PATTERN_BASE + '_' +str(run.name) + '_' +str(BASELINE)\n",
        "    save_src_file()\n",
        "    for hyper_param_name in sweep_config['parameters']:\n",
        "      globals()[hyper_param_name] = run.config[hyper_param_name]\n",
        "      print(\"set \", hyper_param_name, \"=\", run.config[hyper_param_name])\n",
        "    return start_experiments()\n",
        "\n",
        "def start_with_wandb(set_baseline_true=False):\n",
        "  import wandb\n",
        "  global SWEEP_ID, USE_WANDB, PATH_PATTERN, BASELINE\n",
        "  if set_baseline_true:\n",
        "    BASELINE = True\n",
        "  USE_WANDB = True\n",
        "  os.environ[\"WANDB_MODE\"] = \"online\"\n",
        "  try:\n",
        "    SWEEP_ID = wandb.sweep(sweep_config, project=PROJECT_NAME)\n",
        "    wandb.agent(SWEEP_ID, function=main, count=5)\n",
        "  except Exception as e:\n",
        "    error_message = traceback.format_exc()\n",
        "    print(\"final error:\\n\", error_message)\n",
        "    with open('_error_log.txt', 'a') as f:\n",
        "      f.write(error_message + '\\n')\n",
        "    time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cxnsmGC7yLGX",
        "outputId": "3fb4a1a2-b9a3-4982-aa85-cefe05b8770b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 7ru13mbd\n",
            "Sweep URL: https://wandb.ai/nextaid/AliaMoleculePaper/sweeps/7ru13mbd\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1n572s8b with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH_SIZE: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDISC_NOISE: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_DISC_MODEL: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_GEN: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tGAMMA: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgerritgr\u001b[0m (\u001b[33mnextaid\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/colab/AliaMoleculePaper/wandb/run-20231019_073809-1n572s8b</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nextaid/AliaMoleculePaper/runs/1n572s8b' target=\"_blank\">comfy-sweep-1</a></strong> to <a href='https://wandb.ai/nextaid/AliaMoleculePaper' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nextaid/AliaMoleculePaper/sweeps/7ru13mbd' target=\"_blank\">https://wandb.ai/nextaid/AliaMoleculePaper/sweeps/7ru13mbd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nextaid/AliaMoleculePaper' target=\"_blank\">https://wandb.ai/nextaid/AliaMoleculePaper</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/nextaid/AliaMoleculePaper/sweeps/7ru13mbd' target=\"_blank\">https://wandb.ai/nextaid/AliaMoleculePaper/sweeps/7ru13mbd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nextaid/AliaMoleculePaper/runs/1n572s8b' target=\"_blank\">https://wandb.ai/nextaid/AliaMoleculePaper/runs/1n572s8b</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set  BATCH_SIZE = 256\n",
            "set  GAMMA = 0.2\n",
            "set  DISC_NOISE = 0.3\n",
            "set  EPOCHS_DISC_MODEL = 100\n",
            "set  EPOCHS_GEN = 100\n",
            "try to read  dataset.pickle\n",
            "train base model\n",
            "try to read  deg.pickle\n",
            "from 0 to 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/419 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch_geometric/warnings.py:17: UserWarning: The usage of `scatter(reduce='min')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(message)\n",
            "100%|| 419/419 [00:24<00:00, 16.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000000 is: 0.1524 with mean loss 0.1524 with start loss 0.0000 with runtime 24.9154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000001 is: 0.0945 with mean loss 0.1138 with start loss 0.0000 with runtime 22.0702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000002 is: 0.0847 with mean loss 0.1041 with start loss 0.0000 with runtime 21.6692\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000003 is: 0.0805 with mean loss 0.0985 with start loss 0.0000 with runtime 21.9890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000004 is: 0.0782 with mean loss 0.0947 with start loss 0.0000 with runtime 21.8843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000005 is: 0.0763 with mean loss 0.0918 with start loss 0.0000 with runtime 21.4271\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000006 is: 0.0746 with mean loss 0.0895 with start loss 0.0000 with runtime 21.5337\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000007 is: 0.0736 with mean loss 0.0876 with start loss 0.0000 with runtime 21.5182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000008 is: 0.0732 with mean loss 0.0861 with start loss 0.0000 with runtime 21.6208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000009 is: 0.0727 with mean loss 0.0848 with start loss 0.0000 with runtime 21.1935\n",
            "save\n",
            "try to read  aliamol_paper_comfy-sweep-1_True_model_epoch_00000010_004000_wFalse_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_comfy-sweep-1_True_model_epoch_00000010_004000_wFalse_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000010 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137468], x=[43154, 11], batch=[43154], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137928], x=[43287, 11], batch=[43287], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137276], x=[43102, 11], batch=[43102], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 112], x=[36, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137288], x=[43106, 11], batch=[43106], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "try to write  aliamol_paper_comfy-sweep-1_True_model_epoch_00000010_004000_wFalse_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:16<00:00, 247.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "try to read  aliamol_paper_comfy-sweep-1_True_model_epoch_00000010_004000_wTrue_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_comfy-sweep-1_True_model_epoch_00000010_004000_wTrue_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000010 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 136920], x=[42996, 11], batch=[42996], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 112], x=[36, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137100], x=[43054, 11], batch=[43054], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137348], x=[43122, 11], batch=[43122], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 17.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137868], x=[43271, 11], batch=[43271], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 17.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "try to write  aliamol_paper_comfy-sweep-1_True_model_epoch_00000010_004000_wTrue_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:15<00:00, 255.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frac correct graphs:  0.263 with wild inference 0.06775\n",
            "Number of rows must be a positive integer, not 0\n",
            "Number of rows must be a positive integer, not 0\n",
            "[('C1CC2CCC3CC23C1', 0), ('CC12CC13C1CC2C13C', 1), ('CCC1C2C(C)C3CC312', 5), ('CC1C2CCC3(C)C1C23', 7), ('CCCC(C)CC(C)C', 12), ('CCC1CCC(C)CC1', 25), ('CCCC(C)C', 30), ('CCCC1CCC(C)C1', 32), ('CC12C3C1C14CC3C12C4', 39), ('C1C2CC3C(C2)C32CC12', 49), ('CC12CC3(C1)C1CCC123', 55), ('CCCCCCC', 63), ('CC1(C2CC2)CCCC1', 66), ('CCCCCCC(C)C', 67), ('CCC(C)C1(C)C2CC21', 71), ('CC1CC23CC14CC2C43', 77), ('CCC12CC3CC1CC32', 78), ('CC(C)C12CC1CC2C', 81), ('CC12C3CCCC1C32', 85), ('CCCCC1(C)CCC1', 86)]\n",
            "[('CCC1CC2CC23CC13', 4), ('CC1CC(C(C)C)C1', 9), ('CCCCCC1CC1C', 31), ('CC12CCC34C5C1C53C24', 43), ('CCCCC(C)C(C)C', 56), ('CC1CC(C)(C)C12CC2', 64), ('CC1CCCC(C)CC1', 69), ('CCCC(CC)CC', 109), ('CCCC1CCCC1C', 123), ('CCC1(C)CCC1(C)C', 129), ('CC1CCCCC2CC12', 149), ('CCC(C)CC(C)CC', 161), ('CCCCC(C)CCC', 171), ('CCC(C)CC(C)CC', 180), ('CCC(CC)C1CC1C', 186), ('CC1CCCCC(C)C1', 209), ('CC1(CCC2CC2)CC1', 213), ('CCCCC(C)C(C)C', 214), ('CCCC1(CCC)CC1', 216), ('CCC(C)CC1CC1C', 217)]\n",
            "train base model\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000010 from disc.\n",
            "from 10 to 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000010 is: 0.0722 with mean loss 0.0837 with start loss 0.0000 with runtime 22.5231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000011 is: 0.0715 with mean loss 0.0827 with start loss 0.0000 with runtime 22.3263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000012 is: 0.0708 with mean loss 0.0818 with start loss 0.0000 with runtime 21.7951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000013 is: 0.0702 with mean loss 0.0810 with start loss 0.0000 with runtime 21.5354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000014 is: 0.0700 with mean loss 0.0803 with start loss 0.0000 with runtime 21.6094\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000015 is: 0.0698 with mean loss 0.0797 with start loss 0.0000 with runtime 21.6444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000016 is: 0.0697 with mean loss 0.0791 with start loss 0.0000 with runtime 21.6988\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000017 is: 0.0696 with mean loss 0.0786 with start loss 0.0000 with runtime 21.3130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000018 is: 0.0692 with mean loss 0.0781 with start loss 0.0000 with runtime 21.4519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000019 is: 0.0692 with mean loss 0.0777 with start loss 0.0000 with runtime 21.7796\n",
            "save\n",
            "try to read  aliamol_paper_comfy-sweep-1_True_model_epoch_00000020_004000_wFalse_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_comfy-sweep-1_True_model_epoch_00000020_004000_wFalse_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000020 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138176], x=[43358, 11], batch=[43358], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138236], x=[43375, 11], batch=[43375], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137800], x=[43251, 11], batch=[43251], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137100], x=[43051, 11], batch=[43051], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 17.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "try to write  aliamol_paper_comfy-sweep-1_True_model_epoch_00000020_004000_wFalse_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:15<00:00, 253.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "try to read  aliamol_paper_comfy-sweep-1_True_model_epoch_00000020_004000_wTrue_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_comfy-sweep-1_True_model_epoch_00000020_004000_wTrue_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000020 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138884], x=[43557, 11], batch=[43557], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137728], x=[43229, 11], batch=[43229], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 18.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 40], x=[15, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137856], x=[43268, 11], batch=[43268], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138084], x=[43331, 11], batch=[43331], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 17.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "try to write  aliamol_paper_comfy-sweep-1_True_model_epoch_00000020_004000_wTrue_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:15<00:00, 251.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frac correct graphs:  0.311 with wild inference 0.2695\n",
            "Number of rows must be a positive integer, not 0\n",
            "Number of rows must be a positive integer, not 0\n",
            "[('CCCCC1CC(C)C1', 6), ('CCOCCC1CC1', 8), ('OCC12CCC3(C1)OC23', 11), ('CCC(O)CCO', 14), ('CC1C2CC23C(C)OC13', 24), ('CC12C(O)C1C1CCC12', 30), ('CC1CC(C)C1CCO', 34), ('CCC12CC1OC2CO', 37), ('CC1C2COC1C2CO', 38), ('CC12COC3CC31O2', 40), ('OC1C(O)C2(O)CCC12', 41), ('OC1OC2COC3C1C23', 47), ('CCC(O)CC(O)CO', 49), ('CC12CC13CC1C(C3)C12', 53), ('CC12CC1OCC2CO', 55), ('CCC1CC2(C)CC12C', 56), ('CC1CCC23CC1C2C3', 60), ('OCC12CC13C2C31CO1', 62), ('CC12CCC3CC1C32', 63), ('CCC(C)CC(C)O', 64)]\n",
            "[('CC1C2C1C2(C)CCO', 0), ('CCC1(CC)CC1CO', 1), ('CC(O)C1CCC2CC21', 9), ('CCC(C)CC1CCC1', 12), ('CCC1CC2(C)CC1C2', 13), ('CC12CC3CCC1C32C', 19), ('CC1CC2C3CC23C1', 24), ('OCC12CC13CC1C2C13', 28), ('C1CC2C1C1C34CC3C214', 29), ('OC12COC3CC1CC32', 30), ('C1CC(CC2CC2)C1', 31), ('OCCCCCCCO', 34), ('OC1C2CC13CCC23', 41), ('CC1C2CCCC23CC13', 55), ('CC1C(C)C1(C)C1CC1', 58), ('OCC(O)C1CCCC1', 59), ('CCCCCCCCC', 60), ('CCCC(CO)C(C)C', 62), ('OCC12CCCC(C1)C2', 65), ('C1CC2C3C4C5C3(C1)C245', 68)]\n",
            "train base model\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000020 from disc.\n",
            "from 20 to 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000020 is: 0.0687 with mean loss 0.0773 with start loss 0.0000 with runtime 21.8711\n",
            "save\n",
            "try to read  aliamol_paper_comfy-sweep-1_True_model_epoch_00000021_004000_wFalse_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_comfy-sweep-1_True_model_epoch_00000021_004000_wFalse_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000021 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137240], x=[43093, 11], batch=[43093], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 60], x=[21, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137728], x=[43232, 11], batch=[43232], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 135844], x=[42696, 11], batch=[42696], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138032], x=[43317, 11], batch=[43317], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:57<00:00, 17.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11])]\n",
            "try to write  aliamol_paper_comfy-sweep-1_True_model_epoch_00000021_004000_wFalse_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:15<00:00, 252.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "try to read  aliamol_paper_comfy-sweep-1_True_model_epoch_00000021_004000_wTrue_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_comfy-sweep-1_True_model_epoch_00000021_004000_wTrue_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000021 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137980], x=[43301, 11], batch=[43301], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137620], x=[43201, 11], batch=[43201], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 17.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137992], x=[43305, 11], batch=[43305], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 17.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 136812], x=[42971, 11], batch=[42971], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 17.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "try to write  aliamol_paper_comfy-sweep-1_True_model_epoch_00000021_004000_wTrue_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:15<00:00, 257.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frac correct graphs:  0.182 with wild inference 0.0145\n",
            "Number of rows must be a positive integer, not 0\n",
            "Number of rows must be a positive integer, not 0\n",
            "[('OCC(O)CO', 3), ('CC(O)(O)C1CC1(C)O', 4), ('CC1C2COC1(O)CO2', 6), ('CC12C3CC4C15CC5C432', 11), ('CCCC1CC(CC)C1', 20), ('CC(O)COC1(C)CO1', 28), ('CCC1(C)CC2(C)CC12', 34), ('OOC1C(O)C2C3OC132', 40), ('CCC(O)CCO', 45), ('OCC1CC2CCC2O1', 46), ('COCC(C)OC1CC1', 59), ('CCC(C)C(O)CO', 68), ('COC(C)(O)OC(C)O', 69), ('CC1CC23CC24CC134', 73), ('CCOC(C)C(O)O', 74), ('OCCC(CO)CCO', 85), ('OCCC1CC1(O)CO', 89), ('OCC1CC23C4C2(O)C143', 98), ('CC1(CCC2CC2)CO1', 106), ('CC12COC(C)(O)C1C2', 112)]\n",
            "[('CCC1(O)CCCC1', 417), ('CC(C)C(C)C(O)O', 425), ('CC1CC(C)C(C)C1', 699), ('CCCC(C)(O)CO', 751), ('CCC(O)CC1CC1', 769), ('CC1C(CO)C1CCO', 785), ('OCC1CC(O)C1', 848), ('CCCCC(C)CC', 945), ('CCCC1CC2CC12', 1252), ('CC(CO)C(C)OCO', 1282), ('OCCCC(O)CCO', 1283), ('CC(O)C1(C)CC1O', 1302), ('CC(CO)C(O)CCO', 1338), ('CCC(O)CC(C)CO', 1347), ('OCCCC(O)CO', 1431), ('OCC1(CO)CCCC1', 1452), ('CCCC1CC1(O)CC', 1461), ('CC1CC(O)C(O)C1', 1510), ('CCCC(C)C1CC1O', 1644), ('CCCC1CC1CC', 1728)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000021 is: 0.0687 with mean loss 0.0769 with start loss 0.0000 with runtime 22.4195\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 19.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000022 is: 0.0687 with mean loss 0.0766 with start loss 0.0000 with runtime 22.0218\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000023 is: 0.0684 with mean loss 0.0762 with start loss 0.0000 with runtime 22.2981\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000024 is: 0.0684 with mean loss 0.0759 with start loss 0.0000 with runtime 22.4937\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000025 is: 0.0684 with mean loss 0.0756 with start loss 0.0000 with runtime 22.3576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000026 is: 0.0681 with mean loss 0.0754 with start loss 0.0000 with runtime 22.4189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:23<00:00, 17.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000027 is: 0.0680 with mean loss 0.0751 with start loss 0.0000 with runtime 23.4786\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:23<00:00, 18.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000028 is: 0.0679 with mean loss 0.0749 with start loss 0.0000 with runtime 23.0754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000029 is: 0.0678 with mean loss 0.0746 with start loss 0.0000 with runtime 22.3355\n",
            "save\n",
            "try to read  aliamol_paper_comfy-sweep-1_True_model_epoch_00000030_004000_wFalse_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_comfy-sweep-1_True_model_epoch_00000030_004000_wFalse_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000030 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137768], x=[43242, 11], batch=[43242], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:57<00:00, 17.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138140], x=[43348, 11], batch=[43348], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137920], x=[43284, 11], batch=[43284], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:57<00:00, 17.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 136952], x=[43013, 11], batch=[43013], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:57<00:00, 17.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "try to write  aliamol_paper_comfy-sweep-1_True_model_epoch_00000030_004000_wFalse_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:16<00:00, 246.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "try to read  aliamol_paper_comfy-sweep-1_True_model_epoch_00000030_004000_wTrue_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_comfy-sweep-1_True_model_epoch_00000030_004000_wTrue_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000030 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138236], x=[43374, 11], batch=[43374], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137272], x=[43100, 11], batch=[43100], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137644], x=[43207, 11], batch=[43207], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137168], x=[43071, 11], batch=[43071], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "try to write  aliamol_paper_comfy-sweep-1_True_model_epoch_00000030_004000_wTrue_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:16<00:00, 249.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frac correct graphs:  0.2805 with wild inference 0.35875\n",
            "Number of rows must be a positive integer, not 0\n",
            "Number of rows must be a positive integer, not 0\n",
            "[('CCC1C(C)C1(C)CO', 0), ('CC12CCOC1(CO)C2', 1), ('CCC(O)C1CCC1C', 2), ('CC1CCC1(C)C', 4), ('CC1CC12CC2CO', 6), ('CCCC1CC(CC)C1', 8), ('CC12CC34CC3CC14C2', 23), ('CC1C2CC3CC3OC12', 28), ('COCC(O)(O)C(C)O', 36), ('CC1CC2CC2(CO)O1', 41), ('CCC(C)C(C)O', 42), ('CC(O)C1CC1C(O)O', 45), ('OC1OCC12CC1CC12', 48), ('COCC1COC1(C)O', 49), ('CCC(OC)C1(C)CC1', 53), ('CC(CCCO)C1CO1', 54), ('CC1CC1CC(C)(O)O', 55), ('CCCC(C)OCCO', 57), ('CCCC1CCCC1', 58), ('C1C2C1C13CC14C(O3)C24', 59)]\n",
            "[('CC1OC2CC1CCO2', 3), ('CCCC1CCC1C', 7), ('CC1CC1C1CC12CC2', 11), ('CC(CO)C1OCC1O', 12), ('CC1C2CC3CC1C3C2', 15), ('CCC1CC2OC(C)C12', 18), ('OC12CCC3C4C1C342', 19), ('CC(O)C(O)C(C)CO', 26), ('CC1CCC2C(C)C12', 28), ('CCC1CC2C3CC3C12', 29), ('COC(CO)C1CCC1', 40), ('CC1CC23C(O)CC2C13', 41), ('O1C2C3C1C1C4OC41C23', 45), ('C1CCC2CC2CC1', 46), ('CC1C2COCC3C1C23', 47), ('CC1CC23CC2(C1)C3C', 49), ('CCC1CC(C)C1CO', 50), ('COC1CCC1C1CC1', 54), ('CCC1CCCC1C', 55), ('C1CC2CC2C2CC1C2', 56)]\n",
            "train base model\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000030 from disc.\n",
            "from 30 to 40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:23<00:00, 18.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000030 is: 0.0676 with mean loss 0.0744 with start loss 0.0000 with runtime 23.0541\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000031 is: 0.0675 with mean loss 0.0742 with start loss 0.0000 with runtime 22.8656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 19.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000032 is: 0.0675 with mean loss 0.0740 with start loss 0.0000 with runtime 22.0255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000033 is: 0.0673 with mean loss 0.0738 with start loss 0.0000 with runtime 21.8894\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000034 is: 0.0672 with mean loss 0.0736 with start loss 0.0000 with runtime 22.0681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000035 is: 0.0670 with mean loss 0.0734 with start loss 0.0000 with runtime 22.0762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000036 is: 0.0671 with mean loss 0.0733 with start loss 0.0000 with runtime 21.9751\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000037 is: 0.0668 with mean loss 0.0731 with start loss 0.0000 with runtime 21.9147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000038 is: 0.0668 with mean loss 0.0729 with start loss 0.0000 with runtime 21.7635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000039 is: 0.0667 with mean loss 0.0728 with start loss 0.0000 with runtime 22.1790\n",
            "save\n",
            "try to read  aliamol_paper_comfy-sweep-1_True_model_epoch_00000040_004000_wFalse_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_comfy-sweep-1_True_model_epoch_00000040_004000_wFalse_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000040 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137448], x=[43152, 11], batch=[43152], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138272], x=[43384, 11], batch=[43384], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:57<00:00, 17.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137364], x=[43126, 11], batch=[43126], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:57<00:00, 17.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138016], x=[43310, 11], batch=[43310], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:58<00:00, 17.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11])]\n",
            "try to write  aliamol_paper_comfy-sweep-1_True_model_epoch_00000040_004000_wFalse_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:16<00:00, 245.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "try to read  aliamol_paper_comfy-sweep-1_True_model_epoch_00000040_004000_wTrue_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_comfy-sweep-1_True_model_epoch_00000040_004000_wTrue_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000040 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138208], x=[43367, 11], batch=[43367], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137396], x=[43137, 11], batch=[43137], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 17.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138484], x=[43446, 11], batch=[43446], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137260], x=[43098, 11], batch=[43098], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "try to write  aliamol_paper_comfy-sweep-1_True_model_epoch_00000040_004000_wTrue_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:16<00:00, 247.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frac correct graphs:  0.11725 with wild inference 0.018\n",
            "Number of rows must be a positive integer, not 0\n",
            "Number of rows must be a positive integer, not 0\n",
            "[('C1CC2CCC(C1)C2', 2), ('CC1(CO)CCC1O', 9), ('OCC1CC1C1OCO1', 34), ('CCOC1(CC)C=C1O', 38), ('CCC(O)CCC(O)O', 53), ('CC1C(O)N1C1CC1', 66), ('CCC(C=O)C(O)C=O', 74), ('OCC1C2NC(O)(O)C12', 87), ('CCC1C2C(C(=O)O)C12', 88), ('CCC1C(O)CC1CO', 92), ('CCOC(O)(C=O)OC', 93), ('O=C1COCCC1CO', 99), ('CC12COC13C1OC3C12', 104), ('CC1CC12CCCC=N2', 109), ('OC12CC3C4C1C2C34O', 131), ('CCCCCCCCC', 167), ('OCCC(O)OO', 182), ('CCC12C(O)C3CC1C32', 191), ('O=CCC12CC1(O)C2O', 193), ('C1C2OC3CC45C2N4C135', 201)]\n",
            "[('OCCC(O)C(O)CO', 8), ('N1=NN1n1nnnn1', 41), ('CCCC(C)C1(O)CO1', 59), ('CC(O)c1nn2nnn12', 109), ('COCC(O)C(O)(O)O', 116), ('CCCC(C)CO', 199), ('CCC1CC2CC1C2', 259), ('COC1(O)COCC1=O', 262), ('CCC(CC)C(O)O', 296), ('CCC1(O)CC1OCO', 306), ('O=C1CCOC=NN=N1', 319), ('OCCCCCOCO', 400), ('OCCC1(O)CCCO1', 406), ('OCCC1CC1C1CO1', 490), ('n1nnn2nnn2n1', 657), ('CCC(C)C(C)C', 752), ('n1nnn2nnn2n1', 798), ('n1nnn2nnn2n1', 809), ('N1=NN=NN=NN=N1', 812), ('CC(C)C1CCCC1', 826)]\n",
            "train base model\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000040 from disc.\n",
            "from 40 to 40\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>frac_normal</td><td></td></tr><tr><td>frac_normal_unique</td><td></td></tr><tr><td>frac_wild</td><td></td></tr><tr><td>frac_wild_unique</td><td></td></tr><tr><td>loss</td><td></td></tr><tr><td>mean_loss</td><td></td></tr><tr><td>runtime</td><td></td></tr><tr><td>start_loss</td><td></td></tr><tr><td>step</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>frac_normal</td><td>0.11725</td></tr><tr><td>frac_normal_unique</td><td>0.11725</td></tr><tr><td>frac_wild</td><td>0.018</td></tr><tr><td>frac_wild_unique</td><td>0.018</td></tr><tr><td>loss</td><td>0.06674</td></tr><tr><td>mean_loss</td><td>0.07278</td></tr><tr><td>runtime</td><td>22.17898</td></tr><tr><td>start_loss</td><td>0.0</td></tr><tr><td>step</td><td>39</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">comfy-sweep-1</strong> at: <a href='https://wandb.ai/nextaid/AliaMoleculePaper/runs/1n572s8b' target=\"_blank\">https://wandb.ai/nextaid/AliaMoleculePaper/runs/1n572s8b</a><br/>Synced 5 W&B file(s), 0 media file(s), 69 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20231019_073809-1n572s8b/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0bxew7tz with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH_SIZE: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDISC_NOISE: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_DISC_MODEL: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_GEN: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tGAMMA: 0.2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/colab/AliaMoleculePaper/wandb/run-20231019_084009-0bxew7tz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nextaid/AliaMoleculePaper/runs/0bxew7tz' target=\"_blank\">bumbling-sweep-2</a></strong> to <a href='https://wandb.ai/nextaid/AliaMoleculePaper' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nextaid/AliaMoleculePaper/sweeps/7ru13mbd' target=\"_blank\">https://wandb.ai/nextaid/AliaMoleculePaper/sweeps/7ru13mbd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nextaid/AliaMoleculePaper' target=\"_blank\">https://wandb.ai/nextaid/AliaMoleculePaper</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/nextaid/AliaMoleculePaper/sweeps/7ru13mbd' target=\"_blank\">https://wandb.ai/nextaid/AliaMoleculePaper/sweeps/7ru13mbd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nextaid/AliaMoleculePaper/runs/0bxew7tz' target=\"_blank\">https://wandb.ai/nextaid/AliaMoleculePaper/runs/0bxew7tz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set  BATCH_SIZE = 256\n",
            "set  GAMMA = 0.2\n",
            "set  DISC_NOISE = 0.3\n",
            "set  EPOCHS_DISC_MODEL = 100\n",
            "set  EPOCHS_GEN = 100\n",
            "try to read  dataset.pickle\n",
            "train base model\n",
            "try to read  deg.pickle\n",
            "from 0 to 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000000 is: 0.1834 with mean loss 0.1834 with start loss 0.0000 with runtime 21.7935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000001 is: 0.0995 with mean loss 0.1275 with start loss 0.0000 with runtime 21.4348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000002 is: 0.0862 with mean loss 0.1138 with start loss 0.0000 with runtime 21.5214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:21<00:00, 19.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000003 is: 0.0855 with mean loss 0.1080 with start loss 0.0000 with runtime 21.8031\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000004 is: 0.0806 with mean loss 0.1026 with start loss 0.0000 with runtime 22.1123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000005 is: 0.0746 with mean loss 0.0978 with start loss 0.0000 with runtime 22.1319\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000006 is: 0.0726 with mean loss 0.0944 with start loss 0.0000 with runtime 22.6539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000007 is: 0.0717 with mean loss 0.0918 with start loss 0.0000 with runtime 22.7467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000008 is: 0.0711 with mean loss 0.0896 with start loss 0.0000 with runtime 22.6005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000009 is: 0.0709 with mean loss 0.0879 with start loss 0.0000 with runtime 22.6201\n",
            "save\n",
            "try to read  aliamol_paper_bumbling-sweep-2_True_model_epoch_00000010_004000_wFalse_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_bumbling-sweep-2_True_model_epoch_00000010_004000_wFalse_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000010 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137200], x=[43081, 11], batch=[43081], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 136844], x=[42981, 11], batch=[42981], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137056], x=[43041, 11], batch=[43041], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 136820], x=[42972, 11], batch=[42972], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "try to write  aliamol_paper_bumbling-sweep-2_True_model_epoch_00000010_004000_wFalse_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:15<00:00, 253.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "try to read  aliamol_paper_bumbling-sweep-2_True_model_epoch_00000010_004000_wTrue_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'aliamol_paper_bumbling-sweep-2_True_model_epoch_00000010_004000_wTrue_generated.pickle'\n",
            "try to read  dataset.pickle\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000010 from disc.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137572], x=[43187, 11], batch=[43187], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137796], x=[43248, 11], batch=[43248], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 17.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 112], x=[36, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 138156], x=[43351, 11], batch=[43351], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:56<00:00, 17.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 60], x=[21, 11]), Data(edge_index=[2, 112], x=[36, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137496], x=[43164, 11], batch=[43164], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [00:55<00:00, 17.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 60], x=[21, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "try to write  aliamol_paper_bumbling-sweep-2_True_model_epoch_00000010_004000_wTrue_generated.pickle\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4000/4000 [00:15<00:00, 253.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frac correct graphs:  0.32025 with wild inference 0.198\n",
            "Number of rows must be a positive integer, not 0\n",
            "Number of rows must be a positive integer, not 0\n",
            "[('CCC(C)CCC1CC1', 1), ('CC1CCC12CCCC2', 2), ('CCC1C23CC4(C)C2C143', 5), ('CCC(C)CCC(C)C', 6), ('CC12CCCC3C(C1)C32', 10), ('CC1(C)CCCC1(C)C', 12), ('CC1CCC2C(C)C12C', 13), ('CC1C2CC2CC1(C)C', 14), ('CCC1C2C(CC)C12C', 18), ('CCC1CC2C3C1C23', 19), ('CC12CC3CC3(C)C1C2', 22), ('CCCC(C)C(C)CC', 23), ('CC1C2(C)CCC12C', 30), ('CCC12C3CC4C35C1C425', 31), ('CC(C)C(C)(C)C1CC1', 33), ('CC12CC3C4C1C1C4C312', 34), ('CCCC1CC2CC1C2', 40), ('CCC12CCC1C2C', 43), ('CC1C2(C)C3C4C3(C)C142', 52), ('CC(C)CC12C3C1C32C', 54)]\n",
            "[('CCC1CC(CC)C1C', 3), ('CC(C)CCC1CC1C', 10), ('C1CCCCCCC1', 12), ('CCCC(C)(C)CC', 14), ('CCCCC(C)CCC', 19), ('CCCCC(C)CC', 24), ('CCC1(C)CCC(C)C1', 33), ('CC1CCCC(C)C1C', 34), ('CCCCCCCC', 39), ('CCCC1CC(C)C1C', 48), ('CCCCC1CC(C)C1', 65), ('CCC12C(C)CC1C2C', 69), ('CCCC(C)CC1CC1', 70), ('CCCCC1(C)CC1C', 72), ('CCCCC1(C)CC1C', 74), ('CCC1CCCC(C)C1', 78), ('CCC1CC1C(C)C', 84), ('CCC1CC(C)CC1C', 86), ('CCCCCCC(C)C', 91), ('CCC1CCCC2CC12', 92)]\n",
            "train base model\n",
            "try to read  deg.pickle\n",
            "read checkpoint of epoch 00000010 from disc.\n",
            "from 10 to 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000010 is: 0.0705 with mean loss 0.0864 with start loss 0.0000 with runtime 22.2800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000011 is: 0.0703 with mean loss 0.0852 with start loss 0.0000 with runtime 22.2196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000012 is: 0.0701 with mean loss 0.0841 with start loss 0.0000 with runtime 22.1116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000013 is: 0.0700 with mean loss 0.0831 with start loss 0.0000 with runtime 22.3275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000014 is: 0.0698 with mean loss 0.0823 with start loss 0.0000 with runtime 22.3293\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000015 is: 0.0697 with mean loss 0.0815 with start loss 0.0000 with runtime 22.5948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000016 is: 0.0695 with mean loss 0.0809 with start loss 0.0000 with runtime 22.2448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 419/419 [00:22<00:00, 18.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000017 is: 0.0694 with mean loss 0.0803 with start loss 0.0000 with runtime 22.7315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|       | 122/419 [00:06<00:16, 18.56it/s]"
          ]
        }
      ],
      "source": [
        "start_with_wandb()\n",
        "start_with_wandb(set_baseline_true=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UhC4DWeyaRX"
      },
      "outputs": [],
      "source": [
        "#!rm *.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjI30ZSDxcbM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP4nOXZ7ytk/faljf0i7F2k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}