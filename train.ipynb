{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerritgr/Alia/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovmM6MKh0RHG"
      },
      "source": [
        "# ðŸ’ŠðŸŒ€ MoleculeDiffusionGAN ðŸŒ€ðŸ’Š"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXB58ofd0yX-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCcXEAS_7brw"
      },
      "outputs": [],
      "source": [
        "# This is used for the naming of files and folders\n",
        "PROJECT_NAME = \"MoldDiffGAN_gcnweak3\"\n",
        "PATH_PATTERN_BASE = \"moldiffusion\"\n",
        "PATH_PATTERN = PATH_PATTERN_BASE\n",
        "\n",
        "# Setting BASELINE to True would deactivate the discriminator.\n",
        "BASELINE = False\n",
        "DEBUG = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yIsuEC808hb"
      },
      "source": [
        "### Handle Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX4PHxg-00D1"
      },
      "source": [
        "On Colab, we need to install some additional packages.\n",
        "If running on Colab, we use Google Drive to store results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6wrjwx9vQmH",
        "outputId": "a6d08a1f-0ae7-4ca7-8ab2-284db9fe10a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Current Working Directory:  /content\n",
            "New Working Directory:  /content/drive/MyDrive/colab/MoldDiffGAN_gcn\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Check for Google Colab and WandB\n",
        "USE_COLAB = False\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  USE_COLAB = True\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  import wandb # need to do this before chaning CWD\n",
        "except:\n",
        "  os.system(\"pip install wandb\")\n",
        "\n",
        "# Load Google Drive\n",
        "if USE_COLAB:\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "  dir_path = f'/content/drive/MyDrive/colab/{PROJECT_NAME}/'\n",
        "  if not os.path.exists(dir_path):\n",
        "    os.makedirs(dir_path)\n",
        "  print(\"Current Working Directory: \", os.getcwd())\n",
        "  if os.getcwd() != dir_path:\n",
        "    os.chdir(dir_path)\n",
        "    print(\"New Working Directory: \", os.getcwd())\n",
        "\n",
        "\n",
        "torch_version = torch.__version__.split(\"+\")\n",
        "try:\n",
        "  import torch_geometric\n",
        "except:\n",
        "  os.system(\"pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\")\n",
        "  os.system(\"pip install torch-geometric\")\n",
        "\n",
        "try:\n",
        "  import rdkit\n",
        "except:\n",
        "  os.system(\"pip install rdkit\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceqoOXap7kBb"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnxsm_Qm8QQ4",
        "outputId": "77192c01-84a5-4f57-be74-41d4f5985d18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 100  # Set this to 300 to get better image quality\n",
        "import seaborn as sns\n",
        "\n",
        "import networkx as nx\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "import traceback\n",
        "import time\n",
        "import copy\n",
        "import pickle\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import gzip\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Sequential as Seq\n",
        "from torch.nn import Linear as Lin\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import (\n",
        "    PNA,\n",
        "    GATv2Conv,\n",
        "    GraphNorm,\n",
        "    BatchNorm,\n",
        "    global_mean_pool,\n",
        "    global_add_pool\n",
        ")\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx, degree\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6k-s1v37muv"
      },
      "outputs": [],
      "source": [
        "# Load code to convert molecules to pyg tensors if using Colab\n",
        "if USE_COLAB and not os.path.exists(\"smiles_to_pyg\"):\n",
        "  os.system(\"git clone https://github.com/gerritgr/MoleculeDiffusionGAN.git && cp -R MoleculeDiffusionGAN/* .\")\n",
        "\n",
        "from smiles_to_pyg.molecule_load_and_convert import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2oEPNiw_utN"
      },
      "source": [
        "## Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpeBwAGC91Xu"
      },
      "outputs": [],
      "source": [
        "##\n",
        "## Diffusion\n",
        "##\n",
        "TIMESTEPS = 1000\n",
        "START = 0.0001\n",
        "END = 0.015\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 256\n",
        "GAMMA = 0.2\n",
        "\n",
        "##\n",
        "## Prediction/Denoising\n",
        "##\n",
        "LEARNING_RATE_GEN = 0.001\n",
        "EPOCHS_GEN = 60\n",
        "\n",
        "# PNA Pred\n",
        "DROPOUT_PRED = 0.05\n",
        "DEPTH_PRED = 6\n",
        "HIDDEN_CHANNELS_PRED = 32\n",
        "TOWERS_PRED = 2\n",
        "NORMALIZATION_PRED = True\n",
        "\n",
        "##\n",
        "## Discriminator\n",
        "##\n",
        "EPOCHS_DISC_MODEL = 50\n",
        "DISC_NOISE = 0.3\n",
        "\n",
        "# PNA Disc\n",
        "HIDDEN_CHANNELS_DISC = 4\n",
        "DEPTH_DISC = 3 # 4 in original\n",
        "DROPOUT_DISC = 0.05 # 0.03 in original\n",
        "NORMALIZATION_DISC = True\n",
        "\n",
        "##\n",
        "## Molecule Encoding\n",
        "##\n",
        "INDICATOR_FEATURE_DIM = 1\n",
        "FEATURE_DIM = 5  # (has to be the same for atom and bond)\n",
        "ATOM_FEATURE_DIM = FEATURE_DIM\n",
        "BOND_FEATURE_DIM = FEATURE_DIM\n",
        "NON_NODES = [True] + [False] * 5 + [True] * 5\n",
        "NON_EDGES = [True] + [True] * 5 + [False] * 5\n",
        "\n",
        "TIME_FEATURE_DIM = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9vsl3np_y0I"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vbsdmdl-JFm"
      },
      "outputs": [],
      "source": [
        "def log(d):\n",
        "  try:\n",
        "    import wandb\n",
        "    wandb.log(d)\n",
        "  except:\n",
        "    print(d)\n",
        "\n",
        "\n",
        "def load_file(filepath):\n",
        "  print(\"Trying to read\", filepath)\n",
        "  try:\n",
        "    with gzip.open(filepath, 'rb') as f:\n",
        "      return pickle.load(f)\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "def write_file(filepath, data):\n",
        "  try:\n",
        "    data = data.cpu()\n",
        "  except:\n",
        "    pass\n",
        "  print(\"Trying to write\", filepath)\n",
        "  with gzip.open(filepath, 'wb') as f:\n",
        "    pickle.dump(data, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaL_PWZH-eiM"
      },
      "outputs": [],
      "source": [
        "def build_dataset(seed=1234):\n",
        "  try:\n",
        "    dataset_train, dataset_test = load_file('dataset.pickle')\n",
        "    if DEBUG:\n",
        "      return dataset_train[:len(dataset_train) // 10], dataset_test[:len(dataset_test) // 10]\n",
        "    return dataset_train, dataset_test\n",
        "  except Exception as e:\n",
        "    print(f\"Could not load dataset due to error: {str(e)}, generate it now\")\n",
        "\n",
        "  dataset = read_qm9()\n",
        "  dataset_all = [g for g in dataset if g.x.shape[0] > 1]\n",
        "  dataset = list()\n",
        "\n",
        "  for g in tqdm(dataset_all):\n",
        "    try:\n",
        "      assert \"None\" not in str(pyg_to_smiles(g))\n",
        "      dataset.append(g)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  print(f\"Built and cleaned dataset, length is {len(dataset)}, old length was {len(dataset_all)}\")\n",
        "  random.Random(seed).shuffle(dataset)\n",
        "  split = int(len(dataset) * 0.8 + 0.5)\n",
        "  dataset_train = dataset[:split]\n",
        "  dataset_test = dataset[split:]\n",
        "  assert(dataset_train[0].x[0, :].numel() == INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM)\n",
        "\n",
        "  write_file(\"dataset.pickle\", (dataset_train, dataset_test))\n",
        "  return dataset_train, dataset_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQlj5MPz_4PB"
      },
      "outputs": [],
      "source": [
        "def generate_schedule(start = START, end = END, timesteps=TIMESTEPS):\n",
        "  \"\"\"\n",
        "  Generates a schedule of beta and alpha values for a forward process.\n",
        "\n",
        "  Args:\n",
        "  start (float): The starting value for the beta values. Default is START.\n",
        "  end (float): The ending value for the beta values. Default is END.\n",
        "  timesteps (int): The number of timesteps to generate. Default is TIMESTEPS.\n",
        "\n",
        "  Returns:\n",
        "  tuple: A tuple of three tensors containing the beta values, alpha values, and\n",
        "  cumulative alpha values (alpha bars).\n",
        "  \"\"\"\n",
        "  betas = torch.linspace(start, end, timesteps, device = DEVICE)\n",
        "  assert(betas.numel() == TIMESTEPS)\n",
        "  return betas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZJBlO6C_7-x"
      },
      "outputs": [],
      "source": [
        "def visualize_smiles_from_file(filepath):\n",
        "    print(\"Visualize molecules.\")\n",
        "    # Read SMILES from file\n",
        "    with open(filepath, 'r') as file:\n",
        "        smiles_list = [line.split(\"'\")[1] for line in file.readlines() if \"'\" in line]\n",
        "\n",
        "    # Convert SMILES to RDKit Mol objects, filtering out invalid ones\n",
        "    mols = [Chem.MolFromSmiles(smile) for smile in smiles_list[:100]]\n",
        "    mols = [mol for mol in mols if mol is not None]\n",
        "\n",
        "    if len(mols) == 0:\n",
        "        return\n",
        "\n",
        "    # Determine grid size\n",
        "    num_mols = len(mols)\n",
        "    cols = 10\n",
        "    rows = min(10, -(-num_mols // cols))  # ceil division\n",
        "\n",
        "    # Create a subplot grid\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(20, 20),\n",
        "                            gridspec_kw={'wspace': 0.3, 'hspace': 0.3})\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            ax = axs[i, j]\n",
        "            ax.axis(\"off\")  # hide axis\n",
        "            idx = i * cols + j  # index in mols list\n",
        "            if idx < num_mols:\n",
        "                img = Draw.MolToImage(mols[idx], size=(200, 200))\n",
        "                ax.imshow(img)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    # Save the figure\n",
        "    plt.savefig(filepath + '.jpg', format='jpg', bbox_inches='tight')\n",
        "\n",
        "    time.sleep(0.01)\n",
        "    try:\n",
        "        wandb.log_artifact(filepath + '.jpg', name=f\"jpg_{SWEEP_ID}_{filepath.replace('.','')}\", type=\"smiles_grid_graph\")\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5qDfWWc_FJX"
      },
      "outputs": [],
      "source": [
        "def get_pred_from_noise(noise_pred, x_with_noise, future_t):\n",
        "  row_num = x_with_noise.shape[0]\n",
        "  betas = generate_schedule()\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t).view(row_num, 1)\n",
        "\n",
        "  scaled_noise = torch.sqrt(1.0 - alphabar_t)\n",
        "  x_without_noise = x_with_noise - scaled_noise * noise_pred\n",
        "  x_without_noise = x_without_noise / torch.sqrt(alphabar_t)\n",
        "  return x_without_noise\n",
        "\n",
        "\n",
        "def get_noise_from_pred(original_pred, x_with_noise, future_t):\n",
        "  row_num = x_with_noise.shape[0]\n",
        "  betas = generate_schedule()\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t).view(row_num, 1)\n",
        "\n",
        "  scaled_noise = torch.sqrt(alphabar_t)\n",
        "  noise = x_with_noise - scaled_noise * original_pred\n",
        "  noise = noise / torch.sqrt(1.0 - alphabar_t)\n",
        "\n",
        "  return noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvP1-Zdg_mQC"
      },
      "outputs": [],
      "source": [
        "def log_smiles(smiles, filename):\n",
        "  try:\n",
        "    with open(filename, \"w\") as file:\n",
        "      for string in smiles:\n",
        "        file.write(str(string) + \"\\n\")\n",
        "\n",
        "    try:\n",
        "      wandb.log_artifact(filename, name=f\"src_txt_{SWEEP_ID}_{filename}\", type=\"smiles\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "    time.sleep(0.01)\n",
        "    visualize_smiles_from_file(filename)\n",
        "  except Exception as e:\n",
        "    print(\"An error occurred during training: \\n\", str(e))\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04wgZXHaDUwG"
      },
      "source": [
        "## Forward Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfqAxJ2bDWxr"
      },
      "outputs": [],
      "source": [
        "def forward_diffusion(node_features, future_t):\n",
        "  \"\"\"\n",
        "  Performs a forward diffusion process on an node_features tensor.\n",
        "  Each row can theoreetically have its own future time point.\n",
        "  Implements the second equation from https://youtu.be/a4Yfz2FxXiY?t=649\n",
        "  \"\"\"\n",
        "  row_num = node_features.shape[0]\n",
        "\n",
        "  if \"class 'int'\" in str(type(future_t)) or \"class 'float'\" in str(type(future_t)):\n",
        "    future_t = torch.tensor([int(future_t)] * row_num).to(DEVICE)\n",
        "\n",
        "  feature_dim = node_features.shape[1]\n",
        "  future_t = future_t.view(-1)\n",
        "  assert(row_num == future_t.numel())\n",
        "  assert(future_t[0] == future_t[1]) # Let's assume they belong to the same graph.\n",
        "\n",
        "  betas = generate_schedule()\n",
        "\n",
        "  noise = torch.randn_like(node_features, device=DEVICE)\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t).view(row_num, 1)\n",
        "  assert(alphabar_t.numel() == row_num)\n",
        "\n",
        "  new_node_features_mean = torch.sqrt(alphabar_t) * node_features # Column-wise multiplication, now it is a matrix\n",
        "  assert(new_node_features_mean.shape == node_features.shape)\n",
        "  new_node_features_std = torch.sqrt(1.-alphabar_t) # This is a col. vector\n",
        "  new_node_features_std = new_node_features_std.repeat(1,feature_dim) # This is a matrix\n",
        "  assert(new_node_features_mean.shape == new_node_features_std.shape)\n",
        "  noisey_node_features =  new_node_features_mean + new_node_features_std * noise\n",
        "\n",
        "  return noisey_node_features, noise\n",
        "\n",
        "#forward_diffusion(torch.tensor([1,2,3.], device=DEVICE).view(3,1), torch.tensor([0,0,999], device=DEVICE)), print(\"\"), forward_diffusion(torch.tensor([1,2,3.], device=DEVICE).view(3,1), torch.tensor([999,999,999], device=DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2BUMzmPA6-W"
      },
      "source": [
        "## Denoising NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I78sdWNKNJL1"
      },
      "outputs": [],
      "source": [
        "def dataset_to_degree_bin(train_dataset):\n",
        "  \"\"\"\n",
        "  Convert a dataset to a histogram of node degrees (in-degrees).\n",
        "  Load from file if available; otherwise, compute from the dataset.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Attempt to load the degree histogram from a file.\n",
        "    deg = load_file('deg.pickle')\n",
        "    deg = deg.to(DEVICE)\n",
        "    return deg\n",
        "  except Exception as e:\n",
        "    print(f\"Could not find degree bin due to error: {str(e)}, generate it now\")\n",
        "\n",
        "  # Assert that the dataset is provided.\n",
        "  assert(train_dataset is not None)\n",
        "\n",
        "  # Compute the maximum in-degree in the training data.\n",
        "  max_degree = -1\n",
        "  for data in train_dataset:\n",
        "    data = data.to(DEVICE)\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    max_degree = max(max_degree, int(d.max()))\n",
        "\n",
        "  # Create an empty histogram for degrees.\n",
        "  deg = torch.zeros(max_degree + 1, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "  # Populate the histogram with data from the dataset.\n",
        "  for data in train_dataset:\n",
        "    data = data.to(DEVICE)\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    deg += torch.bincount(d, minlength=deg.numel())\n",
        "\n",
        "  # Save the computed histogram to a file.\n",
        "  write_file(\"deg.pickle\", deg.cpu())\n",
        "\n",
        "  return deg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "270vRkHbNYB6"
      },
      "outputs": [],
      "source": [
        "class PNAnet(torch.nn.Module):\n",
        "  def __init__(self, train_dataset=None, hidden_channels=HIDDEN_CHANNELS_PRED, depth=DEPTH_PRED, dropout=DROPOUT_PRED, towers=TOWERS_PRED, normalization=NORMALIZATION_PRED, pre_post_layers=1):\n",
        "    super(PNAnet, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Adjust hidden channels for the given towers.\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1) # must match\n",
        "\n",
        "    # Calculate input and output channels.\n",
        "    in_channels = INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM + TIME_FEATURE_DIM\n",
        "    out_channels = FEATURE_DIM\n",
        "\n",
        "    # Get degree histogram for the dataset\n",
        "    deg = dataset_to_degree_bin(train_dataset)\n",
        "\n",
        "    # Set aggregators and scalers for the PNA layer.\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "\n",
        "    # Create a normalization layer if required.\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "\n",
        "    # Define the PNA layer.\n",
        "    self.pnanet = PNA(\n",
        "        in_channels=in_channels,\n",
        "        hidden_channels=hidden_channels,\n",
        "        out_channels=hidden_channels,\n",
        "        num_layers=depth,\n",
        "        aggregators=aggregators,\n",
        "        scalers=scalers,\n",
        "        deg=deg,\n",
        "        dropout=dropout,\n",
        "        towers=towers,\n",
        "        norm=self.normalization,\n",
        "        pre_layers=pre_post_layers,\n",
        "        post_layers=pre_post_layers\n",
        "    )\n",
        "\n",
        "    # Define the final MLP layer.\n",
        "    self.final_mlp = Seq(\n",
        "        Lin(hidden_channels, hidden_channels),\n",
        "        nn.ReLU(),\n",
        "        Lin(hidden_channels, hidden_channels),\n",
        "        nn.ReLU(),\n",
        "        Lin(hidden_channels, hidden_channels),\n",
        "        nn.ReLU(),\n",
        "        Lin(hidden_channels, out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x_in, t, edge_index):\n",
        "    \"\"\"\n",
        "    Perform a forward pass through the PNAnet.\n",
        "    \"\"\"\n",
        "    row_num = x_in.shape[0]\n",
        "    t = t.view(-1, TIME_FEATURE_DIM)\n",
        "    x = torch.concat((x_in, t), dim=1)\n",
        "\n",
        "    x = self.pnanet(x, edge_index)\n",
        "    x = self.final_mlp(x)\n",
        "\n",
        "    # Assertions for sanity checks\n",
        "    assert(x.numel() > 1)\n",
        "    assert(x.shape[0] == row_num)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2EDh-9vXPrr"
      },
      "outputs": [],
      "source": [
        "def load_latest_checkpoint(model, optimizer, loss_list, epoch_i, path_pattern_checkpoint=None):\n",
        "  \"\"\"\n",
        "  Load the latest checkpoint from the disk.\n",
        "  \"\"\"\n",
        "  if path_pattern_checkpoint is None:\n",
        "    path_pattern_checkpoint = PATH_PATTERN + \"_model_epoch_*.pth\"\n",
        "\n",
        "  try:\n",
        "    checkpoint_paths = sorted(glob.glob(path_pattern_checkpoint))\n",
        "    if len(checkpoint_paths) == 0:\n",
        "      return model, optimizer, loss_list, epoch_i\n",
        "\n",
        "    latest_checkpoint_path = checkpoint_paths[-1]\n",
        "    checkpoint = torch.load(latest_checkpoint_path, map_location=DEVICE)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch_i = checkpoint['epoch']\n",
        "    loss_list = checkpoint['loss_list']\n",
        "\n",
        "    print(f\"Loaded checkpoint of epoch {epoch_i:08} from disk.\")\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to load checkpoint. Error: {str(e)}\")\n",
        "\n",
        "  return model, optimizer, loss_list, epoch_i\n",
        "\n",
        "def save_model(model, optimizer, loss_list, epoch_i, upload=False):\n",
        "  \"\"\"\n",
        "  Save the model state to the disk.\n",
        "  \"\"\"\n",
        "  if epoch_i == 0: # Relevant for load_base_model()\n",
        "    return\n",
        "\n",
        "  save_path = f\"{PATH_PATTERN}_model_epoch_{epoch_i:08}.pth\" # Will do lexicographical ordering to load.\n",
        "\n",
        "  # Save the model and optimizer state dicts in a dictionary.\n",
        "  torch.save({\n",
        "    'epoch': epoch_i,\n",
        "    'loss_list': loss_list,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "  }, save_path)\n",
        "\n",
        "  if upload:\n",
        "    try:\n",
        "      wandb.log_artifact(save_path, name=f\"weights_{SWEEP_ID}_{epoch_i:08}_weightfile\", type=\"weight\")\n",
        "    except Exception as e:\n",
        "      print(f\"Failed to upload model. Error: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGUISrgECQ-e"
      },
      "outputs": [],
      "source": [
        "def load_base_model(dataset_train, path_pattern_checkpoint=None):\n",
        "  model_base = PNAnet(dataset_train)\n",
        "  model_base = model_base.to(DEVICE)\n",
        "  loss_list = None\n",
        "  optimizer = Adam(model_base.parameters(), lr = LEARNING_RATE_GEN)\n",
        "  model_base, optimizer, loss_list, epoch_start = load_latest_checkpoint(model_base, optimizer, loss_list, epoch_i=0, path_pattern_checkpoint=path_pattern_checkpoint)\n",
        "\n",
        "  return model_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc7iJLKJCv3s"
      },
      "source": [
        "## Inference / Reverse Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WRAea25gUJ1"
      },
      "source": [
        "There is a _normal_ and a _restart_ method for inference. The restart version is not implemented in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCpdISE2yxMC"
      },
      "outputs": [],
      "source": [
        "def denoise_one_step(model, g, i):\n",
        "  \"\"\"\n",
        "  Performs one step of denoising using the provided model.\n",
        "  \"\"\"\n",
        "  row_num = g.x.shape[0]\n",
        "\n",
        "  # Generate and calculate betas, alphas, and related parameters\n",
        "  betas = generate_schedule()\n",
        "  t = TIMESTEPS - i - 1  # i=0 indicates full noise\n",
        "  beta_t = betas[t]\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphas_cumprod_t = alphas_cumprod[t]\n",
        "  sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1. - alphas_cumprod_t)\n",
        "  sqrt_recip_alphas_t = torch.sqrt(1.0 / alphas[t])\n",
        "  alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "\n",
        "  # Create the mask\n",
        "  mask = torch.concat(\n",
        "      (torch.tensor([False] * g.x_old.shape[0], device=DEVICE).view(-1, 1),\n",
        "       g.x_old[:, 1:] > -0.5),\n",
        "      dim=1\n",
        "  )\n",
        "\n",
        "  # Define future_t for the model predictions\n",
        "  future_t = torch.tensor([float(t)] * g.x.shape[0], device=DEVICE).view(-1, 1)\n",
        "  original_pred = model(g.x, future_t, g.edge_index)\n",
        "\n",
        "  # Extract noisy values and predict noise\n",
        "  x_with_noise = g.x[mask].view(row_num, -1)\n",
        "  future_t = torch.tensor([int(t)] * g.x.shape[0], device=DEVICE).view(-1)\n",
        "  noise_pred = get_noise_from_pred(original_pred, x_with_noise, future_t)\n",
        "\n",
        "  # Set endpoints values\n",
        "  values_now = g.x[mask].view(row_num, -1)\n",
        "  values_endpoint = noise_pred.view(row_num, -1)\n",
        "  assert values_now.shape == values_endpoint.shape\n",
        "\n",
        "  # Compute denoised values\n",
        "  model_mean = sqrt_recip_alphas_t * (values_now - beta_t * values_endpoint / sqrt_one_minus_alphas_cumprod_t)\n",
        "  values_one_step_denoised = model_mean  # in case that t == 0\n",
        "\n",
        "  if t != 0:\n",
        "    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)  # in the paper this is in 3.2. Note that sigma^2 is variance, not std.\n",
        "    posterior_std_t = torch.sqrt(posterior_variance[t])\n",
        "    noise = torch.randn_like(values_now, device=DEVICE)\n",
        "    values_one_step_denoised = model_mean + posterior_std_t * noise\n",
        "\n",
        "  # Clone and update with denoised values\n",
        "  denoised_x = g.x.clone()\n",
        "  denoised_x[mask] = values_one_step_denoised.flatten()\n",
        "\n",
        "  return denoised_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5AegWA7Cy-c"
      },
      "outputs": [],
      "source": [
        "def overwrite_with_noise(g):\n",
        "  g.x_old = g.x.clone()\n",
        "  mask = torch.concat((torch.tensor([False]*g.x_old.shape[0], device=DEVICE).view(-1,1), g.x_old[:,1:]>-0.5), dim=1)\n",
        "  g.x[mask] = torch.randn_like(g.x[mask])\n",
        "  return g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8RKX4tVZGy2"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def generate_examples(model, dataset_train, num=100, restart_inference_method=False):\n",
        "  \"\"\"\n",
        "  Generate graph samples in batches using the provided model.\n",
        "  \"\"\"\n",
        "  # Setup\n",
        "  print(\"generate samples batched\")\n",
        "  model.eval()\n",
        "  dataset_train_start = list()\n",
        "\n",
        "  while len(dataset_train_start) < num:\n",
        "    g = dataset_train[random.choice(range(len(dataset_train)))]\n",
        "    dataset_train_start.append(g.clone().to(DEVICE))\n",
        "\n",
        "  #old\n",
        "  #while len(dataset_train_start) < num:\n",
        "  #  g = dataset_train[random.sample(range(len(dataset_train)),1)[0]]\n",
        "  #  dataset_train_start.append(g.clone().to(DEVICE))\n",
        "  #  g = dataset_train_start[-1]\n",
        "\n",
        "  assert(len(dataset_train_start) == num)\n",
        "  dataloader = DataLoader(dataset_train_start, batch_size=num)\n",
        "\n",
        "  # Inference\n",
        "  for g in dataloader:\n",
        "    g = g.to(DEVICE)\n",
        "    print(\"load g\", g, g.batch)\n",
        "    g = overwrite_with_noise(g)\n",
        "\n",
        "    for i in tqdm(range(TIMESTEPS)):\n",
        "      t = int(TIMESTEPS - i - 1)\n",
        "      if restart_inference_method:\n",
        "        x_with_less_noise = denoise_one_step_restart(model, g, i) # not implemented\n",
        "      else:\n",
        "        x_with_less_noise = denoise_one_step(model, g, i)\n",
        "      g.x = x_with_less_noise\n",
        "\n",
        "    graph_list = g.to_data_list()\n",
        "    graph_list = [g.cpu() for g in graph_list]\n",
        "\n",
        "    print(\"generated graphs \", graph_list[:10])\n",
        "    return graph_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3snibPC2C4-f"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def find_frac_correct(graphs):\n",
        "  \"\"\"\n",
        "  Determine the fraction and unique of correct graphs based on their conversion to SMILES.\n",
        "  \"\"\"\n",
        "  correct = 0\n",
        "  smiles_list = list()\n",
        "\n",
        "  for i, g in tqdm(enumerate(graphs)):\n",
        "    smiles = pyg_to_smiles(g)\n",
        "    if smiles and '.' not in smiles:\n",
        "      mol = Chem.MolFromSmiles(smiles)\n",
        "      if mol:\n",
        "        correct += 1\n",
        "        smiles_list.append((smiles, i))\n",
        "\n",
        "  frac_correct = correct / len(graphs)\n",
        "  smiles_list_0 = [s[0] for s in smiles_list]\n",
        "  unique_frac = len(set(smiles_list_0)) / len(graphs)\n",
        "\n",
        "  return frac_correct, smiles_list, unique_frac\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm9B-IOqC6j2"
      },
      "outputs": [],
      "source": [
        "def gen_graphs(num_per_generation=1000, num_generations=40, restart_inference_method=False, model_path=None):\n",
        "  \"\"\"\n",
        "  Generate a specified number of graphs.\n",
        "  \"\"\"\n",
        "  print(f\"Generate {num_generations*num_per_generation} graphs.\")\n",
        "  if DEBUG:\n",
        "    num_generations = int(num_generations / 10)\n",
        "\n",
        "  if model_path is None:\n",
        "    model_path = PATH_PATTERN + \"_model_epoch_*.pth\"\n",
        "\n",
        "  path = sorted(glob.glob(model_path))[-1]\n",
        "  num_samples = num_per_generation * num_generations\n",
        "  filepath = path.replace(\".pth\", f'_{num_samples:06d}_w{restart_inference_method}_generated.pickle')\n",
        "\n",
        "  results = list()\n",
        "  try:\n",
        "    results = load_file(filepath)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  if len(results) == num_samples:\n",
        "    return results\n",
        "\n",
        "  dataset_base, dataset_base_test = build_dataset()\n",
        "  model_base = load_base_model(dataset_base, path_pattern_checkpoint=path)\n",
        "\n",
        "  i = 0\n",
        "  while len(results) < num_samples:\n",
        "    i += 1\n",
        "    num = max(num_per_generation, len(results) - num_samples)\n",
        "    graphs = generate_examples(model_base, dataset_base, num=num, restart_inference_method=restart_inference_method)\n",
        "    results.extend(graphs)\n",
        "    if i % 5 == 0 or len(results) >= num_samples:\n",
        "      write_file(filepath, results)\n",
        "\n",
        "  assert(len(results) == num_samples)\n",
        "  return results\n",
        "\n",
        "\n",
        "def test_graph_generation(path_pattern=None, restart_inference_method=False):\n",
        "  generated_graphs = gen_graphs(restart_inference_method=restart_inference_method, model_path=path_pattern)\n",
        "  return find_frac_correct(generated_graphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBi59QK5C91I"
      },
      "source": [
        "## Discriminator NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWIOKsKWC_A5"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import PNA, GCN\n",
        "\n",
        "class PNAdisc(torch.nn.Module):\n",
        "  def __init__(self, train_dataset=None, hidden_channels=HIDDEN_CHANNELS_DISC,\n",
        "               depth=DEPTH_DISC, dropout=DROPOUT_DISC, towers=1,\n",
        "               normalization=NORMALIZATION_DISC, pre_post_layers=1):\n",
        "    super(PNAdisc, self).__init__()\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Adjust hidden channels based on towers\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1)\n",
        "\n",
        "    in_channels = INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM\n",
        "    assert in_channels == 11\n",
        "\n",
        "    deg = dataset_to_degree_bin(train_dataset).to(DEVICE)\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "    self.pnanet = PNA(in_channels=in_channels,\n",
        "                     hidden_channels=hidden_channels,\n",
        "                     out_channels=1,\n",
        "                     num_layers=depth,\n",
        "                     aggregators=aggregators,\n",
        "                     scalers=scalers,\n",
        "                     deg=deg,\n",
        "                     dropout=dropout,\n",
        "                     towers=towers,\n",
        "                     norm=self.normalization,\n",
        "                     pre_layers=pre_post_layers,\n",
        "                     post_layers=pre_post_layers)\n",
        "\n",
        "    self.gcnnet = GCN(in_channels=in_channels,\n",
        "                     hidden_channels=5,\n",
        "                     out_channels=1,\n",
        "                     num_layers=depth,\n",
        "                     dropout=dropout)\n",
        "\n",
        "  def forward(self, x, edge_index, batch=None):\n",
        "    x = x + torch.randn_like(x) * DISC_NOISE\n",
        "    x = self.gcnnet(x, edge_index) # or pna\n",
        "    x = global_mean_pool(x, batch)\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh-x4OlODFBX"
      },
      "outputs": [],
      "source": [
        "def train_epoch_disc(model_disc, dataloader, optimizer):\n",
        "  model_disc.train()\n",
        "  start_time = time.time()\n",
        "  loss_list = []\n",
        "  acc_list = []\n",
        "\n",
        "  for batch in dataloader:\n",
        "    batch = batch.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    pred = model_disc(batch.x, batch.edge_index, batch.batch)\n",
        "    loss = F.binary_cross_entropy(pred.flatten(), batch.y.flatten())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    acc = (torch.abs(pred.flatten() - batch.y.flatten()) < 0.5).float()\n",
        "    acc_list.extend(acc.detach().cpu().tolist())\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "  return np.mean(loss_list), np.mean(acc_list), time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCnHC3mPDGoM"
      },
      "outputs": [],
      "source": [
        "def test_disc(model_disc, dataloader):\n",
        "  model_disc.eval()\n",
        "  start_time = time.time()\n",
        "  loss_list = list()\n",
        "  acc_list = list()\n",
        "  for batch in dataloader:\n",
        "    batch = batch.to(DEVICE)\n",
        "    pred = model_disc(batch.x, batch.edge_index, batch.batch)\n",
        "    loss = F.binary_cross_entropy(pred.flatten(), batch.y.flatten())\n",
        "    acc = (torch.abs(pred.flatten()-batch.y.flatten()) < 0.5).float()\n",
        "    acc_list = acc_list + acc.detach().cpu().tolist()\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "  return np.mean(loss_list), np.mean(acc_list), time.time()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6vWTeUicmzH"
      },
      "outputs": [],
      "source": [
        "def train_disc_model(dataloader_disc, dataloader_disc_test, round_i):\n",
        "  model_disc = PNAdisc(dataloader_disc).to(DEVICE)\n",
        "  weight_path = f\"{PATH_PATTERN}_discriminator_model_round_{round_i:05}.pth\"\n",
        "\n",
        "  try:\n",
        "    checkpoint = torch.load(weight_path)\n",
        "    model_disc.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"found disc model in round {round_i:05}\")\n",
        "    return model_disc\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  epochs = []\n",
        "  losses_train = []\n",
        "  losses_test = []\n",
        "\n",
        "  optimizer_disc = Adam(model_disc.parameters(), lr=0.0001)\n",
        "  for epoch_i in range(EPOCHS_DISC_MODEL):\n",
        "    loss_train, acc_train, t_train = train_epoch_disc(model_disc, dataloader_disc, optimizer_disc)\n",
        "    #if epoch_i % 1 == 1 or epoch_i == EPOCHS_DISC_MODEL - 1:\n",
        "    loss_test, acc_test, t_test = test_disc(model_disc, dataloader_disc_test)\n",
        "    print(f\"train discriminator: epoch: {epoch_i:05}, loss: {loss_train:.4f}, loss test: {loss_test:.4f}, acc: {acc_train:.3f}, acc test: {acc_test:.3f}, time: {t_train:.3f}\")\n",
        "    log({\n",
        "        \"disc/step\": epoch_i + (1+round_i) * EPOCHS_DISC_MODEL,\n",
        "        \"disc/epoch\": epoch_i + (1+round_i) * EPOCHS_DISC_MODEL,\n",
        "        \"disc/loss_train\": loss_train,\n",
        "        'disc/loss_test': loss_test,\n",
        "        \"disc/acc_train\": acc_train,\n",
        "        \"disc/acc_test\": acc_test,\n",
        "        \"disc/time\": t_train\n",
        "    })\n",
        "    epochs.append(epoch_i)\n",
        "    losses_train.append(loss_train)\n",
        "    losses_test.append(loss_test)\n",
        "\n",
        "  # Plotting losses\n",
        "  plt.clf()\n",
        "  plt.plot(epochs, losses_train, label='train')\n",
        "  plt.plot(epochs, losses_test, label='test')\n",
        "  plt.legend()\n",
        "  plt.savefig(f\"discriminator_model_{round_i:05}.png\")\n",
        "\n",
        "  torch.save({\n",
        "      'model_state_dict': model_disc.state_dict(),\n",
        "      'epochs': epochs,\n",
        "      \"losses_train\": losses_train,\n",
        "      \"losses_test\": losses_test\n",
        "  }, weight_path)\n",
        "\n",
        "  return model_disc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF6Sq0OsDLHt"
      },
      "outputs": [],
      "source": [
        "def run_disc(round_i=1):\n",
        "  print(f\"Train discriminator round {round_i}.\")\n",
        "  fake_graphs = gen_graphs(restart_inference_method=False)\n",
        "  dataset_base, dataset_base_test = build_dataset()\n",
        "  real_graphs = random.sample(dataset_base, len(fake_graphs))\n",
        "  dataset = list()\n",
        "\n",
        "  for g in fake_graphs:\n",
        "    g_i = g.clone()\n",
        "    g_i.y = torch.tensor(0.1) # use 0.1 and 0.9 for better stability\n",
        "    dataset.append(g_i)\n",
        "\n",
        "  for g in real_graphs:\n",
        "    g_i = g.clone()\n",
        "    g_i.y = torch.tensor(0.9)\n",
        "    dataset.append(g_i)\n",
        "\n",
        "  random.shuffle(dataset)\n",
        "  cut_off = int(len(dataset) * 0.8)\n",
        "  dataloader_train = DataLoader(dataset[:cut_off], batch_size = BATCH_SIZE, shuffle=True)\n",
        "  dataloader_test = DataLoader(dataset[cut_off:], batch_size = BATCH_SIZE, shuffle=True)\n",
        "\n",
        "  model_disc = train_disc_model(dataloader_train, dataloader_test, round_i)\n",
        "  return model_disc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAVWEgfDDkcM"
      },
      "source": [
        "## Train Jointly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSqf7G3GfIi8"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, model_disc=None):\n",
        "  schedule = generate_schedule()\n",
        "  model.train()\n",
        "  start_time = time.time()\n",
        "  loss_list = []\n",
        "  loss_list_disc = []\n",
        "\n",
        "  for batch in tqdm(dataloader):\n",
        "    if batch.x.shape[0] < 2:\n",
        "      continue\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = batch.to(DEVICE)\n",
        "    row_num = batch.x.shape[0]\n",
        "\n",
        "    num_graphs_in_batch = int(torch.max(batch.batch).item() + 1)\n",
        "    future_t_select = torch.randint(0, TIMESTEPS, (num_graphs_in_batch,), device=DEVICE)\n",
        "    future_t = torch.gather(future_t_select, 0, batch.batch)\n",
        "    assert future_t.numel() == row_num\n",
        "\n",
        "    mask = torch.cat((torch.tensor([False] * row_num, device=DEVICE).view(-1, 1), batch.x[:, 1:] > -0.5), dim=1)\n",
        "    x_start_gt = batch.x[mask].view(row_num, FEATURE_DIM)\n",
        "    x_with_noise, noise_gt = forward_diffusion(x_start_gt, future_t)\n",
        "\n",
        "    x_in = batch.x.clone()\n",
        "    x_in[mask] = x_with_noise.flatten()\n",
        "    x_start_pred = model(x_in, future_t, batch.edge_index)\n",
        "    loss = F.mse_loss(x_start_gt, x_start_pred)\n",
        "\n",
        "    disc_loss = torch.tensor(0.0, device=DEVICE)\n",
        "    if model_disc is not None:\n",
        "      x_in[mask] = x_start_pred.flatten()\n",
        "      disc_loss = torch.mean((1.0 - model_disc(x_in, batch.edge_index, batch=batch.batch))**2)\n",
        "      loss = (1.0 - GAMMA) * loss + GAMMA * disc_loss\n",
        "\n",
        "    loss.backward()\n",
        "    loss_list.append(loss.item())\n",
        "    loss_list_disc.append(disc_loss.item())\n",
        "    optimizer.step()\n",
        "\n",
        "  return np.mean(loss_list), np.mean(loss_list_disc), time.time() - start_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hRd_XXGgkRS"
      },
      "outputs": [],
      "source": [
        "def train_base_model(train_loader, epoch_num=EPOCHS_GEN, model_disc=None):\n",
        "  print(\"Train denoising model.\")\n",
        "  if DEBUG:\n",
        "    epoch_num = int(epoch_num / 10)\n",
        "\n",
        "  dataset_train = train_loader.dataset\n",
        "  model_base = PNAnet(dataset_train).to(DEVICE)\n",
        "\n",
        "  optimizer = Adam(model_base.parameters(), lr=LEARNING_RATE_GEN * 0.01) # the mutliplication makes no real sense\n",
        "  loss_list = []\n",
        "  model_base, optimizer, loss_list, epoch_start = load_latest_checkpoint(model_base, optimizer, loss_list, epoch_i=0)\n",
        "  epoch_start = min(epoch_start, epoch_num)\n",
        "  print(f\"from {epoch_start} to {epoch_num}\")\n",
        "\n",
        "  for epoch_i in range(epoch_start, epoch_num):\n",
        "    try:\n",
        "      loss, loss_disc, time_elapsed = train_epoch(model_base, train_loader, optimizer, model_disc=model_disc)\n",
        "      loss_list.append((epoch_i, loss))\n",
        "      mean_loss = np.mean([y for _, y in loss_list] + [loss])\n",
        "      print(f\"loss in epoch {epoch_i:07} is: {loss:05.4f} with mean loss {mean_loss:05.4f} with disc loss {loss_disc:05.4f} with runtime {time_elapsed:05.4f}\")\n",
        "      log({\n",
        "        \"gen/step\": epoch_i,\n",
        "        \"gen/epoch\": epoch_i,\n",
        "        \"gen/loss\": loss,\n",
        "        \"gen/mean_loss\": mean_loss,\n",
        "        \"gen/start_loss\": loss_disc,\n",
        "        \"gen/runtime\": time_elapsed\n",
        "      })\n",
        "\n",
        "      if (epoch_i % 20 == 0 and epoch_i > epoch_start) or epoch_i == epoch_num - 1 or BATCH_SIZE == 1:\n",
        "        print(\"save\")\n",
        "        save_model(model_base, optimizer, loss_list, epoch_i + 1, upload = epoch_i == epoch_num - 1)\n",
        "        time.sleep(0.01)\n",
        "        frac, smiles_list, unique_frac = test_graph_generation(restart_inference_method=False)\n",
        "        frac_restart, smiles_list_restart, unique_frac_restart = 0, list(), 0 #test_graph_generation(restart_inference_method=True)\n",
        "        print(f\"Fraction of correct graphs: {frac}, with restart_inference_method inference {frac_restart}\")\n",
        "        log({\n",
        "          \"inference/step\": epoch_i,\n",
        "          \"inference/epoch\": epoch_i,\n",
        "          \"inference/frac_normal\": frac,\n",
        "          \"inference/frac_restart\": frac_restart,\n",
        "          \"inference/frac_normal_unique\": unique_frac,\n",
        "          \"inference/frac_restart_unique\": unique_frac_restart\n",
        "        })\n",
        "        log_smiles(smiles_list, f\"{PATH_PATTERN}_smiles_{epoch_i}_normal.txt\")\n",
        "        log_smiles(smiles_list_restart, f\"{PATH_PATTERN}_smiles_{epoch_i}_restart.txt\")\n",
        "        try:\n",
        "          print(smiles_list[:20])\n",
        "          print(smiles_list_restart[:20])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "    except Exception as e:\n",
        "      print(f\"An error occurred during training: \\n{str(e)}\")\n",
        "      traceback.print_exc()\n",
        "      raise e\n",
        "\n",
        "  return model_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qbC0Wq8D5Ux"
      },
      "source": [
        "### Putting Everything Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6zFANMID6j-"
      },
      "outputs": [],
      "source": [
        "def start_experiments(rounds=6): #originally 5\n",
        "  global DISC_NOISE\n",
        "  if DEBUG:\n",
        "    rounds = rounds // 2\n",
        "  dataset_base, dataset_base_test = build_dataset()\n",
        "  dataloader_base = DataLoader(dataset_base, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  model_base = train_base_model(dataloader_base, epoch_num = EPOCHS_GEN*1)\n",
        "\n",
        "  for round_i in range(1, rounds):\n",
        "    if BASELINE:\n",
        "      model_disc = None\n",
        "    else:\n",
        "      model_disc = run_disc(round_i=round_i)\n",
        "    model_base = train_base_model(dataloader_base, epoch_num = EPOCHS_GEN*(round_i+1), model_disc=model_disc)\n",
        "    #DISC_NOISE = DISC_NOISE*0.5\n",
        "\n",
        "  save_src_file()\n",
        "  return  model_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_OiyFytEAKd"
      },
      "source": [
        "### Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2haryq1EA_m"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import wandb\n",
        "except:\n",
        "  # Train with discriminator (our method)\n",
        "  start_experiments(rounds=5)\n",
        "  # Train without discriminator (baseline)\n",
        "  BASELINE = True\n",
        "  start_experiments(rounds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "086_7Od-EVX1"
      },
      "source": [
        "## Training with WandB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7ZWyjSb0d_8"
      },
      "source": [
        "We can use WandB to save the training results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5SWqlSfEWSF",
        "outputId": "b620cea9-f104-46bb-d54b-4661dcb72a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/usr/local/lib/python3.10/dist-packages/wandb']\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "print(wandb.__path__) # this should look like ['/usr/local/lib/python3.10/dist-packages/wandb']. Make sure to not install wandb into your current working dir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFyzrxmZFAMJ"
      },
      "outputs": [],
      "source": [
        "WANDB_TOKEN = \"\" # Add you WandB token here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3MHvxtkEYW2"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"AliaMol\",\n",
        "    \"method\": \"random\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"inference/frac_normal_unique\",\n",
        "        \"goal\": \"maximize\",\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"BATCH_SIZE\": {\"values\": [64]}, #256\n",
        "        \"GAMMA\": {\"values\": [0.2]}, #0.1\n",
        "        \"DISC_NOISE\": {\"values\": [1.0]},  # 0.3 in generation for paper\n",
        "        \"EPOCHS_DISC_MODEL\": {\"values\": [5]},\n",
        "        \"EPOCHS_GEN\": {\"values\": [100]},\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivM55NJ7EeuR"
      },
      "outputs": [],
      "source": [
        "def save_src_file():\n",
        "  try:\n",
        "    os.system(\"pip list > pip_list.txt 2>&1\")\n",
        "    for txt_file in sorted(glob.glob('*.txt')):\n",
        "      z = \"\".join(filter(str.isalnum, txt_file))\n",
        "      wandb.log_artifact(txt_file, name=f\"src_txt_{SWEEP_ID}_{z}\", type=\"my_dataset_txt\")\n",
        "    for python_file in sorted(glob.glob('*.ipynb')):\n",
        "      z = \"\".join(filter(str.isalnum, python_file))\n",
        "      wandb.log_artifact(python_file, name=f\"src_ipynb_{SWEEP_ID}_{z}\", type=\"my_dataset_ipynb\")\n",
        "    for python_file in sorted(glob.glob('*.py')):\n",
        "      z = \"\".join(filter(str.isalnum, python_file))\n",
        "      wandb.log_artifact(python_file, name=f\"src_py_{SWEEP_ID}_{z}\", type=\"my_dataset_py\")\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNLB7QwoEgy9"
      },
      "outputs": [],
      "source": [
        "def get_wand_api_key():\n",
        "  global WANDB_TOKEN\n",
        "  if len(WANDB_TOKEN) > 0:\n",
        "    return WANDB_TOKEN\n",
        "  import sys\n",
        "  IN_COLAB = 'google.colab' in sys.modules\n",
        "  if not IN_COLAB:\n",
        "    os.system(\"cp ~/api_key.txt api_key.txt\")\n",
        "  file_path = 'api_key.txt'\n",
        "  with open(file_path, 'r') as file:\n",
        "      api_key = file.read().strip()\n",
        "  return api_key\n",
        "\n",
        "\n",
        "def main():\n",
        "  global PATH_PATTERN\n",
        "  with wandb.init() as run:\n",
        "    PATH_PATTERN = PATH_PATTERN_BASE + '_' +str(run.name) + '_' +str(BASELINE)\n",
        "    save_src_file()\n",
        "    for hyper_param_name in sweep_config['parameters']:\n",
        "      globals()[hyper_param_name] = run.config[hyper_param_name]\n",
        "      print(\"set \", hyper_param_name, \"=\", run.config[hyper_param_name])\n",
        "    start_experiments()\n",
        "\n",
        "def start_with_wandb(set_baseline_true=False):\n",
        "  global SWEEP_ID, USE_WANDB, PATH_PATTERN, BASELINE\n",
        "  if set_baseline_true:\n",
        "    BASELINE = True\n",
        "  else:\n",
        "    BASELINE = False\n",
        "  USE_WANDB = True\n",
        "  os.environ[\"WANDB_MODE\"] = \"online\"\n",
        "  try:\n",
        "    SWEEP_ID = wandb.sweep(sweep_config, project=PROJECT_NAME)\n",
        "    wandb.agent(SWEEP_ID, function=main, count=10)\n",
        "  except Exception as e:\n",
        "    error_message = traceback.format_exc()\n",
        "    print(\"final error:\\n\", error_message)\n",
        "    with open('_error_log.txt', 'a') as f:\n",
        "      f.write(error_message + '\\n')\n",
        "    time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MoEC1VsxA6Im",
        "outputId": "60fb3e20-704a-4bb1-9b05-4d5b403642c6"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 7bs1xqew\n",
            "Sweep URL: https://wandb.ai/nextaid/MoldDiffGAN_noise/sweeps/7bs1xqew\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vrqdlze2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH_SIZE: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDISC_NOISE: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_DISC_MODEL: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_GEN: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tGAMMA: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgerritgr\u001b[0m (\u001b[33mnextaid\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/colab/MoldDiffGAN_noise/wandb/run-20240103_101321-vrqdlze2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nextaid/MoldDiffGAN_noise/runs/vrqdlze2' target=\"_blank\">royal-sweep-1</a></strong> to <a href='https://wandb.ai/nextaid/MoldDiffGAN_noise' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nextaid/MoldDiffGAN_noise/sweeps/7bs1xqew' target=\"_blank\">https://wandb.ai/nextaid/MoldDiffGAN_noise/sweeps/7bs1xqew</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nextaid/MoldDiffGAN_noise' target=\"_blank\">https://wandb.ai/nextaid/MoldDiffGAN_noise</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/nextaid/MoldDiffGAN_noise/sweeps/7bs1xqew' target=\"_blank\">https://wandb.ai/nextaid/MoldDiffGAN_noise/sweeps/7bs1xqew</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nextaid/MoldDiffGAN_noise/runs/vrqdlze2' target=\"_blank\">https://wandb.ai/nextaid/MoldDiffGAN_noise/runs/vrqdlze2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set  BATCH_SIZE = 256\n",
            "set  GAMMA = 0.2\n",
            "set  DISC_NOISE = 0.4\n",
            "set  EPOCHS_DISC_MODEL = 10\n",
            "set  EPOCHS_GEN = 100\n",
            "Trying to read dataset.pickle\n",
            "Train denoising model.\n",
            "Trying to read deg.pickle\n",
            "from 0 to 100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/419 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch_geometric/warnings.py:17: UserWarning: The usage of `scatter(reduce='min')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(message)\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:37<00:00, 11.11it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000000 is: 0.1748 with mean loss 0.1748 with disc loss 0.0000 with runtime 37.7110\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:39<00:00, 10.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000001 is: 0.0913 with mean loss 0.1191 with disc loss 0.0000 with runtime 39.0135\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.52it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000002 is: 0.0865 with mean loss 0.1098 with disc loss 0.0000 with runtime 36.3803\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000003 is: 0.0816 with mean loss 0.1032 with disc loss 0.0000 with runtime 36.5631\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.49it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000004 is: 0.0780 with mean loss 0.0984 with disc loss 0.0000 with runtime 36.4876\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.40it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000005 is: 0.0758 with mean loss 0.0948 with disc loss 0.0000 with runtime 36.7587\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000006 is: 0.0744 with mean loss 0.0921 with disc loss 0.0000 with runtime 36.5148\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000007 is: 0.0735 with mean loss 0.0899 with disc loss 0.0000 with runtime 36.9453\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.51it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000008 is: 0.0721 with mean loss 0.0880 with disc loss 0.0000 with runtime 36.4275\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.34it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000009 is: 0.0715 with mean loss 0.0864 with disc loss 0.0000 with runtime 36.9639\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000010 is: 0.0708 with mean loss 0.0851 with disc loss 0.0000 with runtime 36.5773\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.33it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000011 is: 0.0702 with mean loss 0.0839 with disc loss 0.0000 with runtime 36.9815\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000012 is: 0.0699 with mean loss 0.0829 with disc loss 0.0000 with runtime 36.5416\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:37<00:00, 11.30it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000013 is: 0.0697 with mean loss 0.0820 with disc loss 0.0000 with runtime 37.0749\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.45it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000014 is: 0.0696 with mean loss 0.0812 with disc loss 0.0000 with runtime 36.6018\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:37<00:00, 11.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000015 is: 0.0694 with mean loss 0.0805 with disc loss 0.0000 with runtime 37.1671\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.48it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000016 is: 0.0692 with mean loss 0.0799 with disc loss 0.0000 with runtime 36.5046\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:37<00:00, 11.30it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000017 is: 0.0690 with mean loss 0.0793 with disc loss 0.0000 with runtime 37.0732\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.45it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000018 is: 0.0687 with mean loss 0.0787 with disc loss 0.0000 with runtime 36.5924\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:37<00:00, 11.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000019 is: 0.0687 with mean loss 0.0783 with disc loss 0.0000 with runtime 37.1565\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:36<00:00, 11.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss in epoch 0000020 is: 0.0686 with mean loss 0.0778 with disc loss 0.0000 with runtime 36.5676\n",
            "save\n",
            "Generate 40000 graphs.\n",
            "Trying to read moldiffusion_run_royal-sweep-1_False_model_epoch_00000021_040000_wFalse_generated.pickle\n",
            "An error occurred: [Errno 2] No such file or directory: 'moldiffusion_run_royal-sweep-1_False_model_epoch_00000021_040000_wFalse_generated.pickle'\n",
            "Trying to read dataset.pickle\n",
            "Trying to read deg.pickle\n",
            "Loaded checkpoint of epoch 00000021 from disk.\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137876], x=[43273, 11], batch=[43273], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:57<00:00,  8.51it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137432], x=[43147, 11], batch=[43147], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:56<00:00,  8.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137244], x=[43093, 11], batch=[43093], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:56<00:00,  8.62it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 136964], x=[43014, 11], batch=[43014], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:55<00:00,  8.67it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 84], x=[28, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137672], x=[43214, 11], batch=[43214], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:55<00:00,  8.62it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 112], x=[36, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "Trying to write moldiffusion_run_royal-sweep-1_False_model_epoch_00000021_040000_wFalse_generated.pickle\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137720], x=[43227, 11], batch=[43227], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:56<00:00,  8.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated graphs  [Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11]), Data(edge_index=[2, 144], x=[45, 11])]\n",
            "generate samples batched\n",
            "load g DataBatch(edge_index=[2, 137816], x=[43256, 11], batch=[43256], ptr=[1001]) tensor([  0,   0,   0,  ..., 999, 999, 999], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 873/1000 [01:42<00:16,  7.52it/s]"
          ]
        }
      ],
      "source": [
        "wandb.login(key=get_wand_api_key())\n",
        "\n",
        "#for _ in range(10):\n",
        "start_with_wandb()\n",
        "\n",
        "#for _ in range(10):\n",
        "#start_with_wandb(set_baseline_true=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oBPW-YMVaZ6U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOv0F9OyrrdprxUBOlLgvZi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}