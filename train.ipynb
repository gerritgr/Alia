{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerritgr/Alia/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovmM6MKh0RHG"
      },
      "source": [
        "# 💊🌀 MoleculeDiffusionGAN 🌀💊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXB58ofd0yX-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gCcXEAS_7brw"
      },
      "outputs": [],
      "source": [
        "# This is used for the naming of files and folders\n",
        "PROJECT_NAME = \"MoldDiffGAN_invariant\"\n",
        "PATH_PATTERN_BASE = \"moldiffusion\"\n",
        "PATH_PATTERN = PATH_PATTERN_BASE\n",
        "\n",
        "# Setting BASELINE to True would deactivate the discriminator.\n",
        "BASELINE = False\n",
        "DEBUG = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yIsuEC808hb"
      },
      "source": [
        "### Handle Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX4PHxg-00D1"
      },
      "source": [
        "On Colab, we need to install some additional packages.\n",
        "If running on Colab, we use Google Drive to store results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6wrjwx9vQmH",
        "outputId": "b424b5fb-5ed4-4b03-e4cf-afe67a7a7796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory:  /content\n",
            "New Working Directory:  /content/drive/MyDrive/colab/MoldDiffGAN_invariant\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Check for Google Colab and WandB\n",
        "USE_COLAB = False\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  USE_COLAB = True\n",
        "except:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  import wandb # need to do this before chaning CWD\n",
        "except:\n",
        "  os.system(\"pip install wandb\")\n",
        "\n",
        "# Load Google Drive\n",
        "if USE_COLAB:\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "  dir_path = f'/content/drive/MyDrive/colab/{PROJECT_NAME}/'\n",
        "  if not os.path.exists(dir_path):\n",
        "    os.makedirs(dir_path)\n",
        "  print(\"Current Working Directory: \", os.getcwd())\n",
        "  if os.getcwd() != dir_path:\n",
        "    os.chdir(dir_path)\n",
        "    print(\"New Working Directory: \", os.getcwd())\n",
        "\n",
        "\n",
        "torch_version = torch.__version__.split(\"+\")\n",
        "try:\n",
        "  import torch_geometric\n",
        "except:\n",
        "  os.system(\"pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\")\n",
        "  os.system(\"pip install torch-geometric\")\n",
        "\n",
        "try:\n",
        "  import rdkit\n",
        "except:\n",
        "  os.system(\"pip install rdkit\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceqoOXap7kBb"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnxsm_Qm8QQ4",
        "outputId": "8152de7e-e0d9-4c51-fe40-a1c35002ff44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 100  # Set this to 300 to get better image quality\n",
        "import seaborn as sns\n",
        "\n",
        "import networkx as nx\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "import traceback\n",
        "import time\n",
        "import copy\n",
        "import pickle\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import gzip\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Sequential as Seq\n",
        "from torch.nn import Linear as Lin\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import (\n",
        "    PNA,\n",
        "    GATv2Conv,\n",
        "    GraphNorm,\n",
        "    BatchNorm,\n",
        "    global_mean_pool,\n",
        "    global_add_pool\n",
        ")\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx, degree\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d6k-s1v37muv"
      },
      "outputs": [],
      "source": [
        "# Load code to convert molecules to pyg tensors if using Colab\n",
        "if USE_COLAB and not os.path.exists(\"smiles_to_pyg\"):\n",
        "  os.system(\"git clone https://github.com/gerritgr/MoleculeDiffusionGAN.git && cp -R MoleculeDiffusionGAN/* .\")\n",
        "\n",
        "from smiles_to_pyg.molecule_load_and_convert import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2oEPNiw_utN"
      },
      "source": [
        "## Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BpeBwAGC91Xu"
      },
      "outputs": [],
      "source": [
        "##\n",
        "## Diffusion\n",
        "##\n",
        "TIMESTEPS = 1000\n",
        "START = 0.0001\n",
        "END = 0.015\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 256\n",
        "GAMMA = 0.2\n",
        "\n",
        "##\n",
        "## Prediction/Denoising\n",
        "##\n",
        "LEARNING_RATE_GEN = 0.001\n",
        "EPOCHS_GEN = 60\n",
        "\n",
        "# PNA Pred\n",
        "DROPOUT_PRED = 0.05\n",
        "DEPTH_PRED = 6\n",
        "HIDDEN_CHANNELS_PRED = 32\n",
        "TOWERS_PRED = 2\n",
        "NORMALIZATION_PRED = True\n",
        "\n",
        "##\n",
        "## Discriminator\n",
        "##\n",
        "EPOCHS_DISC_MODEL = 50\n",
        "DISC_NOISE = 0.3\n",
        "\n",
        "# PNA Disc\n",
        "HIDDEN_CHANNELS_DISC = 4\n",
        "DEPTH_DISC = 3 # 4 in original\n",
        "DROPOUT_DISC = 0.05 # 0.03 in original\n",
        "NORMALIZATION_DISC = True\n",
        "INVARIANT_LOSS = True\n",
        "\n",
        "##\n",
        "## Molecule Encoding\n",
        "##\n",
        "INDICATOR_FEATURE_DIM = 1\n",
        "FEATURE_DIM = 5  # (has to be the same for atom and bond)\n",
        "ATOM_FEATURE_DIM = FEATURE_DIM\n",
        "BOND_FEATURE_DIM = FEATURE_DIM\n",
        "NON_NODES = [True] + [False] * 5 + [True] * 5\n",
        "NON_EDGES = [True] + [True] * 5 + [False] * 5\n",
        "\n",
        "TIME_FEATURE_DIM = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9vsl3np_y0I"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8vbsdmdl-JFm"
      },
      "outputs": [],
      "source": [
        "def log(d):\n",
        "  try:\n",
        "    import wandb\n",
        "    wandb.log(d)\n",
        "  except:\n",
        "    print(d)\n",
        "\n",
        "\n",
        "def load_file(filepath):\n",
        "  print(\"Trying to read\", filepath)\n",
        "  try:\n",
        "    with gzip.open(filepath, 'rb') as f:\n",
        "      return pickle.load(f)\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "def write_file(filepath, data):\n",
        "  try:\n",
        "    data = data.cpu()\n",
        "  except:\n",
        "    pass\n",
        "  print(\"Trying to write\", filepath)\n",
        "  with gzip.open(filepath, 'wb') as f:\n",
        "    pickle.dump(data, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QaL_PWZH-eiM"
      },
      "outputs": [],
      "source": [
        "def build_dataset(seed=1234):\n",
        "  try:\n",
        "    dataset_train, dataset_test = load_file('dataset.pickle')\n",
        "    if DEBUG:\n",
        "      return dataset_train[:len(dataset_train) // 10], dataset_test[:len(dataset_test) // 10]\n",
        "    return dataset_train, dataset_test\n",
        "  except Exception as e:\n",
        "    print(f\"Could not load dataset due to error: {str(e)}, generate it now\")\n",
        "\n",
        "  dataset = read_qm9()\n",
        "  dataset_all = [g for g in dataset if g.x.shape[0] > 1]\n",
        "  dataset = list()\n",
        "\n",
        "  for g in tqdm(dataset_all):\n",
        "    try:\n",
        "      assert \"None\" not in str(pyg_to_smiles(g))\n",
        "      dataset.append(g)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  print(f\"Built and cleaned dataset, length is {len(dataset)}, old length was {len(dataset_all)}\")\n",
        "  random.Random(seed).shuffle(dataset)\n",
        "  split = int(len(dataset) * 0.8 + 0.5)\n",
        "  dataset_train = dataset[:split]\n",
        "  dataset_test = dataset[split:]\n",
        "  assert(dataset_train[0].x[0, :].numel() == INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM)\n",
        "\n",
        "  write_file(\"dataset.pickle\", (dataset_train, dataset_test))\n",
        "  return dataset_train, dataset_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aQlj5MPz_4PB"
      },
      "outputs": [],
      "source": [
        "def generate_schedule(start = START, end = END, timesteps=TIMESTEPS):\n",
        "  \"\"\"\n",
        "  Generates a schedule of beta and alpha values for a forward process.\n",
        "\n",
        "  Args:\n",
        "  start (float): The starting value for the beta values. Default is START.\n",
        "  end (float): The ending value for the beta values. Default is END.\n",
        "  timesteps (int): The number of timesteps to generate. Default is TIMESTEPS.\n",
        "\n",
        "  Returns:\n",
        "  tuple: A tuple of three tensors containing the beta values, alpha values, and\n",
        "  cumulative alpha values (alpha bars).\n",
        "  \"\"\"\n",
        "  betas = torch.linspace(start, end, timesteps, device = DEVICE)\n",
        "  assert(betas.numel() == TIMESTEPS)\n",
        "  return betas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MZJBlO6C_7-x"
      },
      "outputs": [],
      "source": [
        "def visualize_smiles_from_file(filepath):\n",
        "    print(\"Visualize molecules.\")\n",
        "    # Read SMILES from file\n",
        "    with open(filepath, 'r') as file:\n",
        "        smiles_list = [line.split(\"'\")[1] for line in file.readlines() if \"'\" in line]\n",
        "\n",
        "    # Convert SMILES to RDKit Mol objects, filtering out invalid ones\n",
        "    mols = [Chem.MolFromSmiles(smile) for smile in smiles_list[:100]]\n",
        "    mols = [mol for mol in mols if mol is not None]\n",
        "\n",
        "    if len(mols) == 0:\n",
        "        return\n",
        "\n",
        "    # Determine grid size\n",
        "    num_mols = len(mols)\n",
        "    cols = 10\n",
        "    rows = min(10, -(-num_mols // cols))  # ceil division\n",
        "\n",
        "    # Create a subplot grid\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(20, 20),\n",
        "                            gridspec_kw={'wspace': 0.3, 'hspace': 0.3})\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            ax = axs[i, j]\n",
        "            ax.axis(\"off\")  # hide axis\n",
        "            idx = i * cols + j  # index in mols list\n",
        "            if idx < num_mols:\n",
        "                img = Draw.MolToImage(mols[idx], size=(200, 200))\n",
        "                ax.imshow(img)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    # Save the figure\n",
        "    plt.savefig(filepath + '.jpg', format='jpg', bbox_inches='tight')\n",
        "\n",
        "    time.sleep(0.01)\n",
        "    try:\n",
        "        wandb.log_artifact(filepath + '.jpg', name=f\"jpg_{SWEEP_ID}_{filepath.replace('.','')}\", type=\"smiles_grid_graph\")\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X5qDfWWc_FJX"
      },
      "outputs": [],
      "source": [
        "def get_pred_from_noise(noise_pred, x_with_noise, future_t):\n",
        "  row_num = x_with_noise.shape[0]\n",
        "  betas = generate_schedule()\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t).view(row_num, 1)\n",
        "\n",
        "  scaled_noise = torch.sqrt(1.0 - alphabar_t)\n",
        "  x_without_noise = x_with_noise - scaled_noise * noise_pred\n",
        "  x_without_noise = x_without_noise / torch.sqrt(alphabar_t)\n",
        "  return x_without_noise\n",
        "\n",
        "\n",
        "def get_noise_from_pred(original_pred, x_with_noise, future_t):\n",
        "  row_num = x_with_noise.shape[0]\n",
        "  betas = generate_schedule()\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t).view(row_num, 1)\n",
        "\n",
        "  scaled_noise = torch.sqrt(alphabar_t)\n",
        "  noise = x_with_noise - scaled_noise * original_pred\n",
        "  noise = noise / torch.sqrt(1.0 - alphabar_t)\n",
        "\n",
        "  return noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WvP1-Zdg_mQC"
      },
      "outputs": [],
      "source": [
        "def log_smiles(smiles, filename):\n",
        "  try:\n",
        "    with open(filename, \"w\") as file:\n",
        "      for string in smiles:\n",
        "        file.write(str(string) + \"\\n\")\n",
        "\n",
        "    try:\n",
        "      wandb.log_artifact(filename, name=f\"src_txt_{SWEEP_ID}_{filename}\", type=\"smiles\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "    time.sleep(0.01)\n",
        "    visualize_smiles_from_file(filename)\n",
        "  except Exception as e:\n",
        "    print(\"An error occurred during training: \\n\", str(e))\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04wgZXHaDUwG"
      },
      "source": [
        "## Forward Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GfqAxJ2bDWxr"
      },
      "outputs": [],
      "source": [
        "def forward_diffusion(node_features, future_t):\n",
        "  \"\"\"\n",
        "  Performs a forward diffusion process on an node_features tensor.\n",
        "  Each row can theoreetically have its own future time point.\n",
        "  Implements the second equation from https://youtu.be/a4Yfz2FxXiY?t=649\n",
        "  \"\"\"\n",
        "  row_num = node_features.shape[0]\n",
        "\n",
        "  if \"class 'int'\" in str(type(future_t)) or \"class 'float'\" in str(type(future_t)):\n",
        "    future_t = torch.tensor([int(future_t)] * row_num).to(DEVICE)\n",
        "\n",
        "  feature_dim = node_features.shape[1]\n",
        "  future_t = future_t.view(-1)\n",
        "  assert(row_num == future_t.numel())\n",
        "  assert(future_t[0] == future_t[1]) # Let's assume they belong to the same graph.\n",
        "\n",
        "  betas = generate_schedule()\n",
        "\n",
        "  noise = torch.randn_like(node_features, device=DEVICE)\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphabar_t = torch.gather(alphas_cumprod, 0, future_t).view(row_num, 1)\n",
        "  assert(alphabar_t.numel() == row_num)\n",
        "\n",
        "  new_node_features_mean = torch.sqrt(alphabar_t) * node_features # Column-wise multiplication, now it is a matrix\n",
        "  assert(new_node_features_mean.shape == node_features.shape)\n",
        "  new_node_features_std = torch.sqrt(1.-alphabar_t) # This is a col. vector\n",
        "  new_node_features_std = new_node_features_std.repeat(1,feature_dim) # This is a matrix\n",
        "  assert(new_node_features_mean.shape == new_node_features_std.shape)\n",
        "  noisey_node_features =  new_node_features_mean + new_node_features_std * noise\n",
        "\n",
        "  return noisey_node_features, noise\n",
        "\n",
        "#forward_diffusion(torch.tensor([1,2,3.], device=DEVICE).view(3,1), torch.tensor([0,0,999], device=DEVICE)), print(\"\"), forward_diffusion(torch.tensor([1,2,3.], device=DEVICE).view(3,1), torch.tensor([999,999,999], device=DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2BUMzmPA6-W"
      },
      "source": [
        "## Denoising NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I78sdWNKNJL1"
      },
      "outputs": [],
      "source": [
        "def dataset_to_degree_bin(train_dataset):\n",
        "  \"\"\"\n",
        "  Convert a dataset to a histogram of node degrees (in-degrees).\n",
        "  Load from file if available; otherwise, compute from the dataset.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Attempt to load the degree histogram from a file.\n",
        "    deg = load_file('deg.pickle')\n",
        "    deg = deg.to(DEVICE)\n",
        "    return deg\n",
        "  except Exception as e:\n",
        "    print(f\"Could not find degree bin due to error: {str(e)}, generate it now\")\n",
        "\n",
        "  # Assert that the dataset is provided.\n",
        "  assert(train_dataset is not None)\n",
        "\n",
        "  # Compute the maximum in-degree in the training data.\n",
        "  max_degree = -1\n",
        "  for data in train_dataset:\n",
        "    data = data.to(DEVICE)\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    max_degree = max(max_degree, int(d.max()))\n",
        "\n",
        "  # Create an empty histogram for degrees.\n",
        "  deg = torch.zeros(max_degree + 1, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "  # Populate the histogram with data from the dataset.\n",
        "  for data in train_dataset:\n",
        "    data = data.to(DEVICE)\n",
        "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "    deg += torch.bincount(d, minlength=deg.numel())\n",
        "\n",
        "  # Save the computed histogram to a file.\n",
        "  write_file(\"deg.pickle\", deg.cpu())\n",
        "\n",
        "  return deg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "270vRkHbNYB6"
      },
      "outputs": [],
      "source": [
        "class PNAnet(torch.nn.Module):\n",
        "  def __init__(self, train_dataset=None, hidden_channels=HIDDEN_CHANNELS_PRED, depth=DEPTH_PRED, dropout=DROPOUT_PRED, towers=TOWERS_PRED, normalization=NORMALIZATION_PRED, pre_post_layers=1):\n",
        "    super(PNAnet, self).__init__()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Adjust hidden channels for the given towers.\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1) # must match\n",
        "\n",
        "    # Calculate input and output channels.\n",
        "    in_channels = INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM + TIME_FEATURE_DIM\n",
        "    out_channels = FEATURE_DIM\n",
        "\n",
        "    # Get degree histogram for the dataset\n",
        "    deg = dataset_to_degree_bin(train_dataset)\n",
        "\n",
        "    # Set aggregators and scalers for the PNA layer.\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "\n",
        "    # Create a normalization layer if required.\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "\n",
        "    # Define the PNA layer.\n",
        "    self.pnanet = PNA(\n",
        "        in_channels=in_channels,\n",
        "        hidden_channels=hidden_channels,\n",
        "        out_channels=hidden_channels,\n",
        "        num_layers=depth,\n",
        "        aggregators=aggregators,\n",
        "        scalers=scalers,\n",
        "        deg=deg,\n",
        "        dropout=dropout,\n",
        "        towers=towers,\n",
        "        norm=self.normalization,\n",
        "        pre_layers=pre_post_layers,\n",
        "        post_layers=pre_post_layers\n",
        "    )\n",
        "\n",
        "    # Define the final MLP layer.\n",
        "    self.final_mlp = Seq(\n",
        "        Lin(hidden_channels, hidden_channels),\n",
        "        nn.ReLU(),\n",
        "        Lin(hidden_channels, hidden_channels),\n",
        "        nn.ReLU(),\n",
        "        Lin(hidden_channels, hidden_channels),\n",
        "        nn.ReLU(),\n",
        "        Lin(hidden_channels, out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x_in, t, edge_index):\n",
        "    \"\"\"\n",
        "    Perform a forward pass through the PNAnet.\n",
        "    \"\"\"\n",
        "    row_num = x_in.shape[0]\n",
        "    t = t.view(-1, TIME_FEATURE_DIM)\n",
        "    x = torch.concat((x_in, t), dim=1)\n",
        "\n",
        "    x = self.pnanet(x, edge_index)\n",
        "    x = self.final_mlp(x)\n",
        "\n",
        "    # Assertions for sanity checks\n",
        "    assert(x.numel() > 1)\n",
        "    assert(x.shape[0] == row_num)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G2EDh-9vXPrr"
      },
      "outputs": [],
      "source": [
        "def load_latest_checkpoint(model, optimizer, loss_list, epoch_i, path_pattern_checkpoint=None):\n",
        "  \"\"\"\n",
        "  Load the latest checkpoint from the disk.\n",
        "  \"\"\"\n",
        "  if path_pattern_checkpoint is None:\n",
        "    path_pattern_checkpoint = PATH_PATTERN + \"_model_epoch_*.pth\"\n",
        "\n",
        "  try:\n",
        "    checkpoint_paths = sorted(glob.glob(path_pattern_checkpoint))\n",
        "    if len(checkpoint_paths) == 0:\n",
        "      return model, optimizer, loss_list, epoch_i\n",
        "\n",
        "    latest_checkpoint_path = checkpoint_paths[-1]\n",
        "    checkpoint = torch.load(latest_checkpoint_path, map_location=DEVICE)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch_i = checkpoint['epoch']\n",
        "    loss_list = checkpoint['loss_list']\n",
        "\n",
        "    print(f\"Loaded checkpoint of epoch {epoch_i:08} from disk.\")\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to load checkpoint. Error: {str(e)}\")\n",
        "\n",
        "  return model, optimizer, loss_list, epoch_i\n",
        "\n",
        "def save_model(model, optimizer, loss_list, epoch_i, upload=False):\n",
        "  \"\"\"\n",
        "  Save the model state to the disk.\n",
        "  \"\"\"\n",
        "  if epoch_i == 0: # Relevant for load_base_model()\n",
        "    return\n",
        "\n",
        "  save_path = f\"{PATH_PATTERN}_model_epoch_{epoch_i:08}.pth\" # Will do lexicographical ordering to load.\n",
        "\n",
        "  # Save the model and optimizer state dicts in a dictionary.\n",
        "  torch.save({\n",
        "    'epoch': epoch_i,\n",
        "    'loss_list': loss_list,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "  }, save_path)\n",
        "\n",
        "  if upload:\n",
        "    try:\n",
        "      wandb.log_artifact(save_path, name=f\"weights_{SWEEP_ID}_{epoch_i:08}_weightfile\", type=\"weight\")\n",
        "    except Exception as e:\n",
        "      print(f\"Failed to upload model. Error: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jGUISrgECQ-e"
      },
      "outputs": [],
      "source": [
        "def load_base_model(dataset_train, path_pattern_checkpoint=None):\n",
        "  model_base = PNAnet(dataset_train)\n",
        "  model_base = model_base.to(DEVICE)\n",
        "  loss_list = None\n",
        "  optimizer = Adam(model_base.parameters(), lr = LEARNING_RATE_GEN)\n",
        "  model_base, optimizer, loss_list, epoch_start = load_latest_checkpoint(model_base, optimizer, loss_list, epoch_i=0, path_pattern_checkpoint=path_pattern_checkpoint)\n",
        "\n",
        "  return model_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc7iJLKJCv3s"
      },
      "source": [
        "## Inference / Reverse Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WRAea25gUJ1"
      },
      "source": [
        "There is a _normal_ and a _restart_ method for inference. The restart version is not implemented in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oCpdISE2yxMC"
      },
      "outputs": [],
      "source": [
        "def denoise_one_step(model, g, i):\n",
        "  \"\"\"\n",
        "  Performs one step of denoising using the provided model.\n",
        "  \"\"\"\n",
        "  row_num = g.x.shape[0]\n",
        "\n",
        "  # Generate and calculate betas, alphas, and related parameters\n",
        "  betas = generate_schedule()\n",
        "  t = TIMESTEPS - i - 1  # i=0 indicates full noise\n",
        "  beta_t = betas[t]\n",
        "  alphas = 1. - betas\n",
        "  alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "  alphas_cumprod_t = alphas_cumprod[t]\n",
        "  sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1. - alphas_cumprod_t)\n",
        "  sqrt_recip_alphas_t = torch.sqrt(1.0 / alphas[t])\n",
        "  alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "\n",
        "  # Create the mask\n",
        "  mask = torch.concat(\n",
        "      (torch.tensor([False] * g.x_old.shape[0], device=DEVICE).view(-1, 1),\n",
        "       g.x_old[:, 1:] > -0.5),\n",
        "      dim=1\n",
        "  )\n",
        "\n",
        "  # Define future_t for the model predictions\n",
        "  future_t = torch.tensor([float(t)] * g.x.shape[0], device=DEVICE).view(-1, 1)\n",
        "  original_pred = model(g.x, future_t, g.edge_index)\n",
        "\n",
        "  # Extract noisy values and predict noise\n",
        "  x_with_noise = g.x[mask].view(row_num, -1)\n",
        "  future_t = torch.tensor([int(t)] * g.x.shape[0], device=DEVICE).view(-1)\n",
        "  noise_pred = get_noise_from_pred(original_pred, x_with_noise, future_t)\n",
        "\n",
        "  # Set endpoints values\n",
        "  values_now = g.x[mask].view(row_num, -1)\n",
        "  values_endpoint = noise_pred.view(row_num, -1)\n",
        "  assert values_now.shape == values_endpoint.shape\n",
        "\n",
        "  # Compute denoised values\n",
        "  model_mean = sqrt_recip_alphas_t * (values_now - beta_t * values_endpoint / sqrt_one_minus_alphas_cumprod_t)\n",
        "  values_one_step_denoised = model_mean  # in case that t == 0\n",
        "\n",
        "  if t != 0:\n",
        "    posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)  # in the paper this is in 3.2. Note that sigma^2 is variance, not std.\n",
        "    posterior_std_t = torch.sqrt(posterior_variance[t])\n",
        "    noise = torch.randn_like(values_now, device=DEVICE)\n",
        "    values_one_step_denoised = model_mean + posterior_std_t * noise\n",
        "\n",
        "  # Clone and update with denoised values\n",
        "  denoised_x = g.x.clone()\n",
        "  denoised_x[mask] = values_one_step_denoised.flatten()\n",
        "\n",
        "  return denoised_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "z5AegWA7Cy-c"
      },
      "outputs": [],
      "source": [
        "def overwrite_with_noise(g):\n",
        "  g.x_old = g.x.clone()\n",
        "  mask = torch.concat((torch.tensor([False]*g.x_old.shape[0], device=DEVICE).view(-1,1), g.x_old[:,1:]>-0.5), dim=1)\n",
        "  g.x[mask] = torch.randn_like(g.x[mask])\n",
        "  return g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "c8RKX4tVZGy2"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def generate_examples(model, dataset_train, num=100, restart_inference_method=False):\n",
        "  \"\"\"\n",
        "  Generate graph samples in batches using the provided model.\n",
        "  \"\"\"\n",
        "  # Setup\n",
        "  print(\"generate samples batched\")\n",
        "  model.eval()\n",
        "  dataset_train_start = list()\n",
        "\n",
        "  while len(dataset_train_start) < num:\n",
        "    g = dataset_train[random.choice(range(len(dataset_train)))]\n",
        "    dataset_train_start.append(g.clone().to(DEVICE))\n",
        "\n",
        "  #old\n",
        "  #while len(dataset_train_start) < num:\n",
        "  #  g = dataset_train[random.sample(range(len(dataset_train)),1)[0]]\n",
        "  #  dataset_train_start.append(g.clone().to(DEVICE))\n",
        "  #  g = dataset_train_start[-1]\n",
        "\n",
        "  assert(len(dataset_train_start) == num)\n",
        "  dataloader = DataLoader(dataset_train_start, batch_size=num)\n",
        "\n",
        "  # Inference\n",
        "  for g in dataloader:\n",
        "    g = g.to(DEVICE)\n",
        "    print(\"load g\", g, g.batch)\n",
        "    g = overwrite_with_noise(g)\n",
        "\n",
        "    for i in tqdm(range(TIMESTEPS)):\n",
        "      t = int(TIMESTEPS - i - 1)\n",
        "      if restart_inference_method:\n",
        "        x_with_less_noise = denoise_one_step_restart(model, g, i) # not implemented\n",
        "      else:\n",
        "        x_with_less_noise = denoise_one_step(model, g, i)\n",
        "      g.x = x_with_less_noise\n",
        "\n",
        "    graph_list = g.to_data_list()\n",
        "    graph_list = [g.cpu() for g in graph_list]\n",
        "\n",
        "    print(\"generated graphs \", graph_list[:10])\n",
        "    return graph_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3snibPC2C4-f"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def find_frac_correct(graphs):\n",
        "  \"\"\"\n",
        "  Determine the fraction and unique of correct graphs based on their conversion to SMILES.\n",
        "  \"\"\"\n",
        "  correct = 0\n",
        "  smiles_list = list()\n",
        "\n",
        "  for i, g in tqdm(enumerate(graphs)):\n",
        "    smiles = pyg_to_smiles(g)\n",
        "    if smiles and '.' not in smiles:\n",
        "      mol = Chem.MolFromSmiles(smiles)\n",
        "      if mol:\n",
        "        correct += 1\n",
        "        smiles_list.append((smiles, i))\n",
        "\n",
        "  frac_correct = correct / len(graphs)\n",
        "  smiles_list_0 = [s[0] for s in smiles_list]\n",
        "  unique_frac = len(set(smiles_list_0)) / len(graphs)\n",
        "\n",
        "  return frac_correct, smiles_list, unique_frac\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Pm9B-IOqC6j2"
      },
      "outputs": [],
      "source": [
        "def gen_graphs(num_per_generation=1000, num_generations=40, restart_inference_method=False, model_path=None):\n",
        "  \"\"\"\n",
        "  Generate a specified number of graphs.\n",
        "  \"\"\"\n",
        "  print(f\"Generate {num_generations*num_per_generation} graphs.\")\n",
        "  if DEBUG:\n",
        "    num_generations = int(num_generations / 10)\n",
        "\n",
        "  if model_path is None:\n",
        "    model_path = PATH_PATTERN + \"_model_epoch_*.pth\"\n",
        "\n",
        "  path = sorted(glob.glob(model_path))[-1]\n",
        "  num_samples = num_per_generation * num_generations\n",
        "  filepath = path.replace(\".pth\", f'_{num_samples:06d}_w{restart_inference_method}_generated.pickle')\n",
        "\n",
        "  results = list()\n",
        "  try:\n",
        "    results = load_file(filepath)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  if len(results) == num_samples:\n",
        "    return results\n",
        "\n",
        "  dataset_base, dataset_base_test = build_dataset()\n",
        "  model_base = load_base_model(dataset_base, path_pattern_checkpoint=path)\n",
        "\n",
        "  i = 0\n",
        "  while len(results) < num_samples:\n",
        "    i += 1\n",
        "    num = max(num_per_generation, len(results) - num_samples)\n",
        "    graphs = generate_examples(model_base, dataset_base, num=num, restart_inference_method=restart_inference_method)\n",
        "    results.extend(graphs)\n",
        "    if i % 5 == 0 or len(results) >= num_samples:\n",
        "      write_file(filepath, results)\n",
        "\n",
        "  assert(len(results) == num_samples)\n",
        "  return results\n",
        "\n",
        "\n",
        "def test_graph_generation(path_pattern=None, restart_inference_method=False):\n",
        "  generated_graphs = gen_graphs(restart_inference_method=restart_inference_method, model_path=path_pattern)\n",
        "  return find_frac_correct(generated_graphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBi59QK5C91I"
      },
      "source": [
        "## Discriminator NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LWIOKsKWC_A5"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import PNA, GCN\n",
        "\n",
        "class PNAdisc(torch.nn.Module):\n",
        "  def __init__(self, train_dataset=None, hidden_channels=HIDDEN_CHANNELS_DISC,\n",
        "               depth=DEPTH_DISC, dropout=DROPOUT_DISC, towers=1,\n",
        "               normalization=NORMALIZATION_DISC, pre_post_layers=1):\n",
        "    super(PNAdisc, self).__init__()\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Adjust hidden channels based on towers\n",
        "    hidden_channels = towers * ((hidden_channels // towers) + 1)\n",
        "\n",
        "    in_channels = INDICATOR_FEATURE_DIM + ATOM_FEATURE_DIM + BOND_FEATURE_DIM\n",
        "    assert in_channels == 11\n",
        "\n",
        "    deg = dataset_to_degree_bin(train_dataset).to(DEVICE)\n",
        "    aggregators = ['mean', 'min', 'max', 'std']\n",
        "    scalers = ['identity', 'amplification', 'attenuation']\n",
        "    self.normalization = BatchNorm(hidden_channels) if normalization else None\n",
        "    self.pnanet = PNA(in_channels=in_channels,\n",
        "                     hidden_channels=hidden_channels,\n",
        "                     out_channels=1,\n",
        "                     num_layers=depth,\n",
        "                     aggregators=aggregators,\n",
        "                     scalers=scalers,\n",
        "                     deg=deg,\n",
        "                     dropout=dropout,\n",
        "                     towers=towers,\n",
        "                     norm=self.normalization,\n",
        "                     pre_layers=pre_post_layers,\n",
        "                     post_layers=pre_post_layers)\n",
        "\n",
        "    self.gcnnet = GCN(in_channels=in_channels,\n",
        "                     hidden_channels=5,\n",
        "                     out_channels=1,\n",
        "                     num_layers=depth,\n",
        "                     dropout=dropout)\n",
        "\n",
        "  def forward(self, x, edge_index, batch=None):\n",
        "    x = x + torch.randn_like(x) * DISC_NOISE\n",
        "    x = self.gcnnet(x, edge_index) # or pna\n",
        "    x = global_mean_pool(x, batch)\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gh-x4OlODFBX"
      },
      "outputs": [],
      "source": [
        "def train_epoch_disc(model_disc, dataloader, optimizer):\n",
        "  model_disc.train()\n",
        "  start_time = time.time()\n",
        "  loss_list = []\n",
        "  acc_list = []\n",
        "\n",
        "  for batch in dataloader:\n",
        "    batch = batch.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    pred = model_disc(batch.x, batch.edge_index, batch.batch)\n",
        "    loss = F.binary_cross_entropy(pred.flatten(), batch.y.flatten())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    acc = (torch.abs(pred.flatten() - batch.y.flatten()) < 0.5).float()\n",
        "    acc_list.extend(acc.detach().cpu().tolist())\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "  return np.mean(loss_list), np.mean(acc_list), time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gCnHC3mPDGoM"
      },
      "outputs": [],
      "source": [
        "def test_disc(model_disc, dataloader):\n",
        "  model_disc.eval()\n",
        "  start_time = time.time()\n",
        "  loss_list = list()\n",
        "  acc_list = list()\n",
        "  for batch in dataloader:\n",
        "    batch = batch.to(DEVICE)\n",
        "    pred = model_disc(batch.x, batch.edge_index, batch.batch)\n",
        "    loss = F.binary_cross_entropy(pred.flatten(), batch.y.flatten())\n",
        "    acc = (torch.abs(pred.flatten()-batch.y.flatten()) < 0.5).float()\n",
        "    acc_list = acc_list + acc.detach().cpu().tolist()\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "  return np.mean(loss_list), np.mean(acc_list), time.time()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "b6vWTeUicmzH"
      },
      "outputs": [],
      "source": [
        "def train_disc_model(dataloader_disc, dataloader_disc_test, round_i):\n",
        "  model_disc = PNAdisc(dataloader_disc).to(DEVICE)\n",
        "  weight_path = f\"{PATH_PATTERN}_discriminator_model_round_{round_i:05}.pth\"\n",
        "\n",
        "  try:\n",
        "    checkpoint = torch.load(weight_path)\n",
        "    model_disc.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"found disc model in round {round_i:05}\")\n",
        "    return model_disc\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  epochs = []\n",
        "  losses_train = []\n",
        "  losses_test = []\n",
        "\n",
        "  optimizer_disc = Adam(model_disc.parameters(), lr=0.0001)\n",
        "  for epoch_i in range(EPOCHS_DISC_MODEL):\n",
        "    loss_train, acc_train, t_train = train_epoch_disc(model_disc, dataloader_disc, optimizer_disc)\n",
        "    #if epoch_i % 1 == 1 or epoch_i == EPOCHS_DISC_MODEL - 1:\n",
        "    loss_test, acc_test, t_test = test_disc(model_disc, dataloader_disc_test)\n",
        "    print(f\"train discriminator: epoch: {epoch_i:05}, loss: {loss_train:.4f}, loss test: {loss_test:.4f}, acc: {acc_train:.3f}, acc test: {acc_test:.3f}, time: {t_train:.3f}\")\n",
        "    log({\n",
        "        \"disc/step\": epoch_i + (1+round_i) * EPOCHS_DISC_MODEL,\n",
        "        \"disc/epoch\": epoch_i + (1+round_i) * EPOCHS_DISC_MODEL,\n",
        "        \"disc/loss_train\": loss_train,\n",
        "        'disc/loss_test': loss_test,\n",
        "        \"disc/acc_train\": acc_train,\n",
        "        \"disc/acc_test\": acc_test,\n",
        "        \"disc/time\": t_train\n",
        "    })\n",
        "    epochs.append(epoch_i)\n",
        "    losses_train.append(loss_train)\n",
        "    losses_test.append(loss_test)\n",
        "\n",
        "  # Plotting losses\n",
        "  plt.clf()\n",
        "  plt.plot(epochs, losses_train, label='train')\n",
        "  plt.plot(epochs, losses_test, label='test')\n",
        "  plt.legend()\n",
        "  plt.savefig(f\"discriminator_model_{round_i:05}.png\")\n",
        "\n",
        "  torch.save({\n",
        "      'model_state_dict': model_disc.state_dict(),\n",
        "      'epochs': epochs,\n",
        "      \"losses_train\": losses_train,\n",
        "      \"losses_test\": losses_test\n",
        "  }, weight_path)\n",
        "\n",
        "  return model_disc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XF6Sq0OsDLHt"
      },
      "outputs": [],
      "source": [
        "def run_disc(round_i=1):\n",
        "  print(f\"Train discriminator round {round_i}.\")\n",
        "  fake_graphs = gen_graphs(restart_inference_method=False)\n",
        "  dataset_base, dataset_base_test = build_dataset()\n",
        "  real_graphs = random.sample(dataset_base, len(fake_graphs))\n",
        "  dataset = list()\n",
        "\n",
        "  for g in fake_graphs:\n",
        "    g_i = g.clone()\n",
        "    g_i.y = torch.tensor(0.1) # use 0.1 and 0.9 for better stability\n",
        "    dataset.append(g_i)\n",
        "\n",
        "  for g in real_graphs:\n",
        "    g_i = g.clone()\n",
        "    g_i.y = torch.tensor(0.9)\n",
        "    dataset.append(g_i)\n",
        "\n",
        "  random.shuffle(dataset)\n",
        "  cut_off = int(len(dataset) * 0.8)\n",
        "  dataloader_train = DataLoader(dataset[:cut_off], batch_size = BATCH_SIZE, shuffle=True)\n",
        "  dataloader_test = DataLoader(dataset[cut_off:], batch_size = BATCH_SIZE, shuffle=True)\n",
        "\n",
        "  model_disc = train_disc_model(dataloader_train, dataloader_test, round_i)\n",
        "  return model_disc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAVWEgfDDkcM"
      },
      "source": [
        "## Train Jointly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def randomize_trainable_params(model, std=1.0):\n",
        "    \"\"\"\n",
        "    Randomizes all trainable parameters of the given model by setting them\n",
        "    to random values from a normal distribution.\n",
        "    \"\"\"\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:  # Check if the parameter is trainable\n",
        "            nn.init.normal_(param, mean=0.0, std=std)\n",
        "\n",
        "from torch_geometric.nn import GCN, GIN\n",
        "class SimGCNAlt(torch.nn.Module):\n",
        "    def __init__(self, input_dim=11):\n",
        "        super(SimGCNAlt, self).__init__()\n",
        "        self.model1 =  GCN(input_dim, 4, 4, 1)\n",
        "        self.model2 = GCN(1, 4, 4, 1)\n",
        "        randomize_trainable_params(self.model1, std=3)\n",
        "        randomize_trainable_params(self.model2, std=3)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        model1 = self.model1 #self.input_dict[str(input_dim)]\n",
        "\n",
        "        x2 = model1(x*10, edge_index)\n",
        "        x2 = torch.sin(x2*10)\n",
        "        x3 = self.model2(x2, edge_index)\n",
        "\n",
        "        x_nodeembeddings = torch.cat((x2, x3), dim=1) #todo important\n",
        "        x_graphembedding = torch.mean(x_nodeembeddings, dim=0)\n",
        "        return x_graphembedding, x_nodeembeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "\n",
        "class GraphDistance:\n",
        "    \"\"\"\n",
        "    A class to compute the distance between two graphs using a graph neural network model.\n",
        "\n",
        "    Attributes:\n",
        "    - graph_similarity_model (SimGCN): A graph neural network model for computing graph embeddings.\n",
        "\n",
        "    Methods:\n",
        "    - __call__(x1, x2, dist_type=\"L1\"): Computes the distance between two graph embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=4, output_dim=1):\n",
        "        \"\"\"\n",
        "        Initializes the GraphDistance class with a graph similarity model.\n",
        "\n",
        "        Parameters:\n",
        "        - input_dim (int): Input dimension of the graph data.\n",
        "        - output_dim (int): Output dimension of the graph embeddings.\n",
        "        \"\"\"\n",
        "        self.graph_similarity_model = SimGCNAlt().to(DEVICE) #(input_dim=input_dim, output_dim=output_dim)\n",
        "\n",
        "    def __call__(self, x1, x2, dist_type=\"L1\"):\n",
        "        \"\"\"\n",
        "        Computes the distance between the embeddings of two graphs.\n",
        "\n",
        "        Parameters:\n",
        "        - x1 (Data): The first graph data object.\n",
        "        - x2 (Data): The second graph data object.\n",
        "        - dist_type (str): The type of distance to compute (\"L1\" or \"L2\").\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: The computed distance between the graph embeddings.\n",
        "        \"\"\"\n",
        "        # Input validation\n",
        "        if not (isinstance(x1, Data) and isinstance(x2, Data)):\n",
        "            raise ValueError(\"Both inputs must be PyTorch Geometric Data objects.\")\n",
        "        if not all(hasattr(data, attr) for data in (x1, x2) for attr in (\"x\", \"edge_index\")):\n",
        "            raise ValueError(\"Input data objects must have 'x' and 'edge_index' attributes.\")\n",
        "\n",
        "        # Obtain graph embeddings\n",
        "\n",
        "        graph_emb1, _ = self.graph_similarity_model(x1.x, x1.edge_index)\n",
        "        graph_emb2, _ = self.graph_similarity_model(x2.x, x2.edge_index)\n",
        "\n",
        "\n",
        "        # Compute distance based on dist_type\n",
        "        if dist_type == \"L1\":\n",
        "            distance = torch.norm(graph_emb1 - graph_emb2, p=1)  # L1 (Manhattan) distance\n",
        "        elif dist_type == \"L2\":\n",
        "            distance = torch.norm(graph_emb1 - graph_emb2, p=2)  # L2 (Euclidean) distance\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported distance type. Choose 'L1' or 'L2'.\")\n",
        "\n",
        "        return distance"
      ],
      "metadata": {
        "id": "MSAoL8tOif-7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_dist_obj = GraphDistance()\n",
        "def graph_dist(data_gt, data_pred):\n",
        "  return graph_dist_obj(data_gt.to(DEVICE), data_pred.to(DEVICE))\n",
        "  #return F.mse_loss(data_gt.x, data_pred.x)"
      ],
      "metadata": {
        "id": "OSkZ__0sgQfZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vSqf7G3GfIi8"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, model_disc=None):\n",
        "  schedule = generate_schedule()\n",
        "  model.train()\n",
        "  start_time = time.time()\n",
        "  loss_list = []\n",
        "  loss_list_disc = []\n",
        "\n",
        "  for batch in tqdm(dataloader):\n",
        "    if batch.x.shape[0] < 2:\n",
        "      continue\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = batch.to(DEVICE)\n",
        "    row_num = batch.x.shape[0]\n",
        "\n",
        "    num_graphs_in_batch = int(torch.max(batch.batch).item() + 1)\n",
        "    future_t_select = torch.randint(0, TIMESTEPS, (num_graphs_in_batch,), device=DEVICE)\n",
        "    future_t = torch.gather(future_t_select, 0, batch.batch)\n",
        "    assert future_t.numel() == row_num\n",
        "\n",
        "    mask = torch.cat((torch.tensor([False] * row_num, device=DEVICE).view(-1, 1), batch.x[:, 1:] > -0.5), dim=1)\n",
        "    x_start_gt = batch.x[mask].view(row_num, FEATURE_DIM)\n",
        "    x_with_noise, noise_gt = forward_diffusion(x_start_gt, future_t)\n",
        "\n",
        "    x_in = batch.x.clone()\n",
        "    x_in[mask] = x_with_noise.flatten()\n",
        "    x_start_pred = model(x_in, future_t, batch.edge_index)\n",
        "    if INVARIANT_LOSS:\n",
        "      batch_pred = batch.clone()\n",
        "      batch_pred.x[mask] = x_start_pred.flatten()\n",
        "      batch = batch.to(DEVICE)\n",
        "      batch_pred = batch_pred.to(DEVICE)\n",
        "      loss = graph_dist(batch, batch_pred)\n",
        "    else:\n",
        "      loss = F.mse_loss(x_start_gt, x_start_pred)\n",
        "\n",
        "    disc_loss = torch.tensor(0.0, device=DEVICE)\n",
        "    if model_disc is not None:\n",
        "      x_in[mask] = x_start_pred.flatten()\n",
        "      disc_loss = torch.mean((1.0 - model_disc(x_in, batch.edge_index, batch=batch.batch))**2)\n",
        "      loss = (1.0 - GAMMA) * loss + GAMMA * disc_loss\n",
        "\n",
        "    loss.backward()\n",
        "    loss_list.append(loss.item())\n",
        "    loss_list_disc.append(disc_loss.item())\n",
        "    optimizer.step()\n",
        "\n",
        "  return np.mean(loss_list), np.mean(loss_list_disc), time.time() - start_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5hRd_XXGgkRS"
      },
      "outputs": [],
      "source": [
        "def train_base_model(train_loader, epoch_num=EPOCHS_GEN, model_disc=None):\n",
        "  print(\"Train denoising model.\")\n",
        "  if DEBUG:\n",
        "    epoch_num = int(epoch_num / 10)\n",
        "\n",
        "  dataset_train = train_loader.dataset\n",
        "  model_base = PNAnet(dataset_train).to(DEVICE)\n",
        "\n",
        "  optimizer = Adam(model_base.parameters(), lr=LEARNING_RATE_GEN * 0.01) # the mutliplication makes no real sense\n",
        "  loss_list = []\n",
        "  model_base, optimizer, loss_list, epoch_start = load_latest_checkpoint(model_base, optimizer, loss_list, epoch_i=0)\n",
        "  epoch_start = min(epoch_start, epoch_num)\n",
        "  print(f\"from {epoch_start} to {epoch_num}\")\n",
        "\n",
        "  for epoch_i in range(epoch_start, epoch_num):\n",
        "    try:\n",
        "      loss, loss_disc, time_elapsed = train_epoch(model_base, train_loader, optimizer, model_disc=model_disc)\n",
        "      loss_list.append((epoch_i, loss))\n",
        "      mean_loss = np.mean([y for _, y in loss_list] + [loss])\n",
        "      print(f\"loss in epoch {epoch_i:07} is: {loss:05.4f} with mean loss {mean_loss:05.4f} with disc loss {loss_disc:05.4f} with runtime {time_elapsed:05.4f}\")\n",
        "      log({\n",
        "        \"gen/step\": epoch_i,\n",
        "        \"gen/epoch\": epoch_i,\n",
        "        \"gen/loss\": loss,\n",
        "        \"gen/mean_loss\": mean_loss,\n",
        "        \"gen/start_loss\": loss_disc,\n",
        "        \"gen/runtime\": time_elapsed\n",
        "      })\n",
        "\n",
        "      if (epoch_i % 20 == 0 and epoch_i > epoch_start) or epoch_i == epoch_num - 1 or BATCH_SIZE == 1:\n",
        "        print(\"save\")\n",
        "        save_model(model_base, optimizer, loss_list, epoch_i + 1, upload = epoch_i == epoch_num - 1)\n",
        "        time.sleep(0.01)\n",
        "        frac, smiles_list, unique_frac = test_graph_generation(restart_inference_method=False)\n",
        "        frac_restart, smiles_list_restart, unique_frac_restart = 0, list(), 0 #test_graph_generation(restart_inference_method=True)\n",
        "        print(f\"Fraction of correct graphs: {frac}, with restart_inference_method inference {frac_restart}\")\n",
        "        log({\n",
        "          \"inference/step\": epoch_i,\n",
        "          \"inference/epoch\": epoch_i,\n",
        "          \"inference/frac_normal\": frac,\n",
        "          \"inference/frac_restart\": frac_restart,\n",
        "          \"inference/frac_normal_unique\": unique_frac,\n",
        "          \"inference/frac_restart_unique\": unique_frac_restart\n",
        "        })\n",
        "        log_smiles(smiles_list, f\"{PATH_PATTERN}_smiles_{epoch_i}_normal.txt\")\n",
        "        log_smiles(smiles_list_restart, f\"{PATH_PATTERN}_smiles_{epoch_i}_restart.txt\")\n",
        "        try:\n",
        "          print(smiles_list[:20])\n",
        "          print(smiles_list_restart[:20])\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "    except Exception as e:\n",
        "      print(f\"An error occurred during training: \\n{str(e)}\")\n",
        "      traceback.print_exc()\n",
        "      raise e\n",
        "\n",
        "  return model_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qbC0Wq8D5Ux"
      },
      "source": [
        "### Putting Everything Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Q6zFANMID6j-"
      },
      "outputs": [],
      "source": [
        "def start_experiments(rounds=6): #originally 5\n",
        "  global DISC_NOISE\n",
        "  if DEBUG:\n",
        "    rounds = rounds // 2\n",
        "  dataset_base, dataset_base_test = build_dataset()\n",
        "  dataloader_base = DataLoader(dataset_base, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  model_base = train_base_model(dataloader_base, epoch_num = EPOCHS_GEN*1)\n",
        "\n",
        "  for round_i in range(1, rounds):\n",
        "    if BASELINE:\n",
        "      model_disc = None\n",
        "    else:\n",
        "      model_disc = run_disc(round_i=round_i)\n",
        "    model_base = train_base_model(dataloader_base, epoch_num = EPOCHS_GEN*(round_i+1), model_disc=model_disc)\n",
        "    #DISC_NOISE = DISC_NOISE*0.5\n",
        "\n",
        "  save_src_file()\n",
        "  return  model_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_OiyFytEAKd"
      },
      "source": [
        "### Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "H2haryq1EA_m"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import wandb\n",
        "except:\n",
        "  # Train with discriminator (our method)\n",
        "  start_experiments(rounds=5)\n",
        "  # Train without discriminator (baseline)\n",
        "  BASELINE = True\n",
        "  start_experiments(rounds=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "086_7Od-EVX1"
      },
      "source": [
        "## Training with WandB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7ZWyjSb0d_8"
      },
      "source": [
        "We can use WandB to save the training results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5SWqlSfEWSF",
        "outputId": "214b32bc-d3b7-473d-a794-01c6425d418a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/usr/local/lib/python3.10/dist-packages/wandb']\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "print(wandb.__path__) # this should look like ['/usr/local/lib/python3.10/dist-packages/wandb']. Make sure to not install wandb into your current working dir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lFyzrxmZFAMJ"
      },
      "outputs": [],
      "source": [
        "WANDB_TOKEN = \"\" # Add you WandB token here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "M3MHvxtkEYW2"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"AliaMol\",\n",
        "    \"method\": \"random\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"inference/frac_normal_unique\",\n",
        "        \"goal\": \"maximize\",\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"BATCH_SIZE\": {\"values\": [256]}, #256\n",
        "        \"GAMMA\": {\"values\": [0.2]}, #0.2 is good\n",
        "        \"DISC_NOISE\": {\"values\": [2.0]},  # 0.3 in generation for paper\n",
        "        \"EPOCHS_DISC_MODEL\": {\"values\": [3]},\n",
        "        \"EPOCHS_GEN\": {\"values\": [100]},\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ivM55NJ7EeuR"
      },
      "outputs": [],
      "source": [
        "def save_src_file():\n",
        "  try:\n",
        "    os.system(\"pip list > pip_list.txt 2>&1\")\n",
        "    for txt_file in sorted(glob.glob('*.txt')):\n",
        "      z = \"\".join(filter(str.isalnum, txt_file))\n",
        "      wandb.log_artifact(txt_file, name=f\"src_txt_{SWEEP_ID}_{z}\", type=\"my_dataset_txt\")\n",
        "    for python_file in sorted(glob.glob('*.ipynb')):\n",
        "      z = \"\".join(filter(str.isalnum, python_file))\n",
        "      wandb.log_artifact(python_file, name=f\"src_ipynb_{SWEEP_ID}_{z}\", type=\"my_dataset_ipynb\")\n",
        "    for python_file in sorted(glob.glob('*.py')):\n",
        "      z = \"\".join(filter(str.isalnum, python_file))\n",
        "      wandb.log_artifact(python_file, name=f\"src_py_{SWEEP_ID}_{z}\", type=\"my_dataset_py\")\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "TNLB7QwoEgy9"
      },
      "outputs": [],
      "source": [
        "def get_wand_api_key():\n",
        "  global WANDB_TOKEN\n",
        "  if len(WANDB_TOKEN) > 0:\n",
        "    return WANDB_TOKEN\n",
        "  import sys\n",
        "  IN_COLAB = 'google.colab' in sys.modules\n",
        "  if not IN_COLAB:\n",
        "    os.system(\"cp ~/api_key.txt api_key.txt\")\n",
        "  else:\n",
        "    os.system(\"cp ../api_key.txt api_key.txt\")\n",
        "  file_path = 'api_key.txt'\n",
        "  with open(file_path, 'r') as file:\n",
        "      api_key = file.read().strip()\n",
        "  return api_key\n",
        "\n",
        "\n",
        "def main():\n",
        "  global PATH_PATTERN\n",
        "  with wandb.init() as run:\n",
        "    PATH_PATTERN = PATH_PATTERN_BASE + '_' +str(run.name) + '_' +str(BASELINE)\n",
        "    save_src_file()\n",
        "    for hyper_param_name in sweep_config['parameters']:\n",
        "      globals()[hyper_param_name] = run.config[hyper_param_name]\n",
        "      print(\"set \", hyper_param_name, \"=\", run.config[hyper_param_name])\n",
        "    start_experiments()\n",
        "\n",
        "def start_with_wandb(set_baseline_true=False, count=1):\n",
        "  global SWEEP_ID, USE_WANDB, PATH_PATTERN, BASELINE\n",
        "  if set_baseline_true:\n",
        "    BASELINE = True\n",
        "  else:\n",
        "    BASELINE = False\n",
        "  USE_WANDB = True\n",
        "  os.environ[\"WANDB_MODE\"] = \"online\"\n",
        "  try:\n",
        "    SWEEP_ID = wandb.sweep(sweep_config, project=PROJECT_NAME)\n",
        "    wandb.agent(SWEEP_ID, function=main, count=count)\n",
        "  except Exception as e:\n",
        "    error_message = traceback.format_exc()\n",
        "    print(\"final error:\\n\", error_message)\n",
        "    with open('_error_log.txt', 'a') as f:\n",
        "      f.write(error_message + '\\n')\n",
        "    time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MoEC1VsxA6Im",
        "outputId": "8e52aff1-0759-4ccf-8159-e232f1d5d0d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgerrit-dfki\u001b[0m (\u001b[33mhmm-dfki\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: eeqllhhm\n",
            "Sweep URL: https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/sweeps/eeqllhhm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 70veclxf with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH_SIZE: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDISC_NOISE: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_DISC_MODEL: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_GEN: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tGAMMA: 0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/colab/MoldDiffGAN_invariant/wandb/run-20240312_124325-70veclxf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/runs/70veclxf' target=\"_blank\">crisp-sweep-1</a></strong> to <a href='https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/sweeps/eeqllhhm' target=\"_blank\">https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/sweeps/eeqllhhm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant' target=\"_blank\">https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/sweeps/eeqllhhm' target=\"_blank\">https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/sweeps/eeqllhhm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/runs/70veclxf' target=\"_blank\">https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/runs/70veclxf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "set  BATCH_SIZE = 256\n",
            "set  GAMMA = 0.2\n",
            "set  DISC_NOISE = 2\n",
            "set  EPOCHS_DISC_MODEL = 3\n",
            "set  EPOCHS_GEN = 100\n",
            "Trying to read dataset.pickle\n",
            "Train denoising model.\n",
            "Trying to read deg.pickle\n",
            "from 0 to 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/419 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='min')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(message)\n",
            "  6%|▌         | 25/419 [00:04<00:42,  9.35it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
            "  7%|▋         | 28/419 [00:04<00:43,  8.99it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "  7%|▋         | 30/419 [00:05<00:44,  8.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 7iikndyv\n",
            "Sweep URL: https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/sweeps/7iikndyv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 80/419 [00:10<00:39,  8.66it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: egrkala3 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH_SIZE: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDISC_NOISE: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_DISC_MODEL: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_GEN: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tGAMMA: 0.2\n",
            " 27%|██▋       | 114/419 [00:15<00:47,  6.36it/s]Exception in thread Thread-18 (_run_job):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
            "    self._function()\n",
            "  File \"<ipython-input-37-5ae8dfe5c5d6>\", line 19, in main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1195, in init\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1176, in init\n",
            "    run = wi.init()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 785, in init\n",
            "    raise error\n",
            "wandb.errors.CommError: It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 404: Not Found)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/agents/pyagent.py\", line 313, in _run_job\n",
            "    wandb.finish(exit_code=1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 4104, in finish\n",
            "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 420, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 361, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 1961, in finish\n",
            "    return self._finish(exit_code, quiet)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 1968, in _finish\n",
            "    with telemetry.context(run=self) as tel:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 759, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 770, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 101, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem at: <ipython-input-37-5ae8dfe5c5d6> 19 main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 117/419 [00:15<00:44,  6.77it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            " 28%|██▊       | 118/419 [00:15<00:48,  6.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: f38cg82m\n",
            "Sweep URL: https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/sweeps/f38cg82m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 125/419 [00:17<00:55,  5.30it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
            " 31%|███       | 128/419 [00:17<00:45,  6.35it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            " 31%|███       | 129/419 [00:17<00:42,  6.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: hxcbe780\n",
            "Sweep URL: https://wandb.ai/hmm-dfki/MoldDiffGAN_invariant/sweeps/hxcbe780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 135/419 [00:18<00:38,  7.46it/s]Exception in thread ChkStopThr:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 286, in check_stop_status\n",
            "    self._loop_check_status(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n",
            "    local_handle = request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 787, in deliver_stop_status\n",
            "    return self._deliver_stop_status(status)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 484, in _deliver_stop_status\n",
            "    return self._deliver_record(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 449, in _deliver_record\n",
            "    handle = mailbox._deliver_record(record, interface=self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
            "    interface._publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "Exception in thread Exception in thread NetStatThr:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)IntMsgThr:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 300, in check_internal_messages\n",
            "    self._loop_check_status(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n",
            "    local_handle = request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 803, in deliver_internal_messages\n",
            "    return self._deliver_internal_messages(internal_message)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 506, in _deliver_internal_messages\n",
            "\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 268, in check_network_status\n",
            "    self._loop_check_status(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n",
            "    local_handle = request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface.py\", line 795, in deliver_network_status\n",
            "    return self._deliver_network_status(status)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 500, in _deliver_network_status\n",
            "    return self._deliver_record(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 449, in _deliver_record\n",
            "    handle = mailbox._deliver_record(record, interface=self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
            "    interface._publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "    return self._deliver_record(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_shared.py\", line 449, in _deliver_record\n",
            "    handle = mailbox._deliver_record(record, interface=self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
            "    interface._publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            " 33%|███▎      | 140/419 [00:19<00:43,  6.44it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 05pkmsg7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4rboqd49 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH_SIZE: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBATCH_SIZE: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDISC_NOISE: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tDISC_NOISE: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_DISC_MODEL: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_DISC_MODEL: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_GEN: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tGAMMA: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tEPOCHS_GEN: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tGAMMA: 0.2\n",
            " 34%|███▍      | 142/419 [00:19<00:40,  6.80it/s]"
          ]
        }
      ],
      "source": [
        "wandb.login(key=get_wand_api_key())\n",
        "\n",
        "for _ in range(40):\n",
        "  try:\n",
        "    start_with_wandb()\n",
        "  except:\n",
        "    time.sleep(60)\n",
        "\n",
        "for _ in range(40):\n",
        "  try:\n",
        "    start_with_wandb(set_baseline_true=True)\n",
        "  except:\n",
        "    time.sleep(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oBPW-YMVaZ6U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMh8XAdryjw5HVI/2aeWa75",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}